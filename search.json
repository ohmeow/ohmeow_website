[
  {
    "objectID": "pages/guides/fastapi-guide.html",
    "href": "pages/guides/fastapi-guide.html",
    "title": "FastAPI Guide",
    "section": "",
    "text": "What are some good reference architectures?\n\n\n\n\n\nSee:\nUsing FastAPI to Build Python Web APIs\nFull Stack FastAPI and PostgreSQL - Base Project Generator\nUp and running with fastapi series\n\n\n\n\n\n\n\n\n\nHow do I do user registration/login, e-mail verification, password resets, etc… with fastapi?\n\n\n\n\n\nSee:\nHandle Registration in FastAPI and Tortoise ORM\nHandling Email Confirmation During Registration in Flask\nFlask Rest API -Part:5- Password Reset\nE-commerce API with FastAPI | Sending Verification Emails | FastAPI-Mail\n\n\n\n\n\n\n\n\n\nWhat does a star(*) mean in a method parameter?\n\n\n\n\n\nIt simply allows you to order your arguments so those without default values can be placed ahead of those that can. It also ensures that keyword arguments are used everywhere (which may or may not be desirable when refactoring code). I usually find it unnecessary except in places where I’m using BackgroundTasks or a depency injected argument somewhere, for example:\n@app.get(\"/\")\ndef get_username(*, db: Session = Depends(get_db), user_id: int) -&gt; str:\n    return db.query(User).get(user_id).username\nSee:\nWhat does a star(*) mean in a method parameter?\nOrder the parameters as you need, tricks"
  },
  {
    "objectID": "pages/guides/fastapi-guide.html#designing-your-api",
    "href": "pages/guides/fastapi-guide.html#designing-your-api",
    "title": "FastAPI Guide",
    "section": "",
    "text": "What are some good reference architectures?\n\n\n\n\n\nSee:\nUsing FastAPI to Build Python Web APIs\nFull Stack FastAPI and PostgreSQL - Base Project Generator\nUp and running with fastapi series\n\n\n\n\n\n\n\n\n\nHow do I do user registration/login, e-mail verification, password resets, etc… with fastapi?\n\n\n\n\n\nSee:\nHandle Registration in FastAPI and Tortoise ORM\nHandling Email Confirmation During Registration in Flask\nFlask Rest API -Part:5- Password Reset\nE-commerce API with FastAPI | Sending Verification Emails | FastAPI-Mail\n\n\n\n\n\n\n\n\n\nWhat does a star(*) mean in a method parameter?\n\n\n\n\n\nIt simply allows you to order your arguments so those without default values can be placed ahead of those that can. It also ensures that keyword arguments are used everywhere (which may or may not be desirable when refactoring code). I usually find it unnecessary except in places where I’m using BackgroundTasks or a depency injected argument somewhere, for example:\n@app.get(\"/\")\ndef get_username(*, db: Session = Depends(get_db), user_id: int) -&gt; str:\n    return db.query(User).get(user_id).username\nSee:\nWhat does a star(*) mean in a method parameter?\nOrder the parameters as you need, tricks"
  },
  {
    "objectID": "pages/guides/fastapi-guide.html#working-with-pydantic-objects",
    "href": "pages/guides/fastapi-guide.html#working-with-pydantic-objects",
    "title": "FastAPI Guide",
    "section": "Working with Pydantic objects",
    "text": "Working with Pydantic objects\n\n\n\n\n\n\nHow to cast one pydantic object into another?\n\n\n\n\n\nMyModel(**my_model_from.dict())"
  },
  {
    "objectID": "pages/guides/fastapi-guide.html#dockerizing-a-fastapi-application",
    "href": "pages/guides/fastapi-guide.html#dockerizing-a-fastapi-application",
    "title": "FastAPI Guide",
    "section": "Dockerizing a FastAPI application",
    "text": "Dockerizing a FastAPI application\n\n\n\n\n\n\nHow can I Dockerize my app?\n\n\n\n\n\nSee:\nHow to Dockerize a Python App with FastAPI\nAn Extremely Simple Docker, Traefik, and Python FastAPI Example"
  },
  {
    "objectID": "pages/guides/fastapi-guide.html#dealing-with-common-errors",
    "href": "pages/guides/fastapi-guide.html#dealing-with-common-errors",
    "title": "FastAPI Guide",
    "section": "Dealing with common errors",
    "text": "Dealing with common errors\n\n\n\n\n\n\nI get an Unprocessable Entity (422) error when I post/put to my API\n\n\n\n\n\nIt usually means the body of your request doesn’t mesh with what your API method is expected. Make sure that the object your passing in matches what you’ve specified, including using the Body(...[,embed=True]) types correctly\nSee:\nBody - Multiple Parameters\nPython: FastApi (Unprocessable Entity) error\n\n\n\n\n\n\n\n\n\nAttributeError: ‘coroutine’ object has no attribute ‘X’\n\n\n\n\n\nUsually means you are missing “await” from async method (fastapi/python)\n\n\n\n\n\n\n\n\n\nI’m using Encode Databases against a postgres database, but my RETURNING statements don’t return all the columns I specify\n\n\n\n\n\nRETURNING statements work okay with fetch_one/fetch_all. If you are using execute, it won’t work.\nSee: Support for RETURNING\n\n\n\n\n\n\n\n\n\nWhen I deploy using Nginx reverse-proxy, I get mixed content errors like the one below…\n\n\n\n\n\nMixed Content: The page at 'https://page.com' was loaded over HTTPS, but requested an insecure\nXMLHttpRequest endpoint 'http://page.com?filter=xxxx'.\nThis request has been blocked; the content must be served over HTTPS.\nIf you add a trailing slash (/) in your API requests it will fix the problem BUT a much easier solution is to the correct proxy headers for Uvicorn. They should look something like this:\nupstream api_server {\n    server ${API_HOST}:${API_PORT} fail_timeout=0;\n  }\n...\nserver {\n  ...\n  location ~ /api/ {\n    proxy_set_header   Host $host;\n    proxy_set_header   X-Real-IP $remote_addr;\n    proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header   X-Forwarded-Host $server_name;\n    proxy_pass http://api_server;\n  }\nSee the following resources:\nFailure to load any static files when deploying with HTTPS\nfastapi-react nginx.conf\nAdd option for adding a trailing slash automatically\nAjax Product Filter does not work in https - Fixed"
  },
  {
    "objectID": "pages/guides/deployment-guide.html",
    "href": "pages/guides/deployment-guide.html",
    "title": "Deployment Guide",
    "section": "",
    "text": "How can I deploy my full-stack application as Docker containers?\n\n\n\n\n\nSee:\nFlask + Vue + MySQL on Docker. Part III — Let’s Dockerize\n\n\n\n\n\n\n\n\n\nWhat containers should I consider deploying?\n\n\n\n\n\nSee:\nWeb deployment using AWS Lightsail\n\n\n\n\n\n\n\n\n\nWhat to deploy a containerized application to AWS LightSail\n\n\n\n\n\nSee:\nLightsail Containers: An Easy Way to Run your Containers in the Cloud\nAmazon Lightsail Tutorial: Deploy an NGINX Reverse Proxy\nHow to build a Pocket Platform-as-a-Service"
  },
  {
    "objectID": "pages/guides/deployment-guide.html#deployment-techniques",
    "href": "pages/guides/deployment-guide.html#deployment-techniques",
    "title": "Deployment Guide",
    "section": "",
    "text": "How can I deploy my full-stack application as Docker containers?\n\n\n\n\n\nSee:\nFlask + Vue + MySQL on Docker. Part III — Let’s Dockerize\n\n\n\n\n\n\n\n\n\nWhat containers should I consider deploying?\n\n\n\n\n\nSee:\nWeb deployment using AWS Lightsail\n\n\n\n\n\n\n\n\n\nWhat to deploy a containerized application to AWS LightSail\n\n\n\n\n\nSee:\nLightsail Containers: An Easy Way to Run your Containers in the Cloud\nAmazon Lightsail Tutorial: Deploy an NGINX Reverse Proxy\nHow to build a Pocket Platform-as-a-Service"
  },
  {
    "objectID": "pages/guides/deployment-guide.html#aws-tips-tricks",
    "href": "pages/guides/deployment-guide.html#aws-tips-tricks",
    "title": "Deployment Guide",
    "section": "AWS Tips & Tricks",
    "text": "AWS Tips & Tricks\n\n\n\n\n\n\nHow can I use multiple AWS accounts from the CLI?\n\n\n\n\n\nSee:\nHow to use multiple AWS accounts from the command line?"
  },
  {
    "objectID": "guides.html#machine-learning-deep-learning",
    "href": "guides.html#machine-learning-deep-learning",
    "title": "Guides",
    "section": "Machine Learning / Deep Learning",
    "text": "Machine Learning / Deep Learning\nHow to learn (Deep Learning)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ohmeow",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGemini (Part II) - The Unified SDK\n\n\n\n\n\n\nLLMS\n\n\nGemini\n\n\nGoogle\n\n\n\nToday, we’ll look at how to get started with the unified SDK for both the Gemini API and Vertex API users.\n\n\n\n\n\nDec 24, 2024\n\n\nWayde Gilliam\n\n\n14 min\n\n\n12/24/24, 3:29:25 PM\n\n\n\n\n\n\n\n\n\n\n\n\nGemini (Part I) - Why You Should Consider Gemini\n\n\n\n\n\n\nLLMS\n\n\nGemini\n\n\nGoogle\n\n\n\nGemini may not be “all you need”, but it is in my opinion the future. In this series we’ll explore the capabilities offered in the Gemini ecosystem and how this family of LLMs can be used to build better LLM applications for a fraction of the cost. We’ll start with a brief look at how it compares with others models relative to both performance and cost and why you should consider it as a potential “go to” in building AI powered applications of almost any kind.\n\n\n\n\n\nDec 13, 2024\n\n\nWayde Gilliam\n\n\n6 min\n\n\n12/13/24, 7:25:06 PM\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Workshop #4 - L1 Evals and Dataset Curation (Part I)\n\n\n\n\n\n\nNLP\n\n\nLLMS\n\n\nOpenAI\n\n\nBrainTrust\n\n\nEvals\n\n\nDatasets\n\n\nlearning\n\n\nprojects\n\n\n\nHaving proved to ourselves that our objectives are achievable, we are now ready to begin building out an evaluation pipeline to quantifiably measure our progress as we develop our LLM powered app. Such a system is a remedy for the mere anecodtal assessments that are unreliable, subjective, impossible to track over time, and sadly what many folks settle for. With an “evals first” mindset, we can systematically inspect our data, know exactly where things are going well or not, and build some intuition about where we should concentrate our efforts. Good evals also serve as a foundation for curating datasets that can be used in both building better evals and fine tuning.\n\n\n\n\n\nJul 25, 2024\n\n\nWayde Gilliam\n\n\n13 min\n\n\n7/25/24, 6:22:07 PM\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Workshop #3 - How Far Can We Get With Prompting Alone?”\n\n\n\n\n\n\nNLP\n\n\nLLMS\n\n\nOpenAI\n\n\nAnthropic\n\n\nMeta\n\n\nFireworksAI\n\n\nReplicate\n\n\nLlama\n\n\nlearning\n\n\nprojects\n\n\n\nYou have to start somewhere, and that somewhere is with one or more of the big dogs in world of LLMs. Back in the day, that used to mean OpenAI. Today, however, we live in a time that affords us the opportunity to experiment with a number of both closed and open source models from the likes of OpenAI, Anthropic, FireworksAI, Meta, and many others. In this post I’ll demonstrate how we can use several of these vendors to actually build a pipeline that begins to meet our project objectives defined in the previously. I’m going to use the results as a vibe check to guage how realistic my vision is, build intution around where improvements can be made, and also get an idea if using one or more of the big dogs is good enough.\n\n\n\n\n\nJul 18, 2024\n\n\nWayde Gilliam\n\n\n17 min\n\n\n7/18/24, 2:04:55 PM\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Workshop #2 - From Noise to Knowledge: Mastering the Art of Objective Definition and Data Refinement”\n\n\n\n\n\n\nNLP\n\n\nLLMS\n\n\ndata cleaning\n\n\nlearning\n\n\nprojects\n\n\n\nIn generative NLP applications, the effectiveness of your model hinges on the quality and relevance of the contextual data your provide it. Whether tailored to a specific use case or unique business domain, whether generated synthetically or pulled from existing data sources, good contextual data is key for later curating a dataset that can be used in effective evaluation pipelines and potential fine tuning. To get this right, you need to clearly define your objectives … and that’s what we will be talking about in this post.\n\n\n\n\n\nJul 14, 2024\n\n\nWayde Gilliam\n\n\n18 min\n\n\n7/14/24, 4:23:49 PM\n\n\n\n\n\n\n\n\n\n\n\n\nStructuring Enums for Flawless LLM results with Instructor\n\n\n\n\n\n\nLLMs\n\n\npydantic\n\n\nInstructor\n\n\n\nEnums enhance code readability and maintainability by replacing hard-coded constants with meaningful names and restricting variables to predefined values that can be used across all tiers of your application. This reduces bugs and improves code clarity, making it easier to refactor and understand. However, it can be frustrating to get LLMs and libraries like Instructor to use them correctly when dealing with structured output.\n\n\n\n\n\nJul 6, 2024\n\n\nWayde Gilliam\n\n\n11 min\n\n\n7/7/24, 1:11:48 PM\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Workshop #1 - How to take a course that never ends\n\n\n\n\n\n\nNLP\n\n\nLLMS\n\n\ndatasets\n\n\nlearning\n\n\nprojects\n\n\n\nWelcome to the inaugural post of my series on the intricacies of my course project from Hamel Husain and Dan Barker’s expansive LLM Workshop/Course. In this opening article, I’ll be diving into a post-mortem analysis of the course, sharing key takeaways, and offering insights on how to effectively navigate a course of this nature.\n\n\n\n\n\nJul 3, 2024\n\n\nWayde Gilliam\n\n\n15 min\n\n\n12/13/24, 10:07:43 AM\n\n\n\n\n\n\n\n\n\n\n\n\nA Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling\n\n\n\n\n\n\nfastai\n\n\nfastbook\n\n\ntabular\n\n\nstructured\n\n\ndecision trees\n\n\nrandom forest\n\n\nembeddings\n\n\ncategorical variables\n\n\nboosting\n\n\ngdbt\n\n\nxgboost\n\n\n\nIn chapter of 8 of “Deep Learning for Coders with fastai & PyTorch” we learned that the neural network version of of collaborative model is in fact built on something called TabularModel, and that in fact, an EmbeddingNN is nothing but a TabularModel without any continuous (or real) numbers. “Structured” or “tabular” data describes datasets that look like an Excel spreadsheet or a relational database table, of which, it may be a composed of both categorical and/or real numbers. Working with such data is the subject of chapter 9, so lets go!\n\n\n\n\n\nApr 25, 2022\n\n\nWayde Gilliam\n\n\n22 min\n\n\n6/17/24, 6:58:52 PM\n\n\n\n\n\n\n\n\n\n\n\n\nA Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering\n\n\n\n\n\n\nfastai\n\n\nfastbook\n\n\ncollaborative filtering\n\n\nlatent factors\n\n\nembeddings\n\n\nrecommender systems\n\n\nrecsys\n\n\n\nThis chapter of “Deep Learning for Coders with fastai & PyTorch” moves us away from computer vision to collaborative filtering (think recommendation systems). We’ll explore building these models using the traditional “dot product” approach and also using a neural network, but we’ll begin by covering the idea of “latent factors,” which are both important for colloborative and tabular models. Lets go!\n\n\n\n\n\nMar 31, 2022\n\n\nWayde Gilliam\n\n\n10 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nA Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models\n\n\n\n\n\n\nfastai\n\n\nfastbook\n\n\nclassification\n\n\ncomputer vision\n\n\ntechniques\n\n\nbag of tricks\n\n\n\nThis chapter of \"Deep Learning for Coders with fastai & PyTorch\" details several techniques you can apply to getting SOTA results with your image classification models! It’s the last chapter dedicated to computer vision before diving into colloborate filtering, tabular, and NLP models\n\n\n\n\n\nMar 28, 2022\n\n\nWayde Gilliam\n\n\n8 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nA Journey Through Fastbook (AJTFB) - Chapter 6: Regression\n\n\n\n\n\n\nfastai\n\n\nfastbook\n\n\nregression\n\n\ncomputer vision\n\n\nkey point\n\n\n\nIts the more things you can do with computer vision chapter of \"Deep Learning for Coders with fastai & PyTorch\"! Having looked at both multiclass and multilable classification, we now turn our attention to regression tasks. In particular, we’ll look at key point regression models covered in chapter 6. Soooo lets go!\n\n\n\n\n\nFeb 9, 2022\n\n\nWayde Gilliam\n\n\n5 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nA Journey Through Fastbook (AJTFB) - Chapter 6: Multilabel Classification\n\n\n\n\n\n\nfastai\n\n\nfastbook\n\n\nmulti-label classification\n\n\nbinary cross entropy\n\n\nBCE\n\n\ncomputer vision\n\n\n\nIts the more things you can do with computer vision chapter of \"Deep Learning for Coders with fastai & PyTorch\"! We’ll go over everything you need to know to get started with multi-label classification tasks from datablocks to training and everything in between. Next post we’ll look at regression tasks, in particular key point regression models that are also covered in chapter 6. Soooo lets go!\n\n\n\n\n\nJun 10, 2021\n\n\nWayde Gilliam\n\n\n10 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nA Journey Through Fastbook (AJTFB) - Chapter 5: Multiclass classification\n\n\n\n\n\n\nfastai\n\n\nfastbook\n\n\nmulticlass classification\n\n\ncomputer vision\n\n\n\nIts the image classification chapter of \"Deep Learning for Coders with fastai & PyTorch\"! We’ll go over everything you need to know to get started with multiclass classification, from setting up your DataBlock and loss function, to some of the core techniques for evaluating and improving your model’s predictions. So without further adieu, lets go …\n\n\n\n\n\nJun 3, 2021\n\n\nWayde Gilliam\n\n\n11 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nContributing to fastai: Setup your local development environment & submit a PR\n\n\n\n\n\n\nfastai\n\n\ngithub\n\n\nopen source\n\n\npull requests\n\n\n\nA few hours ago I was working on a PR for fastai, and as it has been awhile I realized I couldn’t quite remember all the steps required to do so. Fortunately, I got it figured out pretty quickly and decided I better blog the steps for when I forget next (I am almost 50 after all). So for all you developers looking to contribute to fastai, or really any open source project, here’s everything you need to know to setup your local development environment and submit PRs to fastai. Enjoy!\n\n\n\n\n\nJun 2, 2021\n\n\nWayde Gilliam\n\n\n3 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nMultilingual Sequence Classifaction with the MBart Family\n\n\n\n\n\n\nblurr\n\n\nhuggingface\n\n\nfastai\n\n\nmultilingual\n\n\nsequence classification\n\n\n\nNeed to do some multi-lingual sequence classification? Look no further, at least if you want to use MBart and/or the MBart-50 variety of models. Working against the amazon_reviews_multi dataset I’ll show you how to use the blurr library to configure the huggingface objects, build DataLoaders, and train a model that you can use for classifying German text. I’ll throw in a bit of the inference code so that you can see how easy blurr makes it to use your trained model to boot. Let’s go …\n\n\n\n\n\nMay 25, 2021\n\n\nWayde Gilliam\n\n\n2 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nA Journey Through Fastbook (AJTFB) - Chapter 4: Stochastic Gradient Descent\n\n\n\n\n\n\nfastai\n\n\nfastbook\n\n\n\nThe fourth in a weekly-ish series where I revisit the fast.ai book, \"Deep Learning for Coders with fastai & PyTorch\", and provide commentary on the bits that jumped out to me chapter by chapter. So without further adieu, let’s go!\n\n\n\n\n\nMay 23, 2021\n\n\nWayde Gilliam\n\n\n9 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nA Journey Through Fastbook (AJTFB) - Chapter 3: Data Ethics\n\n\n\n\n\n\nfastai\n\n\nfastbook\n\n\n\nThe third in a weekly-ish series where I revisit the fast.ai book, \"Deep Learning for Coders with fastai & PyTorch\", and provide commentary on the bits that jumped out to me chapter by chapter. So without further adieu, let’s go!\n\n\n\n\n\nNov 22, 2020\n\n\nWayde Gilliam\n\n\n9 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nA Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning\n\n\n\n\n\n\nfastai\n\n\nfastbook\n\n\n\nThe second in a weekly-ish series where I revisit the fast.ai book, \"Deep Learning for Coders with fastai & PyTorch\", and provide commentary on the bits that jumped out to me chapter by chapter. So without further adieu, let’s go!\n\n\n\n\n\nNov 16, 2020\n\n\nWayde Gilliam\n\n\n16 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nA Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning\n\n\n\n\n\n\nfastai\n\n\nfastbook\n\n\n\nThe first in a weekly-ish series where I revisit the fast.ai book, \"Deep Learning for Coders with fastai & PyTorch\", and provide commentary on the bits that jumped out to me chapter by chapter. So without further adieu, let’s go!\n\n\n\n\n\nNov 6, 2020\n\n\nWayde Gilliam\n\n\n25 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nSummarization with blurr\n\n\n\n\n\n\nfastai\n\n\nhuggingface\n\n\nblurr\n\n\nsummarization\n\n\ntext generation\n\n\n\nblurr is a libray I started that integrates huggingface transformers with the world of fastai v2, giving fastai devs everything they need to train, evaluate, and deploy transformer specific models. In this article, I provide a simple example of how to use blurr’s new summarization capabilities to train, evaluate, and deploy a BART summarization model.\n\n\n\n\n\nMay 23, 2020\n\n\nWayde Gilliam\n\n\n4 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nFinding DataBlock Nirvana with fast.ai v2 - Part 1\n\n\n\n\n\n\nfastai\n\n\ndatablock api\n\n\ndata\n\n\npytorch\n\n\n\nThe path to enlightment begins here!\n\n\n\n\n\nApr 11, 2020\n\n\nWayde Gilliam\n\n\n25 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nLoss Functions: Cross Entropy Loss and You!\n\n\n\n\n\nMeet multi-classification’s favorite loss function\n\n\n\n\n\nApr 4, 2020\n\n\nWayde Gilliam\n\n\n4 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the F-Beta metric\n\n\n\n\n\nWhat is F-Beta, how should I use it, and what in the hell is ‘average’ and ‘sample_weight’\n\n\n\n\n\nJan 1, 2019\n\n\nWayde Gilliam\n\n\n4 min\n\n\n6/17/24, 6:27:14 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "",
    "text": "Other posts in this series:\nA Journey Through Fastbook (AJTFB) - Chapter 1\nA Journey Through Fastbook (AJTFB) - Chapter 2\nA Journey Through Fastbook (AJTFB) - Chapter 3\nA Journey Through Fastbook (AJTFB) - Chapter 4\nA Journey Through Fastbook (AJTFB) - Chapter 5\nA Journey Through Fastbook (AJTFB) - Chapter 6a\nA Journey Through Fastbook (AJTFB) - Chapter 6b\nA Journey Through Fastbook (AJTFB) - Chapter 7\nA Journey Through Fastbook (AJTFB) - Chapter 8"
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#tabular-modeling",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#tabular-modeling",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "Tabular Modeling",
    "text": "Tabular Modeling\nWhat is it?\n“Tabular modeling takes data in the form of a table (like a spreadsheet or CSV). The objective is to predict the value of one column based on the values in the other columns.” Tabular data is also called “structured data” while “unstructured data” represents things like text, images, audio, etc…\nWhy is it important?\nThough it is reported that 80-90% of data is unstructured (think images, text, audio), ironically, it appears that the vast majority of “real world” machine learning is concerened with tabular/structured data.\n\n\nIf you are just starting out with Data Science you should know that still the vast majority of DS problems in the industry concern structured/tabular data. This is what you should focus on in order to make a professional inroad. 1/3\n\n— Bojan Tunguz (@tunguz) February 23, 2022\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnd here’s the good news … “recent studies have shown that the vast majority of datasets can be be modeled with just two methods.”\n\n\nWhat are they?\n\nFor structured data, ensembles of decision trees (e.g., random forests and gradient boosting machines like XGBoost).\nFor unstructured data, multilayered neural networks learned with SGD.\n\n\n\n\n\n\n\nNote\n\n\n\n“… ensembles of decision trees tend to train faster, are often easier to interpret, do not require special GPU hardware for inference at scale, and often require less hyperparameter tuning.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSince a “critical step of interpreting a model of tabular data is significantly easier for decesion tree ensembles … ensembles of decision trees are our first approach for analyzing a new tabular dataset”\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNeural networks will considered when “there are some high-cardinality categorical variables” or there are columns with unstructured data. A example of a high “cardinality” (e.g., the number of discrete levels representing the categories) would be something like zip code.\n\n\nSee pages 282-284 for more discussion on the pros/cons of decision trees and neural networks for tabular data."
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#categorical-embeddings",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#categorical-embeddings",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "Categorical Embeddings",
    "text": "Categorical Embeddings\nContinuous v. Categorical\nContinuous variables “contain a real numbers that be fed into a model directly and are meaningful in and out of themselves. Examples include”age” and “price”.\nCategorical variables “contain a number of discrete levels, such as ‘movie ID,’ for which addition and multiplication don’t have any meaning (even if they’re stored as numbers). Other examples include dates, columns indicating”sex”, “gender”, “department”, etc…\nHow do we represent “categorical” data?\nWe learned this in chapter 8, we represent such data via entity embeddings.\n\n\n\n\n\n\nImportant\n\n\n\nEntity embeddings allow for a more complex and learnt numerical representation of a thing. This representation is likely task/data specific to one degree or another such that “Sunday” may have one representation in a task predicting how many hours people work on a day, and another for a task attempting to predict the number of trades that will be executed on each day of the week.\n\n\n\n\n\n\n\n\nNote\n\n\n\n“… by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variable. [It] is especially useful for datasets with lots of high cardinality features.. See pp.278-282 for examples of this in relation to the Rossmann sales competition on kaggle.\n\n\nBecause “an embedding layer is exactly equivalent to placing an ordinary linear layer after every one-hot-encoded input layer … the embedding transforms the categorical variables into inputs that are both continuous and meaningful.”\n\nIn other words …\n\n\n\n\n\n\nImportant\n\n\n\n“… we provide the model fundamentally categorical data about discrete entities … and the model learns an embedding for these entities that defines a continuous notion of distance between them.”\n\n\n\n\n\n\n\n\nNote\n\n\n\nGiven all this, “we can combine our continous embedding values with truly continuous input data [by just concatentating] the variables and feed[ing] the concatenation into our final dense layers. For an example of this, see the “Wide & Deep Learning for Recommender Systems” paper. See the below from page 282 on what that approach looks like."
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#imports",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#imports",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "Imports",
    "text": "Imports\n\nfrom kaggle import api\nfrom dtreeviz.trees import *\nfrom fastai.tabular.all import *\nfrom IPython.display import Image, display_svg, SVG\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 8"
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#data-preparation",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#data-preparation",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "Data preparation",
    "text": "Data preparation\n\nStep 1: Get the data\nWe’ll be getting the data from kaggle. If you’re running on colab, check out these instructions for getting setup with the kaggle API\n\npath = Path(\"bluebook\")\npath\n\nPath('bluebook')\n\n\n\nif not path.exists():\n    path.mkdir()\n    api.competition_download_cli(\"bluebook-for-bulldozers\", path=path)\n    file_extract(path / \"bluebook-for-bulldozers.zip\")\n\npath.ls(file_type=\"text\")\n\nDownloading bluebook-for-bulldozers.zip to bluebook\n\n\n100%|██████████| 48.4M/48.4M [00:00&lt;00:00, 112MB/s] \n\n\n\n\n\n(#7) [Path('bluebook/Machine_Appendix.csv'),Path('bluebook/Test.csv'),Path('bluebook/ValidSolution.csv'),Path('bluebook/median_benchmark.csv'),Path('bluebook/TrainAndValid.csv'),Path('bluebook/Valid.csv'),Path('bluebook/random_forest_benchmark_test.csv')]\n\n\n\n\nStep 2: EDA\n\n\n\n\n\n\nImportant\n\n\n\n“In any sort of data science work, it’s important to look at your data directly to make sure you understand the format, how it’s stored, what types of values it holds, etc.”\n\n\n\n\n\n\n\n\nTip\n\n\n\n“… it’s a good idea to also specify low_memory=False unless Pandas acutally runs out of memory.” The default = True (will look only at the first few rows of data to infer column datatypes).\n\n\n\ntrain_df = pd.read_csv(path / \"TrainAndValid.csv\", low_memory=False)\ntest_df = pd.read_csv(path / \"Test.csv\", low_memory=False)\n\ntrain_df.columns\n\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n\n\ndescribe() is a method that gives you some basic stats for each column.\n\ntrain_df.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nSalesID\n412698.0\n2.011161e+06\n1.080068e+06\n1139246.0\n1421897.75\n1645852.5\n2261012.50\n6333349.0\n\n\nSalePrice\n412698.0\n3.121518e+04\n2.314174e+04\n4750.0\n14500.00\n24000.0\n40000.00\n142000.0\n\n\nMachineID\n412698.0\n1.230061e+06\n4.539533e+05\n0.0\n1088593.25\n1284397.0\n1478079.25\n2486330.0\n\n\nModelID\n412698.0\n6.947202e+03\n6.280825e+03\n28.0\n3261.00\n4605.0\n8899.00\n37198.0\n\n\ndatasource\n412698.0\n1.351694e+02\n9.646749e+00\n121.0\n132.00\n132.0\n136.00\n173.0\n\n\nauctioneerID\n392562.0\n6.585268e+00\n1.715841e+01\n0.0\n1.00\n2.0\n4.00\n99.0\n\n\nYearMade\n412698.0\n1.899050e+03\n2.921902e+02\n1000.0\n1985.00\n1995.0\n2001.00\n2014.0\n\n\nMachineHoursCurrentMeter\n147504.0\n3.522988e+03\n2.716993e+04\n0.0\n0.00\n0.0\n3209.00\n2483300.0\n\n\n\n\n\n\n\nadvanced_describe() is a method I created that builds on top of the default describe() method to include stats on missing and unique values (which are both very helpful in terms of cleanup, understanding potential issues, and in determining the size of your embeddings for categorical data). For categorical variables with few discrete levels, this method will also show you what they are.\n\nadvanced_describe(train_df)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n...\nunique\nunique%\nunique_values\ndtype\n\n\n\n\nSalesID\n412698\n2011161.16364\n1080067.724498\n1139246.0\n...\n412698\n7786.75\nNaN\nint64\n\n\nSalePrice\n412698\n31215.181414\n23141.743695\n4750.0\n...\n954\n18.00\nNaN\nfloat64\n\n\nMachineID\n412698\n1230061.436646\n453953.25795\n0.0\n...\n348808\n6581.28\nNaN\nint64\n\n\nModelID\n412698\n6947.201828\n6280.824982\n28.0\n...\n5281\n99.64\nNaN\nint64\n\n\ndatasource\n412698\n135.169361\n9.646749\n121.0\n...\n6\n0.11\n[121, 132, 136, 149, 172, 173]\nint64\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nBackhoe_Mounting\n80712\nNaN\nNaN\nNaN\n...\n3\n0.06\n[nan, None or Unspecified, Yes]\nobject\n\n\nBlade_Type\n81875\nNaN\nNaN\nNaN\n...\n11\n0.21\n[nan, PAT, None or Unspecified, Semi U, VPAT, Straight, Angle, No, U, Landfill, Coal]\nobject\n\n\nTravel_Controls\n81877\nNaN\nNaN\nNaN\n...\n8\n0.15\n[nan, None or Unspecified, Differential Steer, Lever, Finger Tip, 2 Pedal, Pedal, 1 Speed]\nobject\n\n\nDifferential_Type\n71564\nNaN\nNaN\nNaN\n...\n5\n0.09\n[Standard, nan, Limited Slip, No Spin, Locking]\nobject\n\n\nSteering_Controls\n71522\nNaN\nNaN\nNaN\n...\n6\n0.11\n[Conventional, nan, Command Control, Four Wheel Standard, Wheel, No]\nobject\n\n\n\n\n53 rows × 14 columns\n\n\n\n\n\nStep 3: Preprocessing\n\nHandling Ordinal columns\n\n\n\n\n\n\nTip\n\n\n\n“… a good next step is to handle ordinal columns … columns containing strings or similar, **but where those strings have a natural ordering.”\n\n\n\ntrain_df.ProductSize.unique()\n\narray([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large',\n       'Compact'], dtype=object)\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“tell Pandas about a suitable ordering of these levels”\n\n\n\nsizes = [\"Large\", \"Large / Medium\", \"Medium\", \"Small\", \"Mini\", \"Compact\"]\ntrain_df.ProductSize = train_df.ProductSize.astype(\"category\")\ntrain_df.ProductSize = train_df.ProductSize.cat.set_categories(\n    sizes, ordered=True\n)  # note: \"inplace=True\" is depreciated as of 1.30\ntrain_df.ProductSize.unique()\n\n[NaN, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact']\nCategories (6, object): ['Large' &lt; 'Large / Medium' &lt; 'Medium' &lt; 'Small' &lt; 'Mini' &lt; 'Compact']\n\n\n\n\nHandling Your Dependent Variable(s)\n\n\n\n\n\n\nTip\n\n\n\nUpdate the dependent variable to suit your objective.\n\n\n“You should think carefully about which metric, or set of metrics, actually measures the notion of model quality that matters to you … in this case, Kaggle tells us [our measure is] root mean squared log error (RMLSE)” and because of this we need to make our target the log of the price “so that the m_rmse of that value will give us what we ultimately need.”\n\ndep_var = \"SalePrice\"\ntrain_df[dep_var] = np.log(train_df[dep_var])\n\n\n\nHandling Dates\n\n\n\n\n\n\nImportant\n\n\n\n“… enrich our representation of dates”\n\n\nDates are “different from most ordinal values in that some dates are qualitatively different from others in a way that is often relevant to the systems we are modeling.” As such, we want the model to know if whether the day is a holiday, or part of the weekend, or in a certain month, etc… is important. To do this, “we **replace every date column with a set of date metadata columns, such as holiday, day of week, and month” = categorical data that might be very useful!\nCan use fastai’s add_datepart() function to do this.\n\n\n\n\n\n\nImportant\n\n\n\nApply same preprocessing to both your train/evaluation and test sets!\n\n\n\ntrain_df = add_datepart(train_df, \"saledate\")\ntest_df = add_datepart(test_df, \"saledate\")\n\n\n[col for col in train_df.columns if col.startswith(\"sale\")]\n\n['saleYear',\n 'saleMonth',\n 'saleWeek',\n 'saleDay',\n 'saleDayofweek',\n 'saleDayofyear',\n 'saleIs_month_end',\n 'saleIs_month_start',\n 'saleIs_quarter_end',\n 'saleIs_quarter_start',\n 'saleIs_year_end',\n 'saleIs_year_start',\n 'saleElapsed']\n\n\n\n\nHandling Strings and Missing Data\nFor this we can use fastai’s TabularPandas class (allows us to apply TabularProc transforms to the DataFrame it wraps to do things like fill missing values, make columns categorical, etc…).\nCategorify: “a TabularProc that replaces a column with a numeric categorical column”\nFillMissing: “a TabularProc that replaces missing values with the median of the column, and **creates a new Boolean column that is set to True for any row where the value was missing.  You can change this fill strategy via thefill_strategy` argument.\n\nprocs = [Categorify, FillMissing]"
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#creating-our-tabularpandas",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#creating-our-tabularpandas",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "Creating our TabularPandas",
    "text": "Creating our TabularPandas\n\nStep 1: Define our continuous and categorical columns\nWe need to tell TabularPandas what columns are continumous and which are categorical, and we can use fastai’s cont_cat_split to do that like so …\n\ncont, cat = cont_cat_split(train_df, 1, dep_var=dep_var)\n\n\n\nStep 2: Define our training and validation splits\nWhat is the difference between validation and test sets again?\nRecall that a validation set “is data we hold back from training in order to ensure that the training process does not overfit on the training data” … while a test set “is data that is held back from ourselves in order to ensure that we don’t overfit on the validation data as we export various model architectures and hyperparameters.”\n\n\n\n\n\n\nImportant\n\n\n\n“define our validation data so that it has the same sort of relationship wot the training data as the test set will have.”\n\n\nBecause this is a time series problem, we’ll make the validation set include data for the last 6 months of the full training set, and the training set everything before that. See p.291 for more on this!\n\ncond = (train_df.saleYear &lt; 2011) | (train_df.saleMonth &lt; 10)\ntrain_idxs = np.where(cond)[0]\nvalid_idxs = np.where(~cond)[0]\n\nsplits = (list(train_idxs), list(valid_idxs))\n\n\n\nStep 3: Build our TabularPandas object\nAnd finally, we instantiate our TabularPandas object, passing in our data, procs, splits and dependent variables as such.\n\nto = TabularPandas(train_df, procs, cat, cont, y_names=dep_var, splits=splits)\ntype(to)\n\nfastai.tabular.core.TabularPandas\n\n\n\n\n\n\n\n\nNote\n\n\n\n“A TabularPandas behaves a lot like a fastai Datasets object, including train and valid attributes”\n\n\n\nlen(to.train), len(to.valid)\n\n(404710, 7988)\n\n\n\nto.show(3)\n\n\n\n\n\nUsageBand\nfiModelDesc\nfiBaseModel\nfiSecondaryDesc\nfiModelSeries\nfiModelDescriptor\nProductSize\nfiProductClassDesc\nstate\nProductGroup\nProductGroupDesc\nDrive_System\nEnclosure\nForks\nPad_Type\nRide_Control\nStick\nTransmission\nTurbocharged\nBlade_Extension\nBlade_Width\nEnclosure_Type\nEngine_Horsepower\nHydraulics\nPushblock\nRipper\nScarifier\nTip_Control\nTire_Size\nCoupler\nCoupler_System\nGrouser_Tracks\nHydraulics_Flow\nTrack_Type\nUndercarriage_Pad_Width\nStick_Length\nThumb\nPattern_Changer\nGrouser_Type\nBackhoe_Mounting\nBlade_Type\nTravel_Controls\nDifferential_Type\nSteering_Controls\nsaleIs_month_end\nsaleIs_month_start\nsaleIs_quarter_end\nsaleIs_quarter_start\nsaleIs_year_end\nsaleIs_year_start\nauctioneerID_na\nMachineHoursCurrentMeter_na\nSalesID\nMachineID\nModelID\ndatasource\nauctioneerID\nYearMade\nMachineHoursCurrentMeter\nsaleYear\nsaleMonth\nsaleWeek\nsaleDay\nsaleDayofweek\nsaleDayofyear\nsaleElapsed\nSalePrice\n\n\n\n\n0\nLow\n521D\n521\nD\n#na#\n#na#\n#na#\nWheel Loader - 110.0 to 120.0 Horsepower\nAlabama\nWL\nWheel Loader\n#na#\nEROPS w AC\nNone or Unspecified\n#na#\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n2 Valve\n#na#\n#na#\n#na#\n#na#\nNone or Unspecified\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nStandard\nConventional\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139246\n999089\n3157\n121\n3.0\n2004\n68.0\n2006\n11\n46\n16\n3\n320\n1.163635e+09\n11.097410\n\n\n1\nLow\n950FII\n950\nF\nII\n#na#\nMedium\nWheel Loader - 150.0 to 175.0 Horsepower\nNorth Carolina\nWL\nWheel Loader\n#na#\nEROPS w AC\nNone or Unspecified\n#na#\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n2 Valve\n#na#\n#na#\n#na#\n#na#\n23.5\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nStandard\nConventional\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139248\n117657\n77\n121\n3.0\n1996\n4640.0\n2004\n3\n13\n26\n4\n86\n1.080259e+09\n10.950807\n\n\n2\nHigh\n226\n226\n#na#\n#na#\n#na#\n#na#\nSkid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity\nNew York\nSSL\nSkid Steer Loaders\n#na#\nOROPS\nNone or Unspecified\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nAuxiliary\n#na#\n#na#\n#na#\n#na#\n#na#\nNone or Unspecified\nNone or Unspecified\nNone or Unspecified\nStandard\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\n#na#\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n1139249\n434808\n7009\n121\n3.0\n2001\n2838.0\n2004\n2\n9\n26\n3\n57\n1.077754e+09\n9.210340\n\n\n\n\n\n\nto.items.head(3)\n\n\n\n\n\n\n\n\nSalesID\nSalePrice\nMachineID\nModelID\n...\nsaleIs_year_start\nsaleElapsed\nauctioneerID_na\nMachineHoursCurrentMeter_na\n\n\n\n\n0\n1139246\n11.097410\n999089\n3157\n...\n1\n1.163635e+09\n1\n1\n\n\n1\n1139248\n10.950807\n117657\n77\n...\n1\n1.080259e+09\n1\n1\n\n\n2\n1139249\n9.210340\n434808\n7009\n...\n1\n1.077754e+09\n1\n1\n\n\n\n\n3 rows × 67 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs you can see above, the underlying string values (the levels) have been replaced with a unique number associated to that level.\n\n\nWe can see that mapping via the classes attribute:\n\nto.classes[\"ProductSize\"]\n\n['#na#', 'Large', 'Large / Medium', 'Medium', 'Small', 'Mini', 'Compact']\n\n\n\n\n\n\n\n\nTip\n\n\n\nSave your preprocessed TabularPandas object so you don’t have to process the data again\n\n\n\n# save to filesystem\nsave_pickle(path / \"to.pkl\", to)\n\n# load from filesystem\nto = load_pickle(path / \"to.pkl\")"
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#approach-1-decision-trees",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#approach-1-decision-trees",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "Approach 1: Decision Trees",
    "text": "Approach 1: Decision Trees\n“A decision tree asks a series of binary (yes or no) questions about the data. After each question, the data at that part of the tree is split between a Yes and a No branch…. After one or more questions, either a prediction can be made on the basis of all previous answers or another question is required.”\n“… for regression, we take the target mean of the items in the group”\n\n\n\n\n\n\nNote\n\n\n\nSee p.288 for how we find the right questions to ask.\n\n\n\nStep 1: Define independent/dependent variables\nAfter this, our data is completely numeric and there are no missing values!\n\ntrain_xs, train_y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y\n\n\n\nStep 2: Build our tree\nmax_leaf_nodes=4 = “Stop when you have 4 leaf nodes”\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(train_xs, train_y)\n\nDecisionTreeRegressor(max_leaf_nodes=4)\n\n\n\ndraw_tree(m, train_xs, size=7, leaves_parallel=True, precision=2)\n\n\n\n\n\n\n\n\nWhat is in each box above?:\n\nThe decision criterion for the best split that was found\n“samples”: The # of examples in the group\n“value”: The average value of the target for that group\n“squared_error”: The MSE for that group\n\n\n\n\n\n\n\nNote\n\n\n\n“The top node represents the initial model before any splits have been done, when all the data is in one group. This is the simplest possible model. It is the result of asking zero questions and will always predict the value to be the average value of the whole dataset.”\n\n\nA “leaf node” is a node “with no answers coming out of them, because there are no more questions to be answered.”\nSee p.293 for more on intrepreting the diagram above.\n\nsamp_idx = np.random.permutation(len(train_y))[:500]\n\ndtreeviz(\n    m,\n    train_xs.iloc[samp_idx],\n    train_y.iloc[samp_idx],\n    train_xs.columns,\n    dep_var,\n    fontname=\"DejaVu Sans\",\n    scale=1.6,\n    label_fontsize=10,\n    orientation=\"LR\",\n)\n\n/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n  \"X does not have valid feature names, but\"\n\n\n\n\n\n\n\n\n\n“This shows a cart of the distribution of the data for each split point”\n\n\n\n\n\n\nTip\n\n\n\nUse dtreeviz to find problems with the data.\n\n\nFor example, you can see there is a problem with “YearMade” as a bunch of tractors show they are made in the year 1000. The likely explanation is that if they don’t have the info on a tractor, they set it = 1000 to indicate “Unknown”.\nWe can replace this with something like 1950 to make the visuals more clear …\n\ntrain_xs.loc[train_xs[\"YearMade\"] &lt; 1900, \"YearMade\"] = 1950\nvalid_xs.loc[valid_xs[\"YearMade\"] &lt; 1900, \"YearMade\"] = 1950\n\n\nsamp_idx = np.random.permutation(len(train_y))[:500]\n\ndtreeviz(\n    m,\n    train_xs.iloc[samp_idx],\n    train_y.iloc[samp_idx],\n    train_xs.columns,\n    dep_var,\n    fontname=\"DejaVu Sans\",\n    scale=1.6,\n    label_fontsize=10,\n    orientation=\"LR\",\n)\n\n/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n  \"X does not have valid feature names, but\"\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Build other trees\n\nm = DecisionTreeRegressor()\nm.fit(train_xs, train_y)\n\nDecisionTreeRegressor()\n\n\n\n\n\n\n\n\nImportant\n\n\n\nCreate/Use the metric used by the competition (or your work)\n\n\n\ndef r_mse(preds, targs):\n    return round(math.sqrt(((preds - targs) ** 2).mean()), 6)\n\n\ndef m_rmse(m, xs, y):\n    return r_mse(m.predict(xs), y)\n\n\nm_rmse(m, train_xs, train_y)\n\n0.0\n\n\n\nm_rmse(m, valid_xs, valid_y)\n\n0.333111\n\n\nWhat does the above indicate?\nThat we might be overfitting … badly (because our training set is perfect and our validation set not so much).\nWhy are we overfitting?\nBecause “we have nearly as many leaf nodes as data points … sklearn’s default settings allow it to continue splitting nodes until there is only one item in each leaf node.” See pp.295-296 for more intuition on why trees overfit.\n\nm.get_n_leaves(), len(train_xs)\n\n(324559, 404710)\n\n\n\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\n\nm_rmse(m, to.train.xs, to.train.y), m_rmse(m, to.valid.xs, to.valid.y)\n\n(0.211677, 0.268059)\n\n\nmin_samples_leaf=25 = “Stop when all leaf nodes have a minimum of 25 samples”\n\n\nA note on categorical variables\nDecision trees **don’t have embedding layers - “so how can these untreated categorical variables do anything useful”?\nAnswer: “It just works!”\n\n\n\n\n\n\nNote\n\n\n\nWhile it is possible to replace a categorical variable with multiple OHE columns using something like get_dummies, “there is not really any evidence that such an approach improves the end result.”\n\n\nSee p.297 for more about why OHE aren’t necessary and why decision tree just work with categorical variables out-of-the-box."
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#approach-2-random-forests",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#approach-2-random-forests",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "Approach 2: Random Forests",
    "text": "Approach 2: Random Forests\nA random forest “is a model that averages the predictions of a large number of decision trees, which are generated by randomly varying various parameters that specify what data is used to train the tree (what columns and rows are included in each tree) and other tree parameters”\nWhy does it work so well?\nBecause it utlizes bagging.\nWhat is “bagging”?\nFrom the “Bagging Predictors” paper … “Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor…. The multiple versions are formed by making bootstrap replicates (a randomly chosen subset of rows) of the learning set.”\n\n\n\n\n\n\nNote\n\n\n\n“… although each of the models trained on a subset of data will make more errors than a model trained on the full dataset, those errors will not be correlated with each other. **Different models make different errors … the average of those errors, therefore, is zero!”\n\n\nThis means that we can improve the performance of a model by training it multiple times with a different random subset of the data each time, and then averaging the predictions.\n\n\n\n\n\n\nNote\n\n\n\nEnsembling is a form of bagging\n\n\nSee p.298 for more details on how bagging works.\n\nStep 1: Define your Random Forest\nSince we need to define a variety of parameters that indicate the number of trees we want, how subsets of rows should be randomly chosen, how subsets of columns should likewise be randomly chosen, etc., we’ll put the creation behind a function we can call.\n\ndef fit_rf(\n    xs,\n    y,\n    n_estimators=40,\n    max_samples=200_000,\n    max_features=0.5,\n    min_samples_leaf=5,\n    **kwargs,\n):\n    return RandomForestRegressor(\n        n_jobs=-1,  # Tells sklearn to use all our CPUs to build the trees in parallel\n        n_estimators=n_estimators,  # The number of trees\n        max_samples=max_samples,  # The number of rows to sample for training ea. tree\n        max_features=max_features,  # The number of columns to sample at each split\n        min_samples_leaf=min_samples_leaf,  # Stop when all leaf nodes have at least this number of samples\n        oob_score=True,\n    ).fit(xs, y)\n\n\nm = fit_rf(train_xs, train_y)\n\n\nm_rmse(m, train_xs, train_y), m_rmse(m, valid_xs, valid_y)\n\n(0.170771, 0.232215)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRandom Forests are very sensitive to hyperparameter choices!\n\n\nRecommended hyperparameter values:\n\nn_estimators: “as high a number as you have time to train … more trees = more accurate\nmax_samples: default (200,000)\nmax_features: default (“auto”) or 0.5\nmin_samples_leaf: default (1) or 4\n\n\n\n\n\n\n\nTip\n\n\n\nBigger forests using a smalle subset of features tens to be better (see chart below)\n\n\n\nHow to get the predictions for a SINGLE tree?\n\ntree_preds = np.stack(\n    [t.predict(valid_xs.values) for t in m.estimators_]\n)  # added .values (see: https://stackoverflow.com/a/69378867)\n\nr_mse(tree_preds.mean(0), valid_y)\n\n0.232215\n\n\nHow does n_estimators impact model performance?\nTo answer this, we can increment the number of trees we use in our predictions one at a time like so:\n\nplt.plot([r_mse(tree_preds[: i + 1].mean(0), valid_y) for i in range(40)])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse the above technique to determine a good range of trees to try.\n\n\n\n\nStep 2: Determine why our validation set is worse than training\n\n\n\n\n\n\nTip\n\n\n\nUse the out-of-bag (OOB) error to determine if we’re overfitting, or if the validation set covers a different time period, or if it’s a bit of both.\n\n\n“The OOB error is a way of measuring prediction error in the training dataset” based on rows not used in the training of a particular tree. “This allows us to see whether the model is overfitting, without needing a separate validation set.”\n“… out-of-bag error is a little like imagining that every tree therefore also has its own validation set” based on the prediction of rows not used in its training.\n\n\n\n\n\n\nImportant\n\n\n\n“This is particularly beneficial in cases where we have only a small amount of training data” (we don’t necessarily have to remove items to create a validation set).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf your OOB error is &lt;&lt; than our validation set error, “something else is causing the error”\n\n\n\n\n\n\n\n\nNote\n\n\n\n“we compoare [OOB predictions] to our training labels”since this is being calculated on trees using the training set.”\n\n\n\nr_mse(m.oob_prediction_, train_y)\n\n0.210601\n\n\n\n\n\nModel Interpretation\n\nHow confident are we in our predictions using a particular row of data?\nAnswer: “use the standard deviation of predictions across the trees, instead of just the mean. This tells us the relative confidence of predictions”\n\n\n\n\n\n\nImportant\n\n\n\n“…be more cautious of using the reulsts for rows where trees give very different results (higher standard deviations)”\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis information is helpful in production where “if you were using this model to decide which items to bid on at auction, a low-confidence prediction might cause you to look more carefully at an item before you made a bid.”\n\n\n\npreds = np.stack([t.predict(valid_xs.values) for t in m.estimators_])\npreds.shape  # =&gt; (# of trees, # of predictions)\n\n(40, 7988)\n\n\n\npreds_std = preds.std(0)  # get rid of first dimension (the trees)\n\n\n# std for first 5 rows of validation set\npreds_std[:5]\n\narray([0.25911739, 0.08550421, 0.11939131, 0.29835108, 0.16490343])\n\n\n\n\nWhich columns are the strongest predictors (and which can we ignore)?\n“It’s not normally enough to just know that a model can make accurate predictions - we also want to know how it’s making predictions”\nAnswer: “feature importances give us this insight.”\n\ndef rf_feature_importance(m, df):\n    return pd.DataFrame(\n        {\"cols\": df.columns, \"imp\": m.feature_importances_}\n    ).sort_values(\"imp\", ascending=False)\n\n\ndef plot_fi(fi_df):\n    return fi_df.plot(\"cols\", \"imp\", \"barh\", figsize=(12, 7), legend=False)\n\n\nfi_df = rf_feature_importance(m, train_xs)\n\n# Let's look at the 10 most important features\nfi_df[:10]\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n57\nYearMade\n0.186662\n\n\n6\nProductSize\n0.133412\n\n\n30\nCoupler_System\n0.098091\n\n\n7\nfiProductClassDesc\n0.071924\n\n\n32\nHydraulics_Flow\n0.064008\n\n\n65\nsaleElapsed\n0.050607\n\n\n54\nModelID\n0.049313\n\n\n3\nfiSecondaryDesc\n0.045434\n\n\n1\nfiModelDesc\n0.033899\n\n\n31\nGrouser_Tracks\n0.026686\n\n\n\n\n\n\n\n\nplot_fi(fi_df[:30])\n\n\n\n\n\n\n\n\nSee p.304 for how feature importance is calculated.\n\n\n\n\n\n\nImportant\n\n\n\nBy removing variables of low importance we can still get good results while making our model simpler, more interpretable, and easier to maintain\n\n\n\ncols_to_keep = fi_df[fi_df.imp &gt; 0.005].cols\nlen(cols_to_keep)\n\n21\n\n\n\ntrain_xs_keep = train_xs[cols_to_keep]\nvalid_xs_keep = valid_xs[cols_to_keep]\n\n\nm = fit_rf(train_xs_keep, train_y)\n\n\nm_rmse(m, train_xs_keep, train_y), m_rmse(m, valid_xs_keep, valid_y)\n\n(0.181366, 0.231985)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nCompare accuracy of full model with column subset to ensure equitable performance!\n\n\nThe accuracy is about the same as before, but the model is much more interpretable …\n\nlen(train_xs.columns), len(train_xs_keep.columns)\n\n(66, 21)\n\n\n\nplot_fi(rf_feature_importance(m, train_xs_keep[:30]))\n\n\n\n\n\n\n\n\n\n\nWhich columns are effectively redundant?\n\n\n\n\n\n\nImportant\n\n\n\nLike removing unimportant features, by removing redundant information, we make our model simpler, more interpretable, and easier to maintain\n\n\n\ncluster_columns(train_xs_keep)\n\n\n\n\n\n\n\n\nHow do we determine similarity?\n\n\n\n\n\n\nImportant\n\n\n\n“… the pairs of columns that are most similar are the ones that were merged together early”\n\n\n“The most similar paris are found by calculating the rank correlation, which means that all the values are replaced with their rank … and then the correlation is calculated.\nHow do we know if a feature is “redundant” and can be dropped?\nAnswer: By removing potentially redundant variables one at a time\n“We don’t need [the model] to be very accurate” so we can use a lower max_samples and a higher min_samples_leaf. We just want to see the effect it has by removing certain columns using the OOB score (“a number returned by sklearn that ranges between 1.0 for a perfect model and 0.0 for a random model.”\n\ndef get_oob(df, y):\n    m = RandomForestRegressor(\n        n_estimators=10,\n        min_samples_leaf=15,\n        max_samples=50_000,\n        max_features=0.5,\n        n_jobs=-1,\n        oob_score=True,\n    )\n    m.fit(df, y)\n    return m.oob_score_\n\nOur baseline:\n\nget_oob(train_xs_keep, train_y)\n\n0.8723047876078355\n\n\nTry removing redundant features one at a time:\n\nredundant_cols = [\n    \"saleYear\",\n    \"saleElapsed\",\n    \"ProductGroupDesc\",\n    \"ProductGroup\",\n    \"fiModelDesc\",\n    \"fiBaseModel\",\n    \"Hydraulics_Flow\",\n    \"Grouser_Tracks\",\n    \"Coupler_System\",\n]\n\n{c: get_oob(train_xs_keep.drop(c, axis=1), train_y) for c in redundant_cols}\n\n{'Coupler_System': 0.8729675628149272,\n 'Grouser_Tracks': 0.8735142839769211,\n 'Hydraulics_Flow': 0.8730586366090629,\n 'ProductGroup': 0.8731366740450576,\n 'ProductGroupDesc': 0.8728712200071638,\n 'fiBaseModel': 0.87126582392193,\n 'fiModelDesc': 0.8714879359004835,\n 'saleElapsed': 0.8681954238416791,\n 'saleYear': 0.8713304844511609}\n\n\nNow try dropping multiple variables (one from each of the tightly aligned pairs we noticed above), and compare accuracy:\n\ncols_to_drop = [\"saleYear\", \"ProductGroupDesc\", \"fiBaseModel\", \"Grouser_Tracks\"]\n\nget_oob(train_xs_keep.drop(cols_to_drop, axis=1), train_y)\n\n0.8695950962336326\n\n\nAssuming that its not much worse, create new DataFrames and save …\n\ntrain_xs_final = train_xs_keep.drop(cols_to_drop, axis=1)\nvalid_xs_final = valid_xs_keep.drop(cols_to_drop, axis=1)\n\n# save to filesystem\nsave_pickle(path / \"train_xs_final.pkl\", train_xs_final)\nsave_pickle(path / \"valid_xs_final.pkl\", valid_xs_final)\n\n… then load them back in later\n\ntrain_xs_final = load_pickle(path / \"train_xs_final.pkl\")\nvalid_xs_final = load_pickle(path / \"valid_xs_final.pkl\")\n\n\nm = fit_rf(train_xs_final.values, train_y)\nm_rmse(m, train_xs_final.values, train_y), m_rmse(m, valid_xs_final.values, valid_y)\n\n(0.183291, 0.232867)\n\n\n\n\nHow do find the relationship between two predictors (columns)?\n\n\n\n\n\n\nImportant\n\n\n\nDo this for the most important predictors!\n\n\nFirst, check the count of values per category:\n\np = valid_xs_final[\"ProductSize\"].value_counts().sort_index().plot.barh()\nc = to.classes[\"ProductSize\"]\nplt.yticks(range(len(c)), c)\n\n([&lt;matplotlib.axis.YTick at 0x7f6564704d10&gt;,\n  &lt;matplotlib.axis.YTick at 0x7f6562ce8350&gt;,\n  &lt;matplotlib.axis.YTick at 0x7f6562b34d10&gt;,\n  &lt;matplotlib.axis.YTick at 0x7f65645b61d0&gt;,\n  &lt;matplotlib.axis.YTick at 0x7f656473b310&gt;,\n  &lt;matplotlib.axis.YTick at 0x7f656473bd90&gt;,\n  &lt;matplotlib.axis.YTick at 0x7f656473b250&gt;],\n [Text(0, 0, '#na#'),\n  Text(0, 0, 'Large'),\n  Text(0, 0, 'Large / Medium'),\n  Text(0, 0, 'Medium'),\n  Text(0, 0, 'Small'),\n  Text(0, 0, 'Mini'),\n  Text(0, 0, 'Compact')])\n\n\n\n\n\n\n\n\n\n\nx = valid_xs_final[\"YearMade\"].hist()\n\n\n\n\n\n\n\n\nSecond, built a partial dependence plot\n\n\n\n\n\n\nNote\n\n\n\n“Partial dependence plots answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable?\n\n\nSee pp.309-10 for info on how to do this, but in essence, we are attempting to isolate the effects of changes to a single variable.\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\nfig, ax = plt.subplots(figsize=(12, 4))\nPartialDependenceDisplay.from_estimator(\n    m, valid_xs_final, [\"YearMade\", \"ProductSize\"], grid_resolution=20, ax=ax\n)\n\n\n\n\n\n\n\n\nIntrepretation\nThe “YearMade” plot makes sense as it shows a linear relationship with price (“remember that our dependent variable is after taking the logarithm, so this means that in practice there is an exponential increase in price). Again, this is expected given that depreciation =”a multiplicative factor over time … varying year made ought to show an exponential relationship to sales price”\nThe “ProductSize” should raise concerns, “it shows that for the final group, which we saw is for missing values, has the lowest price.\n\n\n\n\n\n\nImportant\n\n\n\nIf you see an unusual relationship like this between a variable and the target, you want to understand why there are missing values and what a missing value means!\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMissing values can sometimes be useful predictors\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMissing values can sometimes indicate data leakage.\n\n\nWhat is “data leakage”?\n“The introduction of information about the target of a data mining problem, which should not be legitimately available to mine from.” The idea is that you’re cheating by introducing something by way of the independent variables that biases your predictions favorably that you won’t be able to utlize at inference time. See p.311-12 for examples.\nHow to determine if you have data leakage?\nFrom p.321 … “build a model and then …\n\nCheck whether the accuracy of the model is too good to be true.\nLook for important predictors that don’t make sense in pratice.\nLook for partial dependence plot results that don’t make sense in practice. (see above)\n\n\n\n\n\n\n\nTip\n\n\n\n“… often a good idea to build a model first and then do you data cleaning rather than vice versa … can help you identify potentially problematic data issues.\n\n\n\n\nFor predicting a specific row of data, what were the most important factors and how did they influence the prediction?\nAnswer: Use treeinterpreter. Note that:\nprediction = the prediction made by RF\nbias = the prediction based on taking the mean of the dependent variable (the root model for every tree)\ncontributions = “tells us the total change in prediction due to each of the independent variables.\n“Therfore, the sum of contributions plus bias must equal the prediction for each row!\n\nfrom treeinterpreter import treeinterpreter\n\n\nrow = valid_xs_final.iloc[:5]\n\n\npred, bias, contributions = treeinterpreter.predict(m, row.values)\n\n\npred[0], bias[0], contributions[0].sum()\n\n(array([10.00298786]), 10.104457702350853, -0.10146984568626016)\n\n\nWhat is the best waty to visualize this?\nAnswer: A waterfull plot will show us “how the positive and negative contributions from all the independent variables sum up to create the final prediction.”\n\nimport waterfall_chart\n\n\nwaterfall_chart.plot(\n    valid_xs_final.columns,\n    contributions[0],\n    threshold=0.08,\n    rotation_value=45,\n    formatting=\"{:,.3f}\",\n)\n\n\n\n\n\n\n\n\nWhere is this information useful?\nAnswer: In production “rather than during model development. You can use it to provide useful information to users of your data product about underlying reasoning behind the predictions.”\n\n\n\n\nThe “Extrapolation” problem\n\n\n\n\n\n\nImportant\n\n\n\n“Random forests are not able to extrapolate the types of data they have seen … that’s why we need to make sure our validation set does not contina out-of-domain data.”\n\n\nSee. pp.315-316.\nHow do determine if our “test set” is distributed in the same way as our “training set”, and if it isn’t, what columns are causing the difference?\nAnswer: Use a random forest where we “try to predict whether a row is in the validation set or training set.”\n\ncombined_df = pd.concat([train_xs_final, valid_xs_final])\nis_valid = np.array([0] * len(train_xs_final) + [1] * len(valid_xs_final))\n\n\nood_m = fit_rf(combined_df, is_valid)\n\n\nrf_feature_importance(ood_m, combined_df)[:6]\n\n\n  \n    \n      \n\n\n\n\n\n\ncols\nimp\n\n\n\n\n5\nsaleElapsed\n0.900343\n\n\n9\nSalesID\n0.073015\n\n\n14\nMachineID\n0.023312\n\n\n0\nYearMade\n0.001036\n\n\n6\nModelID\n0.000405\n\n\n10\nEnclosure\n0.000402\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n“This shows that three columns differ significantly between the training and validation sets: saleElapsed, SalesID, and MachineID’\nSee p.317 for infering what this is.\nWhat to do with this info?\nAnswer: Compare the effects of removing each of these columns with the original model\n\nm = fit_rf(train_xs_final, train_y)\nprint(\"orig:\", m_rmse(m, valid_xs_final, valid_y))\n\norig: 0.231632\n\n\n\nfor c in (\"SalesID\", \"saleElapsed\", \"MachineID\"):\n    m = fit_rf(train_xs_final.drop(c, axis=1), train_y)\n    print(c, m_rmse(m, valid_xs_final.drop(c, axis=1), valid_y))\n\nSalesID 0.231139\nsaleElapsed 0.235471\nMachineID 0.229936\n\n\n“… looks like we should be able to remove SaleID and MachineID without losing any accuracy”\n\ntime_vars = [\"SalesID\", \"MachineID\"]\ntrain_xs_final_time = train_xs_final.drop(time_vars, axis=1)\nvalid_xs_final_time = valid_xs_final.drop(time_vars, axis=1)\n\n\nm = fit_rf(train_xs_final_time, train_y)\nm_rmse(m, valid_xs_final_time, valid_y)\n\n0.228524\n\n\n\n\n\n\n\n\nNote\n\n\n\n“Removing these variables has slightly improved the model’s accuracy; but more importantly, it should make it more resilient over time, and eaiser to maintain and understand.”\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBuild a model where the dependent variable is is_valid for ALL datasets!\n\n\n“It can often uncovere subtle domain shift issues that you may otherwise miss.\n\n\n\n\n\n\nImportant\n\n\n\n“One thing that might help in our case is simply to avoid using old data”\n\n\n“Often, old data shows relationships that just aren’t valid anymore”\n\n\n\n\n\n\nImportant\n\n\n\nThis “shows you shouldn’t always use your entire dataset; sometimes a subset can be better.”\n\n\n\ntrain_xs[\"saleYear\"].hist()\n\n\n\n\n\n\n\n\n\nrecent_sales = train_xs[\"saleYear\"] &gt; 2004\n\ntrain_xs_recent = train_xs_final_time[recent_sales]\ntrain_y = train_y[recent_sales]\n\n\nm = fit_rf(train_xs_recent, train_y)\nm_rmse(m, train_xs_recent, train_y), m_rmse(m, valid_xs_final_time, valid_y)\n\n(0.177533, 0.229519)"
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#approach-3-neural-networks",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#approach-3-neural-networks",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "Approach 3: Neural Networks",
    "text": "Approach 3: Neural Networks\n\n\n\n\n\n\nTip\n\n\n\n“We can leverage the work we did to trim unwanted columns in the random forest by using the same set of columns”\n\n\n\nnn_df = pd.read_csv(path / \"TrainAndValid.csv\", low_memory=False)\n\nsizes = [\"Large\", \"Large / Medium\", \"Medium\", \"Small\", \"Mini\", \"Compact\"]\nnn_df.ProductSize = nn_df.ProductSize.astype(\"category\")\nnn_df.ProductSize = nn_df.ProductSize.cat.set_categories(\n    sizes, ordered=True\n)  # note: \"inplace=True\" is depreciated as of 1.30\n\nnn_df[dep_var] = np.log(nn_df[dep_var])\n\nnn_df = add_datepart(nn_df, \"saledate\")\n\n\nnn_df_final = nn_df[list(train_xs_final_time.columns) + [dep_var]]\n\n\n\n\n\n\n\nImportant\n\n\n\nCategorical columns require embeddings.\n\n\nfastai uses the max_card to determine what columns should be treated as categoricals … “Embedding sizes larger that 10,000 should generaly be used only after you’ve tested whether there are better ways to group the variable”\n\nnn_cont, nn_cat = cont_cat_split(nn_df_final, max_card=9000, dep_var=dep_var)\n\n\n\n\n\n\n\nImportant\n\n\n\n“A categorical variable cannot, by definition, extrapolate outside the range of values that it has seen, **but we want to be able to predict auction sale prices in the future”\n\n\nTherfore, “saleElapsed” needs to be made into a continous variable … which it alread is\n\nnn_cont\n\n['saleElapsed']\n\n\nHow to find the cardinality of each categorical?\n\nnn_df_final[nn_cat].nunique()\n\nYearMade                73\nProductSize              6\nCoupler_System           2\nfiProductClassDesc      74\nHydraulics_Flow          3\nModelID               5281\nfiSecondaryDesc        177\nfiModelDesc           5059\nEnclosure                6\nHydraulics              12\nProductGroup             6\nfiModelDescriptor      140\nTire_Size               17\nDrive_System             4\ndtype: int64\n\n\n“… two variables pertaining to the ‘model’ of the equipment, both with similar very high cardinalities, suggests that they may contain similar, redundant information.”\n\n\n\n\n\n\nImportant\n\n\n\nRemoving rendundant columns can make your model more efficient! Try and see what the impact is.\n\n\n\ncols_to_drop = [\"fiModelDescriptor\"]\ntrain_xs_recent2 = train_xs_recent.drop(cols_to_drop, axis=1)\nvalid_xs_recent2 = valid_xs_final_time.drop(cols_to_drop, axis=1)\n\nm2 = fit_rf(train_xs_recent2, train_y)\nm_rmse(m2, train_xs_recent2, train_y), m_rmse(m2, valid_xs_recent2, valid_y)\n\n(0.176776, 0.230194)\n\n\n\nnn_cat.remove(\"fiModelDescriptor\")\n\n\nprint(nn_cont)\nprint(nn_cat)\n\n['saleElapsed']\n['YearMade', 'ProductSize', 'Coupler_System', 'fiProductClassDesc', 'Hydraulics_Flow', 'ModelID', 'fiSecondaryDesc', 'fiModelDesc', 'Enclosure', 'Hydraulics', 'ProductGroup', 'Tire_Size', 'Drive_System']\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNeural networks require normalization while random forest does not because neural networks care about scaling!\n\n\n\nnn_procs = [Categorify, FillMissing, Normalize]\n\nnn_to = TabularPandas(\n    nn_df_final, nn_procs, nn_cat, nn_cont, splits=splits, y_names=dep_var\n)\n\n\n\n\n\n\n\nNote\n\n\n\n“Tabular models and data don’t generally require much GPU RAM, so we can use larger batch sizes”\n\n\n\ndls = nn_to.dataloaders(bs=1024)\n\n\n\n\n\n\n\nTip\n\n\n\n“it’s a good idea to set y_range for regression models\n\n\n\ny = nn_to.train.y\ny.min(), y.max()\n\n(8.465899467468262, 11.863582611083984)\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe default number of hidden layers (200,100) work well for small datasets, but should be increased if you are working with a larger dataset (500,250) for example.\n\n\n\nfrom fastai.tabular.all import *\n\nlearn = tabular_learner(\n    dls, y_range=(8, 12), layers=[500, 250], n_out=1, loss_func=F.mse_loss\n)\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.00015848931798245758)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is no need to fine tune since we aren’t starting with a pretrained model.\n\n\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.062468\n0.063260\n00:15\n\n\n1\n0.053739\n0.074410\n00:15\n\n\n2\n0.047957\n0.055398\n00:15\n\n\n3\n0.042492\n0.050278\n00:15\n\n\n4\n0.039967\n0.050096\n00:14\n\n\n\n\n\n\npreds, targs = learn.get_preds()\nr_mse(preds, targs)\n\n\n\n\n\n\n\n\n0.223821\n\n\n\nlearn.save(\"nn\")\n\nPath('models/nn.pth')"
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#approach-4-boosting",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#approach-4-boosting",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "Approach 4: Boosting",
    "text": "Approach 4: Boosting\nHere “we add models instead of averaging them” (as compared to ensembling). See p.323-24 for all the details, but here’s how it works in general:\n\nTrain small model that underfits\nGet predictions for model from #1\nGet the “residuals” (‘the error for each point in the training set’) by subtracting the predictions from the targets.\nGo back to step 1, “but instead of using the original targets, use the residuals as the targets for training.\nContinue stesp 1-4 until you need to stop (e.g., max number of trees, you start overfitting, etc…)\n\nSteps 3-4 are the bread and butter of things.\n\n\n\n\n\n\nImportant\n\n\n\n“… each new tree will be attempting to fit the error of all of the previous trees combined. Because we are continually creating new residuals by subtracting the predictions of each new tree from the residulas from the previous tree, the residuals will get smaller and smaller.”\n\n\nSome go as far as saying that such models are all you need :)\n\n\nXGBoost Is All You NeedDeep Neural Networks and Tabular Data: A Surveyhttps://t.co/Z2KsHP3fvp pic.twitter.com/uh5NLS1fVP\n\n— Bojan Tunguz (@tunguz) March 30, 2022\n\n\nHow do you make predictions with an ensemble of boosted trees?\nAnswer: By calculating the predictions from each tree and then adding them all together.\n\n\n\n\n\n\nNote\n\n\n\n“… unlike with random forests, with this approach, there is nothing to stop us from overfitting …. in a boosted ensemble, the more trees you have, the better the training error becomes, and eventually you will see overfitting on the validation set.”\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“Unlike random forests, gradient boosted trees are extremely sensitive to the choices of these hyperparameters.”\n\n\nXGBoost and sklearn’s HistGradientBooting models are legit ML models to try …\n\n\nHere are my top used ML algos for tabular data problems:1. XGBoost2. HistGradientBoosting3. Logistic regression/ Ridge regression4. LightGBM5. MLP6. A blend of 1. And 5.\n\n— Bojan Tunguz (@tunguz) April 22, 2022"
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#other-things-to-try",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#other-things-to-try",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "Other things to try",
    "text": "Other things to try\nEnsembling (pp.322-23)\nThis is a form of “bagging” where averaging the predictions of models trained using different algorithms, each of which it is reasonable to expect would produce differnt kinds of errors, would likely product better predictions.\nCombining embeddings with other models (pp.324-25)\n“… shows that you can get much of the performance improvement of a neural network without having to use a neural network as inference time” by using the embeddings as inputs (which are just array lookups) to small decision tree ensemble!"
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#in-summary",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#in-summary",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "In summary",
    "text": "In summary\n\n\n\n\n\n\nTip\n\n\n\nStart with a random forest!\n\n\n“This will give you a strong baseline … can then use that model for feature selection and partial dependence analysis to get a better understanding of your data.”\nPros/Cons of each approach:\nRandom forests: Pros:\n\nEasiest to train\nResiliant to hyperparameter choices\nRequire little preprocessing\nFast to train\nShould not overfit if you have enough trees Cons:\nCan be less accurate (esp. if extrapolation is required “such as predicting future time periods.”\n\nGradient boosting machines (GBMs): Pros:\n\nOften more accurate than random forests Cons:\nSensitive to hyperparmeter choices\nCan overfit\n\nNeural Networks: Pros:\n\nCan extrapolate well\nCan provide good results. Cons:\nTake longest time to train\nRequire extra preprocessing (e.g., normalization)"
  },
  {
    "objectID": "posts/2022-04-25-ajtfb-chapter-9.html#resources",
    "href": "posts/2022-04-25-ajtfb-chapter-9.html#resources",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 9: Tabular Modeling",
    "section": "Resources",
    "text": "Resources\n\nhttps://book.fast.ai - The book’s website; it’s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc…\n“Hands-On Gradient Boosting with XGBoost and scikit-learn” - Unread personally, but on my bookshelf and recommended by those in the know.\ntsai - A time series/sequences focused library built on PyTorch and fastai by Ignacio Oguiza"
  },
  {
    "objectID": "posts/2020-11-06-ajtfb-chapter-1.html",
    "href": "posts/2020-11-06-ajtfb-chapter-1.html",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning",
    "section": "",
    "text": "Other posts in this series:\nA Journey Through Fastbook (AJTFB) - Chapter 2\nA Journey Through Fastbook (AJTFB) - Chapter 3\nA Journey Through Fastbook (AJTFB) - Chapter 4\nA Journey Through Fastbook (AJTFB) - Chapter 5\nA Journey Through Fastbook (AJTFB) - Chapter 6a\nA Journey Through Fastbook (AJTFB) - Chapter 6b\nA Journey Through Fastbook (AJTFB) - Chapter 7\nA Journey Through Fastbook (AJTFB) - Chapter 8\nA Journey Through Fastbook (AJTFB) - Chapter 9\nCervantes once wrote that “the journey is better than the inn”, but I rather like to think that the journey is the inn.\nIt means that the journey, irrespective to its difficulties (and likely because of them), is what you look back on with fondness at its end rather than the end itself. It’s why I enjoy reading “The Lord of the Rings” every five years or so, where as I age and experience the hand life has dealt me, I find myself appreciating different aspects of the story from the time before and gaining new insights into what I value and want to be as a human being. I find my journey with deep learning to be roughly analgous to that.\nI’ve been a part of the fast.ai community for several years. I’ve been through the course multiple times (since it was using theano back in the old days), I’ve contributed to the library, and use it as the basis for one of my own. And as with each course, with a re-reading of the book I find myself deriving new insights and appreciating different ideas than those I had before.\nAnd so, while your journey may bring you different revelations, here are the meandering thoughts of one 49 year old married father of 3 living in San Diego, California, USA, as I embark upon the first chapter in what I consider “The Lord of the Rings” of deep learning."
  },
  {
    "objectID": "posts/2020-11-06-ajtfb-chapter-1.html#how-to-learn-deep-learning",
    "href": "posts/2020-11-06-ajtfb-chapter-1.html#how-to-learn-deep-learning",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning",
    "section": "How to learn Deep Learning",
    "text": "How to learn Deep Learning\n\nYou can do this!\n\nHi, everybody; I’m Jeremy … I do not have any formal technical education … didn’t have great grades. I was much more interested in doing real projects.\n\nThis is meaningful to me as someone with a BA in History and a MA in Theology. It’s a reminder that if you want something, it’s within your grasp to make it happen if you are willing to put in the work. It’s also a reminder that key to getting there is actually doing something! I find too many people thinking that if they just get into that school, or if they can just take that class, then they’ll be a good software enginner or deep learning practitioner. The reality is that the only way you get there is by doing it … just like pull-ups (which aren’t much fun when you’re starting out and/or you’re 49 and overweight).\n\n\nThe problem with traditional education\n\n… how math is taught - we require students to spend years doing rote memorization and learning dry disconnected fundatmentals that we claim will pay off later, long after most of them quit the subject.\n\nThis also is the problem with higher education in general, where young people spend at least four to five years learning things they already learned in High School or else things they don’t really care about and will be forgotten right after finals, spending in excess of $100,000 for the privilege of it and likely going into debt in the tens of thousands of dollars, all with this idea that having done it they will be prepared for the real world. Unfortunately, that’s not how it works. Whether you are in a university of even go to university, what matter is what you do … not what classes you took or what your GPA is.\n\n\nDeep Learning (and coding in general) is an art maybe more so than a science\n\nThe hardest part of deep learning is artisanal.\n\nI remember going to an iOS conference way back in the day and a conference speaker asking how many folks in the session I was sitting in had a background in music. 80-90% of the audience raised their hands. Sure, there is math and stats and a science to deep learning, but like any coding enterprise, it’s an art … with some artists being better than others along with room for improvement regardless of whether you’re Van Gough or painting by the numbers.\n\n\nDoing is how you learn, and what you’ve done is what matters\n\n… focus on your hobbies and passions … Common character traits in the people who do well at deep learning include playfulness and curiosity.\n\n\nat Tesla .. CEO Elon Musk says ‘A PhD is definitely not required. All that matters is a deep understanding of AI & ability to implement NNs in a way that is actually useful …. Don’t care if you even graduated High School.’\n\n\n… the most important thing for learning deep learning is writing code and experimenting.”\n\n\n\nFolks to follow\nIt’s always helpful to have some role models; folks who practice the lessons learned above and can help you along your journey.\nFor starters, consider this image of the top 12 users based on most likes in the fast.ai forums: \nAside from the founders of fast.ai and a bunch of them working for noteable ML companies like Hugging Face and Weights & Biases, I can think of at least FOUR things these folks have in common:\n\nThey are fearless in asking what they may have even considered, dumb questions.\nThey are active in researching the answers to their own questions (even the dumb ones) and those asked by others.\nThey are active in teaching others through blogs, books, open source libraries, study groups, and podcasts.\nThey build things! That is, they all have experience building models and making them usable via deployed applications and/or in kaggle compeititions. Anyone can bake a half-cooked model in a Jupyter notebook, but few can turn it into something others can use.\n\nThese traits aren’t just key to learning deep learning; they are key to learning anything! Practice them and you guarantee yourself success in learning anything you’ve set your mind on.\nIf you had to choose just three …\nAside from Jeremy (@jeremyphoward), who’s a given, if I could only follow three people who have mastered to art of learning deep learning, they would be …\nRadek Osmulsk: (twitter: @radekosmulski)\n\n\nIf you found this of value, you might be interested in a book on learning deep learning that I wrotecheck it out here &gt;&gt;&gt; https://t.co/ApKlm8BRmy\n\n— @radek@sigmoid.social (Mastodon) 🇺🇦 (@radekosmulski) November 2, 2021\n\n\nZach Mueller: (twitter: @TheZachMueller)\n\n\nTo me, I think it boiled down to how I learned. I took those two courses essentially over the course of a year or so. Approaching each lesson slowly, and letting myself wander in the related concepts, learning as much as I could through online communities.\n\n— Zach Mueller (@TheZachMueller) October 23, 2021\n\n\nSanyam Bhutani: (twitter: @bhutanisanyam1)\n\n\nThe @PyTorch book reading group @weights_biases comes to an end🙏We had an incredible 10 weeks of learning!As a group wanted to extend our gratitude to the incredible authors: Eli, @lantiga & @ThomasViehmann A few words from our community:https://t.co/3ODz6J1vad\n\n— Sanyam Bhutani (@bhutanisanyam1) October 25, 2021\n\n\nPersonally, I do follow each of these individuals on twitter and you should too! Though I’ve never met any of them IRL, I consider the colleagues, friends, and amongst the most helpful for those looking to get started in machine learning.\n\n\n\n\n\n\nTip\n\n\n\nTwitter is imo the best place to network with fellow ML/DL practioners and stay up-to-date with the latest developments in ML in general\n\n\nHere’s a tl;dr for folks too lazy to read the above …"
  },
  {
    "objectID": "posts/2020-11-06-ajtfb-chapter-1.html#what-is-machine-learning",
    "href": "posts/2020-11-06-ajtfb-chapter-1.html#what-is-machine-learning",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning",
    "section": "What is machine learning?",
    "text": "What is machine learning?\nHere we look at machine learning in general (of which deep learning is a subset) as well as the process of finetuning a pretrained ML model. When you think of deep learning … think neural networks.\n\n\nA picture\n\n\n\nAn explanation\n\n“Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would ‘learn’ from its experince” - Arthur Samuel\n\n\nArchitecture vs. model\n\n… a model is a special kind of program: it’s one that can do many different things, depending on the weights.\n\n\nThe functional form of the model is called its architecture.\n\n\n\n\n\n\n\nNote\n\n\n\nThe architecture is “the template of the model that we’re trying to fit; i.e., the actual mathematical function that we’re passing the input data and parameters to” … whereas the model is a particular set of parameters + the architecture.\n\n\n\n\nParameters\n\nWeights are just variables, and a weight assignment is a particuarl choice of values for those variables. [Weights] are generally referred to as model parameters … the term weights being reserved for a particular type of model parameter.\n\n\nThe weights are called parameters.\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters are the things that are “learnt”; the values that can be updated, whereas activations in a neural network are simply numbers as the result of some calculation.\n\n\n\n\nInputs vs.labels\nThe inputs, also known as your independent variable(s) [your X] is what your model uses to make predictions.\nThe labels, also known as your dependent variable(s) [your y] represent the correct target value for your task.\n\n\nLoss\n\nThe [model’s] measure of performance is called the loss … [the value of which depends on how well your model is able to predict] the correct labels.\n\nThe loss is a measure of model performance that SGD can use to make your model better. A good loss function provides good gradients (slopes) that can be used to make even very minor changes to your weights so as to improve things. Visually, you want gentle rolling hills rather than abrupt steps or jagged peaks.\n\n\n\n\n\n\nNote\n\n\n\nYou can think of the loss as the model’s metric, that is, how it both understands how good it is and can help it improve."
  },
  {
    "objectID": "posts/2020-11-06-ajtfb-chapter-1.html#transfer-learning",
    "href": "posts/2020-11-06-ajtfb-chapter-1.html#transfer-learning",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning",
    "section": "Transfer learning",
    "text": "Transfer learning\nTransfer learning is the process of taking a “pretrained model” that has been trained on a very large dataset with proven SOTA results, and “fine tuning” it for your specific task, which while likely similar to the task the pretrained model was trained for to one degree or another, is not the necesarily the same.\n\nHow does it work?\n\nThe head of your model (the newly added part specific to your dataset/task) should be trained first since it is the only one with completely random weights.\nThe degree to which your weights of the pretrained model will need to be updated is proportional to how similar your data is to the data it was trained on. The more dissimilar, the more the weights will need to be changed.\nYour model will only be as good as the data it was trained on, so make sure what you have is representative of what it will see in the real world. It “can learn to operate on only the patterns seen in the input data used to train it.”\n\n\nThe process of training (or fitting) the model is the process of finding a set of parameter values (or weights) that specialize that general architecture into a model that works well for our particular kind of data [and task]\n\n\n\nWhat is the high-level approach in fastai?\nfastai provides a fine_tune method that uses proven tricks and hyperparameters for various DL tasks that the author’s have found works well most of the time.\n\n\nWhat do we have at the end of training (or finetuning)?\n\n… once the model is trained - that is, once we’ve chosen our final weight assignments - then we can think of the weights as being part of the model since we’re not varying them anymore.\n\nThis means a trained model can be treated like a typical function."
  },
  {
    "objectID": "posts/2020-11-06-ajtfb-chapter-1.html#metrics",
    "href": "posts/2020-11-06-ajtfb-chapter-1.html#metrics",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning",
    "section": "Metrics",
    "text": "Metrics\n\nA definition\nMetrics are a human-understandable measures of model quality whereas the loss is the machine’s. They are based on your validation set and are what you really care about, whereas the loss is “a measure of performance” that the training system can use to update weights automatically.\nA good choice for loss is a function “that is easy for stochastic gradient descent (SGD) to use, whereas a good choies for your metrics are functions that your business users will care about. Seldom are they the same because most metrics don’t provide smooth gradients that SGD can use to update your model’s weights.\n\n\n\n\n\n\nNote\n\n\n\nAgain, they are based on your validation/test sets (not your training set). Ultimately, we want to have a model that generalizes well to inputs it was not trained on, and this is what our validation/test sets represent. This is how we relay our model quality.\n\n\n\n\nExamples\nThere are a whole list of metrics built into the fastai library, see here. Below I begin a listing of the most common ones as they come up in the fastbook (and from personal experience).\nerror rate = “the proportion of images that were incorrectly identified.”\naccuracy = the proportation of images that were correctly identified (1 - error rate)\n\n\nMetrics to use based on task\n\n\n\n\n\n\n\nMetric\nMulticlass classification\nMultilabel classification\nRegression\n\n\n\n\nerror rate\nYes\nYes*\nNo\n\n\naccuracy\nYes\nYes*\nNo\n\n\n\n\n\n* indicates that other metrics may be better for the given task."
  },
  {
    "objectID": "posts/2020-11-06-ajtfb-chapter-1.html#training-validation-and-test-datasets",
    "href": "posts/2020-11-06-ajtfb-chapter-1.html#training-validation-and-test-datasets",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning",
    "section": "Training, validation, and test datasets",
    "text": "Training, validation, and test datasets\n\nWhat is a training set?\nA training set consits of the data your model sees during training. These are the inputs and labels your model will use to determine the loss and update it’s parameters in a way that will hopefully lead to a model that works well for its given task.\n\nWhy do we need a training set?\nBecause a model needs something to train on. It should be representative of the data the model will see in the future, and it should be updated if/when you discover that is not the case.\n\n\nHow to use a training set?\n\nTo train a model on examples resembling that which the model will seen in the future. More is generally better, but quality is king (e.g., bad data in, bad data out).\nTo provide augmented examples for your model to see so as to increase the number of examples and better reflect what the model may see in the real world.\n\n\n\n\nWhat is a validation set?\nA validation set (also know as the “development set”) does not include any data from the training set. It’s purpose to is gauge the generalization prowess of your model and also ensure you are neight overfitting or underfitting.\n\n“If [the model] makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by actually having seen that particular item.”\n\n\nWhy do we need a validation set?\n\n“[because] what we care about is how well our model works on previously unseen images … the longer you train for, the better your accuracy will get on the training set … as the model starts to memorize the training set rather than finding generalizable underlying patterns in the data = overfitting”\n\n\nOverfitting happens when the model “remembers specific features of the input data, rather than generalizing well to data not seen during training.”\n\n\n\n\n\n\nNote\n\n\n\nALWAYS overfit before anything else. It is your training loss gets better while your validation loss gets worse … in other words, if you’re validation loss is improving, even if not to the extent of your training loss, you are not overfitting\n\n\n\n\n\n\n\n\nNote\n\n\n\nALWAYS include a validation set.\n\n\n\n\n\n\n\n\nNote\n\n\n\nALWAYS use the validation set to measure your accuracy (or any metrics).\n\n\n\n\n\n\n\n\nNote\n\n\n\nALWAYS set the seed parameter so that you “get the same validation set every time” so that “if we change our model and retrain it, we know any differences are due to the changes to the model, not due to having a different random validation set.”\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor a good discussion of how to achieve predictable randomness, see this discussion on the fast.ai forums. There are actually several seeds you need to set and in several places when using fast.ai to achieve reproducibility.\n\n\n\n\nHow to use a validation set?\n\nIt gives us a sense of how well our model is doing on examples it hasn’t seen, which makes sense since the ultimate worth of a model is in how well it generalizes to things unseen in the future.\nThe validation set also informs us how we may change the hyperparamters (e.g., model architecture, learning rates, data augmentation, etc…) to improve results. These parameters are NOT learned … they are choices WE make that affect the learning of the model parameters.\n\n\n\n\nWhat is a test set?\nA test set ensures that we aren’t overfitting our hyperparameter choices; it is held back even from ourselves and used to evaulate the model at the very end.\n\n“[Since we] are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values … subsequent version of the model are, indirectly, shaped by us having seen the validation data … [and therefore], we are in danger of overfitting the validation data through human trial and error and exploration.”\n\n\n\n\n\n\n\nNote\n\n\n\nA key property of the validation and test sets is that they must be representative of the new data you will see in the future.\n\n\n\nWhy do we need a test set?\nTo ensure we aren’t inadvertently causing the model to overfit via our hyperparameter tuning which happens as a result of us looking at the validation set. It is a completely hidden dataset; it isn’t used for training or tuning, only for measuring performance.\n\n\nHow to use a test set?\n\nIf evaluating 3rd party solutions. You’ll want to know how to create a good test set and how to create a good baseline model. Hold these out from the potential consultants and use them to fairly evaluate their work.\nTo ensure you aren’t overfitting your model as a result of validation set examination. As with the validation set, a good test set offers further assurance your model isn’t learning particular ancillary features of particular things in your images.\n\n\n\n\nHow to create good validation and test sets\nIt isn’t always as easy as randomly shuffling your data!\nAgain, what both of these sets should haven in common is that they “must be representative of the new data you will see in the future.” And what this looks like often dependes on your use case and task.\n\n\n\n\n\n\nTip\n\n\n\nYou really need to think about what you need to predict and what you’d look at to make that prediction. You also need to make sure your training data is qualitatively different enough from your real world data (e.g., what the validation and test sets represent) as to learn patterns and not specific examples.\n\n\nFirst, consider cases where historical data is required to predict the future, for example of quant traders use “backtesting to check whether their models are predictive of future periods, based on past data”\n\n\n\n\n\n\nNote\n\n\n\n“For a time series … (where you are using historical data to build a model for use in the future … you will want to choose a continuous section with the latest dates as your validation set”\n\n\n“A second common case occurs when you can easily anticipate ways the data you will be making predictions for in production may be qualitatively different from the data you have to train your model with.”\nAs an example of this, the Kaggle distracted driver competition is used. In it, based on pictures of drivers you need to predict categories of distraction. Since the goal of such a model would be to make good predictions against drivers the model hasn’t seen, it would make sense to create a validation and also a test set consiting of specific drivers the training set doesn’t include (in fact, the competition’s test set is exactly that!). “If you used all the people in training your model, your model might be overfitting to the paricipants of those specific people and not just learning the states (texting, eating, etc.).”\nAnother example of this is the Kaggle fisheries competition where the objective is to predict the species of fish caught on fishing boats. As the goal of such a model is to predict the species on other/future boats, it makes sense then that “the test set consisted of images from boats that didn’t appear in the training data, so in this case you’d want your validation set to also include boats that are not in the training set.”\n\n\n\n\n\n\nTip\n\n\n\nStart with training a model and let the results guide your EDA!\n\n\nFor a stellar example of how this looks in practice, see this thread from Boris Dayma on an issue he noticed when looking at his results on the training and validation sets. Note how his EDA was directed via training a model … and also make sure to read through all the comments, replies, etc… for other things to pay attention too when seeing unusual results during training (there is a lot of good stuff there). Ultimately, in his case, what he found out was that the dataset was imbalanced and the imbalanced data was getting lumped together in the same batches due to poor shuffling strategy. He documents his fix in a subsequent thread so check that out too.\n\n\n\n\n\n\nTip\n\n\n\nKnowing how to read your training/validation results drives EDA and will lead to better train/validation/test splits."
  },
  {
    "objectID": "posts/2020-11-06-ajtfb-chapter-1.html#categorical-datatypes",
    "href": "posts/2020-11-06-ajtfb-chapter-1.html#categorical-datatypes",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning",
    "section": "Categorical datatypes",
    "text": "Categorical datatypes\nCategorical data “contains values that are one of a discrete set of choice” such as gender, occupation, day of week, etc…\n\nWhat if our target is categorical?\nIf your target/lables are categorical, then you have either a multi-classification classification problem (e.g., you are trying to predict a single class) or a multi-label classification problem (e.g., you are trying to predict whether your example belongs to zero or multiple classes).\n\nMulti-classification tasks\nFor multi-classification tasks, a sensible loss function would be cross entropy loss (nn.CrossEntropyLoss) and useful metrics are likely to include error rate, accuracy, F1, recall, and/or precision depending on your business objectices and the make up of your dataset. For example, if you’re dealing with a highly imbalanced dataset, choosing accuracy would lead to an inflated sense of model performance since it may be learning to just predict the most common class.\n\n\n\n\n\n\nNote\n\n\n\nWhat if you need to predict “None”? This is more real world and covered nicely in Zach Mueller’s Recognizing Unknown Images (or the Unknown Label problem).\n\n\n\n\nMulti-label tasks\nFor multi-label tasks, a sensible loss function would be binary cross entropy loss (BCE) (nn.BCEWithLogitsLoss) and useful metrics are likely to include F1, recall, and/or precision depending on your business objectices and the make up of your dataset. Notice that I didn’t include error rate, or its opposite accuracy, as their datasets are generally highly imbalanced.\n\n\n\nWhat if our input is categorical?\nCategorical inputs are generally represented by an embedding (e.g., a vector of numbers). Why? Mostly because it gives your model the ability to provide a more complex representation of your category than a single numer would.\nFor example, imagine that one of your inputs is day of week (e.g., Sunday, Monday, etc.) … what does that mean? When combined with other inputs, its likely that the meaning of it is going to be much more nuanced than a single number can represent, and so we’d like to use multiple learned numbers. This is what an embedding is."
  },
  {
    "objectID": "posts/2020-11-06-ajtfb-chapter-1.html#continuous-datatypes",
    "href": "posts/2020-11-06-ajtfb-chapter-1.html#continuous-datatypes",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning",
    "section": "Continuous datatypes",
    "text": "Continuous datatypes\nContinuous data is numerical that represents a quantity such as age, salary, prices, etc…\n\nWhat if our target is continuous?\nIf your target/labels are continuous, then you have a regression problem and the most likely loss function you would choose would be mean-square-error loss (MSE) (nn.MSELoss) and your metric MSE as well\n\n“… MSE is already a a useful metric for this task (although its’ probably more interpretable after we take the square root)” … the RMSE (% fn 3 %}\n\n\n\n\n\n\n\nNote\n\n\n\nFor tasks that predict a continuous number, consider using y_range to constrain the network to predicting a value in the known range of valid values.\n\n\n\n\nWhat if our input is continuous?\nIn many cases there isn’t anything special you need to do, in others, it makes sense to scale these numbers so they are in the same range (usually 0 to 1) as the rest of your continuous inputs. This process is called normalization. The reason you would want to do this is so continuous values with bigger range of values (say 1000) don’t drown out those with a smaller range (say 5) during model training.\n\n\nNormalization\n\n\n\n\n\n\nNote\n\n\n\n“When training a model, if helps if your input data is normalizaed - that is, has a mean of 0 and a standard deviation of 1.\n\n\nSee How To Calculate the Mean and Standard Deviation — Normalizing Datasets in Pytorch\n\nimport torch\n\nprint('Example 1')\nnums = torch.tensor([0, 50, 100], dtype=float)\nprint(f'Some raw values: {nums}')\n\n# 1. calculate their mean and standard deviation\nm = nums.mean()\nstd = nums.std()\nprint(f'Their mean is {m} and their standard deviation is {std}')\n\n# 2. normalize their values \nnormalized = (nums - m) / std\nprint(f'Here are their values after normalization: {normalized}')\nprint('')\n\nprint('Example 2')\nnums = torch.tensor([0, 5000, 10000], dtype=float)\nprint(f'Some raw values: {nums}')\n\n# 1. calculate their mean and standard deviation\nm = nums.mean()\nstd = nums.std()\nprint(f'Their mean is {m} and their standard deviation is {std}')\n\n# 2. normalize their values \nnormalized = (nums - m) / std\nprint(f'Here are their values after normalization: {normalized}')\nprint('')\n\nExample 1\nSome raw values: tensor([  0.,  50., 100.], dtype=torch.float64)\nTheir mean is 50.0 and their standard deviation is 50.0\nHere are their values after normalization: tensor([-1.,  0.,  1.], dtype=torch.float64)\n\nExample 2\nSome raw values: tensor([    0.,  5000., 10000.], dtype=torch.float64)\nTheir mean is 5000.0 and their standard deviation is 5000.0\nHere are their values after normalization: tensor([-1.,  0.,  1.], dtype=torch.float64)\n\n\n\nfastai supplies a Normalize transform you can use to do this … “it acts on a whole mini-batch at once, so you can add it to the batch_tfms secion of your data block … you need to pass to this transform the mean and standard deviation that you want to use. If you don’t,”fastai will automatically calculate them from a single batch of your data). p.241\n\n\n\n\n\n\nNote\n\n\n\n“This means that when you distribute a model, you need to also distribute the statistics used for normalization.” (p.242)\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“… if you’re using a model that someon else has trained, make sure you find out what normalization statistics they used and match them.” (p.242)"
  },
  {
    "objectID": "posts/2020-11-06-ajtfb-chapter-1.html#resnets",
    "href": "posts/2020-11-06-ajtfb-chapter-1.html#resnets",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning",
    "section": "ResNets",
    "text": "ResNets\n\nWhat is a ResNet & Why use it for computer vision tasks?\nA ResNet is a model architecture that has proven to work well in CV tasks. Several variants exist with different numbers of layers with the larger architectures taking longer to train and more prone to overfitting especially with smaller datasets.\nThe number represents the number of layers in this particular ResNet variant … “(other options are 18, 50, 101, and 152) … model architectures with more layers take longer to train and are more prone to overfitting … on the other hand, when using more data, they can be qite a bit more accurate.”\n\nWhat other things can use images recognizers for besides image tasks?\nSound, time series, malware classification … “a good rule of thumb for converting a dataset into an image representation: if the human eye can recognize categories from the images, then a deep learning model should be able to do so too.”\n\n\nHow does it fare against more recent architectures like vision transformers?\nPretty well apparently (at least at the time this post was written) …\n\n\nI'm pleased to announce that the 'ResNet strikes back' paper is now on arxiv! Moving the baseline forward to 80.4% top-1 for a vanilla ResNet-50 arch w/ better training recipes. No extra data, no distillation. https://t.co/WP3UDXfV0r\n\n— Ross Wightman (@wightmanr) October 4, 2021\n\n\n\n\n\nBest practices\n\n\n\n\n\n\nTip\n\n\n\nStart with a smaller ResNet (like 18 or 34) and move up as needed.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have a lot of data, the bigger resnets will likely give you better results.\n\n\n\n\nAn example\n\nStep 1: Build our DataLoaders\n\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): \n  return x[0].isupper()                  # if filename is Capitalized, its a cat image\n\ndls = ImageDataLoaders.from_name_func(\n    path,                                # where our image are\n    get_image_files(path),               # how to build our inputs (our x)\n    label_func=is_cat,                   # how to build our labels (our y)\n    valid_pct=0.2,                       # how to build our validation set\n    item_tfms=Resize(224),               # things we want to do to each image when we fetch it\n    seed=42                              # for reproducibility\n)\n\nWhy do we make images 224x224 pixels?\n“This is the standard size for historical reasons (old pretrained models require this size exactly) … If you increase the size, you’ll often get a model with better results since it will be able to focus on more details.”\n\n\n\n\n\n\nTip\n\n\n\nTrain on progressively larger image sizes using the weights trained on smaller sizes as a kind of pretrained model.\n\n\n\n\nStep 2: Build our cnn_learner\n\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\n\n\n\nStep 3: Train\n\nlearn.fine_tune(1)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.149967\n0.042716\n0.015562\n01:16\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.054710\n0.021014\n0.008119\n01:07"
  },
  {
    "objectID": "posts/2020-11-06-ajtfb-chapter-1.html#visualizing-what-a-nn-is-learning",
    "href": "posts/2020-11-06-ajtfb-chapter-1.html#visualizing-what-a-nn-is-learning",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning",
    "section": "Visualizing what a NN is learning",
    "text": "Visualizing what a NN is learning\n\nWhy is it important?\nBecause it allows you to know both what your NN is doing/learning and whether it is learning anything at all. The former is helpful because it gives you confidence that your model is learning to look at the right information and insights on how to improve it, the later because a model that isn’t learning anything (e.g., able to update its parameters so as to improve itself) isn’t a helpful or useful model.\n\n\n\n\n\n\nTip\n\n\n\nLearn how to visualize and understand your activations and gradients\n\n\n\n\nComputer vision models\nThe top of this image is a visualization of the weights (what the model is learning), and the one below is a visualization of the activations, in particular, the parts of training images that most strongly match each set of weights above.\n\n\n\n\n\n\n\nTip\n\n\n\nThis kind of visualization is particularly helpful in transfer learning as it allows us to infer which layers may require more or less training for our task. For example, the layer above probably requires little to no training as it looks to be identifying edges and gradients, thing likely helpful and necessary for all computer vision tasks.\n\n\n\nExamples\nVectors into 2D grayscale images (MNIST)\n*Courtesy of Abishek Thakur’s, “Approaching (almost) any Machine Learning Problem”\n\ninputs, targets = datasets.fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\ntargets = targets.astype(int)\n\ninputs.shape, targets.shape # always helpful to see the shape of things\n\n((70000, 784), (70000,))\n\n\n\n# Here we are using a numpy array, but if you're using Pytorch, you could use either .view or .reshape\n# see https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\nimages = inputs.reshape((-1,28,28))\n\nprint(images.shape)\nplt.imshow(images[0], cmap='gray')\n\n(70000, 28, 28)\n\n\n\n\n\n\n\n\n\nVectors as clusters (MNIST)\n*Courtesy of Abishek Thakur’s, “Approaching (almost) any Machine Learning Problem”\n\ntsne = manifold.TSNE(n_components=2, random_state=42)\ntransformed_data = tsne.fit_transform(inputs[:1000]) # reduces dimensionality of each vector to 2\n\n/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  FutureWarning,\n/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  FutureWarning,\n\n\n\ncluster_data = np.column_stack((transformed_data, targets[:1000]))\ncluster_data.shape # transformed_data 2 dims (call them x and y) + targets 1 dim = 3\n\n(1000, 3)\n\n\n\ntsne_df = pd.DataFrame(cluster_data, columns=['x', 'y', 'targets'])\nprint(len(tsne_df))\ntsne_df.head(2)\n\n1000\n\n\n\n  \n    \n      \n\n\n\n\n\n\nx\ny\ntargets\n\n\n\n\n0\n22.735518\n14.271368\n5.0\n\n\n1\n45.913292\n0.439934\n0.0\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nviz = sns.FacetGrid(tsne_df, hue='targets', height=8)\nviz.map(plt.scatter, 'x', 'y').add_legend()"
  },
  {
    "objectID": "posts/2020-11-06-ajtfb-chapter-1.html#resources",
    "href": "posts/2020-11-06-ajtfb-chapter-1.html#resources",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning",
    "section": "Resources",
    "text": "Resources\n\nhttps://book.fast.ai - The book’s website; it’s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc…\nhttps://course.fast.ai/datasets - A variety of slimmed down datasets you can use for various DL tasks that support “rapid prototyping and experimentation.”\nhttps://huggingface.co/docs/datasets/ - Serves a similar purpose to the fastai datasets but for the NLP domain. Includes metrics and full/sub-set datasets that you can use to benchmark your results against the top guns of deep learning.\n\n\n\n\n\n\n\nImportant\n\n\n\nStart with a smaller dataset and scale up to full size to accelerate modeling!"
  },
  {
    "objectID": "posts/2024-07-06-llms-and-enums.html",
    "href": "posts/2024-07-06-llms-and-enums.html",
    "title": "Structuring Enums for Flawless LLM results with Instructor",
    "section": "",
    "text": "I’m spending some time with the Jason Liu’s Instructor library in building a function calling solution that returns structured output because, well, Hamel recommends it for proprietary models.\n\n For open models you should use outlines. for closed models APIs you should use instructor.\n\nThe library is intuitive, fun to use, and has some really nice documentation. When it comes to choosing whether to use enums or literals in your pydantic classes, the docs recommend the following:\n\nFor classification we’ve found theres generally two methods of modeling.\n\nusing Enums\nusing Literals\n\nUse an enum in Python when you need a set of named constants that are related and you want to ensure type safety, readability, and prevent invalid values. Enums are helpful for grouping and iterating over these constants.\nUse literals when you have a small, unchanging set of values that you don’t need to group or iterate over, and when type safety and preventing invalid values is less of a concern. Literals are simpler and more direct for basic, one-off values.\n\n… and they also seems to indicate that getting them to work as expected might be challenging …\n\nIf you’re having a hard time with Enum an alternative is to use Literal\n\nI found this out first-hand when I was attempting to define an enum for a number of named entities I wanted an LLM to identifiy in a given document. My intial code worked pretty nicely with GPT-4o but failed miserabley time and time again with every Antrhopic model I tried (I’ll explain why below). If you’re looking for the TL;DR, the final version of my code at the end of this post represents a substantially more resiliant solution that works across vendors (I also tested this with Fireworks), offering a better guaranttee your LLM calls find the entities you care about correctly."
  },
  {
    "objectID": "posts/2024-07-06-llms-and-enums.html#instructor-best-practices-and-cautions",
    "href": "posts/2024-07-06-llms-and-enums.html#instructor-best-practices-and-cautions",
    "title": "Structuring Enums for Flawless LLM results with Instructor",
    "section": "",
    "text": "I’m spending some time with the Jason Liu’s Instructor library in building a function calling solution that returns structured output because, well, Hamel recommends it for proprietary models.\n\n For open models you should use outlines. for closed models APIs you should use instructor.\n\nThe library is intuitive, fun to use, and has some really nice documentation. When it comes to choosing whether to use enums or literals in your pydantic classes, the docs recommend the following:\n\nFor classification we’ve found theres generally two methods of modeling.\n\nusing Enums\nusing Literals\n\nUse an enum in Python when you need a set of named constants that are related and you want to ensure type safety, readability, and prevent invalid values. Enums are helpful for grouping and iterating over these constants.\nUse literals when you have a small, unchanging set of values that you don’t need to group or iterate over, and when type safety and preventing invalid values is less of a concern. Literals are simpler and more direct for basic, one-off values.\n\n… and they also seems to indicate that getting them to work as expected might be challenging …\n\nIf you’re having a hard time with Enum an alternative is to use Literal\n\nI found this out first-hand when I was attempting to define an enum for a number of named entities I wanted an LLM to identifiy in a given document. My intial code worked pretty nicely with GPT-4o but failed miserabley time and time again with every Antrhopic model I tried (I’ll explain why below). If you’re looking for the TL;DR, the final version of my code at the end of this post represents a substantially more resiliant solution that works across vendors (I also tested this with Fireworks), offering a better guaranttee your LLM calls find the entities you care about correctly."
  },
  {
    "objectID": "posts/2024-07-06-llms-and-enums.html#v0-using-enum",
    "href": "posts/2024-07-06-llms-and-enums.html#v0-using-enum",
    "title": "Structuring Enums for Flawless LLM results with Instructor",
    "section": "v0: Using Enum",
    "text": "v0: Using Enum\nThis is the initial Enum and pydantic classes I started with. It works pretty damn well with OpenAI’s GPT-4o but fails spectacularly when using any of the Anthopic models.\nclass EntityGroup(str, Enum):\n    \"\"\"A named entity type.\"\"\"\n\n    PERSON = \"PERSON\"\n    ORGANIZATION = \"ORGANIZATION\"\n    LOCATION = \"LOCATION\"\n    DATE = \"DATE\"\n    TIME = \"TIME\"\n    PERCENT = \"PERCENT\"\n    MONEY = \"MONEY\"\n    QUANTITY = \"QUANTITY\"\n    ORDINAL = \"ORDINAL\"\n    CARDINAL = \"CARDINAL\"\n    EMAIL = \"EMAIL\"\n    PHONE_NUMBER = \"PHONE_NUMBER\"\n    CREDIT_CARD_NUMBER = \"CREDIT_CARD_NUMBER\"\n    SSN = \"SSN\"\n\n\nclass NamedEntity(BaseModel):\n    \"\"\"The type of named entity and it's value.\"\"\"\n\n    entity_group: EntityGroup = Field(..., description=\"The type of named entity\")\n    word: str = Field(..., description=\"The named entity found\")\n\n\nclass DocumentNER(BaseModel):\n    \"\"\"Information about named entities to extract.\"\"\"\n\n    named_entities: list[NamedEntity] = Field(\n        ...,\n        description=f\"Perform Named Entity Recognition that finds the following entities: {', '.join([x.name for x in EntityGroup])}\",\n    )\nWhen using the Anthropic models, I would consistently see it trying to set entity_group to a string rather than a proper enum value from the EntityGroup enum.\nAfter iterating through a number of prompt and class/field description modifications, I decided to give up and replace my Enum with a Literal. And guess what, everything worked great across all model vendors.\nI also decided to lookup the named entities used in Spacy and use those names in my Enum as it makes sense to me that perhaps these libraries might have been included in the training of these LLMs and so maybe will help it do a better job of finding the entities I care about."
  },
  {
    "objectID": "posts/2024-07-06-llms-and-enums.html#v1-using-literal",
    "href": "posts/2024-07-06-llms-and-enums.html#v1-using-literal",
    "title": "Structuring Enums for Flawless LLM results with Instructor",
    "section": "v1: Using Literal",
    "text": "v1: Using Literal\nUsing the Literal type fixed everything and works great across all models! Here’s what it looks like:\nclass NamedEntity(BaseModel):\n    \"\"\"A named entity found in a document.\"\"\"\n\n    entity_type: Literal[\n        \"PERSON\",\n        \"NORP\",\n        \"FAC\",\n        \"ORG\",\n        \"GPE\",\n        \"LOC\",\n        \"PRODUCT\",\n        \"EVENT\",\n        \"WORK_OF_ART\",\n        \"LAW\",\n        \"LANGUAGE\",\n        \"DATE\",\n        \"TIME\",\n        \"PERCENT\",\n        \"MONEY\",\n        \"QUANTITY\",\n        \"ORDINAL\",\n        \"CARDINAL\",\n        \"OTHER\",\n    ]\n    entity: str = Field(..., description=\"The named entity found\")\n\n\nclass DocumentNERTask(BaseModel):\n    \"\"\"Extracts the named entities in the document.\n\n    This tool should be used anytime the user asks for named entity recognition (NER)\n    or wants to identify named entities.\n    \"\"\"\n\n    named_entities: list[NamedEntity] = Field(\n        ...,\n        description=\"Perform Named Entity Recognition and return a list of any 'NamedEntity' objects found.\",\n    )\nThis works great … but I really wanted to use an Enum for the reasons listed at the top of this post. And as I’m the kinda guy who enjoys fighting with CUDA installs on his local DL rig, I decided to give it a go after taking a few hours off to enjoy the Euros and Copa America tourneys (also Germany should have won; that was a handball but nah, I’m not angry, nope, not bent at all)."
  },
  {
    "objectID": "posts/2024-07-06-llms-and-enums.html#v2-using-enum-revisted",
    "href": "posts/2024-07-06-llms-and-enums.html#v2-using-enum-revisted",
    "title": "Structuring Enums for Flawless LLM results with Instructor",
    "section": "v2: Using Enum Revisted",
    "text": "v2: Using Enum Revisted\nHere’s the TL;DR version of the code. This version is working fabulously across all APIs and I have yet to encounter a single exception involving Instructor being unable to assign a valid value from the Enum.\nclass NamedEntityType(str, Enum):\n    \"\"\"Valid types of named entities to extract.\"\"\"\n\n    PERSON = \"PERSON\"\n    NORP = \"NORP\"\n    FAC = \"FAC\"\n    ORG = \"ORG\"\n    GPE = \"GPE\"\n    LOC = \"LOC\"\n    PRODUCT = \"PRODUCT\"\n    EVENT = \"EVENT\"\n    WORK_OF_ART = \"WORK_OF_ART\"\n    LAW = \"LAW\"\n    LANGUAGE = \"LANGUAGE\"\n    DATE = \"DATE\"\n    TIME = \"TIME\"\n    PERCENT = \"PERCENT\"\n    MONEY = \"MONEY\"\n    QUANTITY = \"QUANTITY\"\n    ORDINAL = \"ORDINAL\"\n    CARDINAL = \"CARDINAL\"\n    OTHER = \"OTHER\"\n\n\nclass NamedEntity(BaseModel):\n    \"\"\"A named entity result.\"\"\"\n\n    def convert_str_to_named_entity_type(v: str | NamedEntityType) -&gt; NamedEntityType:\n        \"\"\"Ensure entity type is a valid enum.\"\"\"\n        if isinstance(v, NamedEntityType):\n            return v\n        else:\n            try:\n                return NamedEntityType(v)\n            except ValueError:\n                return NamedEntityType.OTHER\n\n    entity_type: Annotated[str, BeforeValidator(convert_str_to_named_entity_type)]\n    entity_mention: str = Field(..., description=\"The named entity recognized.\")\n\n\nclass DocumentNERTask(BaseModel):\n    \"\"\"Extracts the named entities found in the document.\n\n    This tool should be used anytime the user asks for named entity recognition (NER)\n    or wants to identify named entities.\n    \"\"\"\n\n    named_entities: list[NamedEntity] = Field(\n        ...,\n        description=f\"Perform Named Entity Recognition that finds the following entities: {', '.join([x.name for x in NamedEntityType])}\",\n    )\nBesides the return of the Enum, the most noticeable change involves the inclusion of a BeforeValidator that ensures the value is assigned to a valid enum as defined in NamedEntity. In cases where it wants to add an entity to the list of named_entities that isn’t defined in the NamedEntityType enum or is named differently (e.g., “ORGANIZATION” vs. “ORG”), it will assign them to OTHER.\nWith this in place, I now have a solution that is:\n\nMore resiliant\nCan be used in debugging named entity recogintion (e.g, I can explore what named entities might be missing from the Enum or getting named differently by looking at those that get associated with the OTHER value)\nI can use that same beautiful Enum across all parts of my application"
  },
  {
    "objectID": "posts/2024-07-06-llms-and-enums.html#v2.0.1-using-enum-and-fuzzywuzzy",
    "href": "posts/2024-07-06-llms-and-enums.html#v2.0.1-using-enum-and-fuzzywuzzy",
    "title": "Structuring Enums for Flawless LLM results with Instructor",
    "section": "v2.0.1: Using Enum and fuzzywuzzy",
    "text": "v2.0.1: Using Enum and fuzzywuzzy\nA suggestion from a Twitter user inspired me to enhance our approach by implementing similarity-based matching rather than relying on exact matches. To make it so, I installed the fuzzywuzzy library and made the necessary modifications to increase the likelihood of delivering high-quality results.\nclass NamedEntityType(str, Enum):\n    \"\"\"Valid types of named entities to extract.\"\"\"\n\n    PERSON = \"PERSON\"\n    NORP = \"NORP\"\n    FAC = \"FAC\"\n    ORG = \"ORG\"\n    GPE = \"GPE\"\n    LOC = \"LOC\"\n    PRODUCT = \"PRODUCT\"\n    EVENT = \"EVENT\"\n    WORK_OF_ART = \"WORK_OF_ART\"\n    LAW = \"LAW\"\n    LANGUAGE = \"LANGUAGE\"\n    DATE = \"DATE\"\n    TIME = \"TIME\"\n    PERCENT = \"PERCENT\"\n    MONEY = \"MONEY\"\n    QUANTITY = \"QUANTITY\"\n    ORDINAL = \"ORDINAL\"\n    CARDINAL = \"CARDINAL\"\n    OTHER = \"OTHER\"\n\n\nclass NamedEntity(BaseModel):\n    \"\"\"A named entity result.\"\"\"\n\n    def convert_str_to_named_entity_type(v: str | NamedEntityType) -&gt; NamedEntityType:\n        \"\"\"Ensure entity type is a valid enum.\"\"\"\n        if isinstance(v, NamedEntityType):\n            return v\n        else:\n            try:\n                match, score = fuzzy_process.extractOne(v.upper(), [e.value for e in list(NamedEntityType)])\n                return NamedEntityType(match) if score &gt;= 60 else NamedEntityType.OTHER\n            except ValueError:\n                return NamedEntityType.OTHER\n\n    entity_type: Annotated[str, BeforeValidator(convert_str_to_named_entity_type)]\n    entity_mention: str = Field(..., description=\"The named entity recognized.\")\n\n\nclass DocumentNERTask(BaseModel):\n    \"\"\"Extracts the named entities found in the document.\n\n    This tool should be used anytime the user asks for named entity recognition (NER)\n    or wants to identify named entities.\n    \"\"\"\n\n    named_entities: list[NamedEntity] = Field(\n        ...,\n        description=f\"Perform Named Entity Recognition that finds the following entities: {', '.join([x.name for x in NamedEntityType])}\",\n    )\nThis improves those cases where, for example, the LLM wants to define the entity type as “ORGANIZATION” but it is defined in the Enum as “ORG”.\nAnother option potentially worth exploring is to use the llm_validator function to make a call out to the LLM when exceptions happen and prompt it to coerce the value into something in the Enum. This could hike up your costs a bit but I imagine using a cheap model like GPT-3.5-Turbo could do the job just fine, and would likely you give an addtional robustness in quality results."
  },
  {
    "objectID": "posts/2024-07-06-llms-and-enums.html#conclusion",
    "href": "posts/2024-07-06-llms-and-enums.html#conclusion",
    "title": "Structuring Enums for Flawless LLM results with Instructor",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s it.\nIf you found this helpful and/or have suggestions on how to improve the use of Enums in Instructor, lmk in the comments below or on X. Until then, time to enjoy some football and see if Brazil can make it into the semis."
  },
  {
    "objectID": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html",
    "href": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html",
    "title": "LLM Workshop #4 - L1 Evals and Dataset Curation (Part I)",
    "section": "",
    "text": "Two recurring themes from the workshop are “look at your data (alot)” and iterate quickly. When building AI powered applications, we will need three things in place to make both a reality.\n\n\nA mechanism to evaluate performance.\nA mechanism to look at our data and see where things are working well or not so well (overall and by example).\nThe ability to make modifications to our system and measure how well they improved things or not.\nThe ability to curate datasets that can be used for better evals and fine tuning.\n\n\n\nSo having prototyed our tool calling application with off the shelf models, the recommendation is to get going in building such a mechanism with the inclusion of “simple tests and assertions.” Hamel calls these tests “L1 Evals” (e.g., Level 1: Unit Tests) and describes them like this in his blog post, “Your AI Product Needs Evals”:\n\nUnit tests for LLMs are assertions (like you would write in pytest). Unlike typical unit tests, you want to organize these assertions for use in places beyond unit tests, such as data cleaning and automatic retries (using the assertion error to course-correct) during model inference. The important part is that these assertions should run fast and cheaply as you develop your application so that you can run them every time your code changes. \n\nIn the case of our tool calling project here, these may take the form of scoring functions that verify the model returns JSON, calls all the tools expected, and doesn’t generate any errors.\nTo begin with, what makes for a “great” eval and how do great evals allow us to “look at our data” and iterate with speed?"
  },
  {
    "objectID": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#iterating-quickly",
    "href": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#iterating-quickly",
    "title": "LLM Workshop #4 - L1 Evals and Dataset Curation (Part I)",
    "section": "",
    "text": "Two recurring themes from the workshop are “look at your data (alot)” and iterate quickly. When building AI powered applications, we will need three things in place to make both a reality.\n\n\nA mechanism to evaluate performance.\nA mechanism to look at our data and see where things are working well or not so well (overall and by example).\nThe ability to make modifications to our system and measure how well they improved things or not.\nThe ability to curate datasets that can be used for better evals and fine tuning.\n\n\n\nSo having prototyed our tool calling application with off the shelf models, the recommendation is to get going in building such a mechanism with the inclusion of “simple tests and assertions.” Hamel calls these tests “L1 Evals” (e.g., Level 1: Unit Tests) and describes them like this in his blog post, “Your AI Product Needs Evals”:\n\nUnit tests for LLMs are assertions (like you would write in pytest). Unlike typical unit tests, you want to organize these assertions for use in places beyond unit tests, such as data cleaning and automatic retries (using the assertion error to course-correct) during model inference. The important part is that these assertions should run fast and cheaply as you develop your application so that you can run them every time your code changes. \n\nIn the case of our tool calling project here, these may take the form of scoring functions that verify the model returns JSON, calls all the tools expected, and doesn’t generate any errors.\nTo begin with, what makes for a “great” eval and how do great evals allow us to “look at our data” and iterate with speed?"
  },
  {
    "objectID": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#what-is-a-great-eval",
    "href": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#what-is-a-great-eval",
    "title": "LLM Workshop #4 - L1 Evals and Dataset Curation (Part I)",
    "section": "What is a “Great Eval”?",
    "text": "What is a “Great Eval”?\nI really liked the definition Ankur Goyal’s provided in his conference talk on doing LLM Eval For Text2SQL. He says that a “great eval” is comprised of three componnents:\n\nSome data to act on\nA task to perform with that data (e.g., “I want you to take some input and generate some output”)\nOne or more scoring functions that measure how well your task did the job\n\n\n\n\nThe anatomy of a great eval\n\n\n\n\n\n\n\n\nWhy you should eval?\n\n\n\nAnkur provides 4 key reasons:\n\nTo understand whether an update you made is an improvement or a regression (is it easy to figure this out?)\nDrill down into good or bad examples (is it easy to get at them?)\nDiff specific examples vs. prior runs (is it easy to see how your current evals did against your last evals?)\nAvoid playing whack-a-mole (can you easily track and know what you broke and/or fixed as you develop)\n\n\n\nSo as we iterate through our evaluation pipeline, that’s what we’ll do. At each step we’ll grab some data, define a task, and configure a number of scoring functions. It’s really that simple. That is an “eval.”\nBut before we get to that, what exaclty is an “L1” eval?"
  },
  {
    "objectID": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#level-1-unit-tests-l1-evals",
    "href": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#level-1-unit-tests-l1-evals",
    "title": "LLM Workshop #4 - L1 Evals and Dataset Curation (Part I)",
    "section": "Level 1: Unit Tests (L1 Evals)",
    "text": "Level 1: Unit Tests (L1 Evals)\nFrom the blog post mentioned above, we can infer some things about these particular evals.\n\nEach should test a specific feature or scenario (e.g., the model called all the tools we expected).\nThey can also be general tests that aren’t specific to any feature or scenario (e.g., the model returns valid JSON).\nYou can use an LLM to synthetically generate inputs for these evals if necessary (see here for an exmaple)\nYou need to be able to track the results of these tests over time.\n\n\n\n\n\n\n\nYou will continuously update these evals\n\n\n\nAs you observe new failures, you should update your evals to capture those use cases. “You must constantly update these tests as you observe data through human evaluation and debugging. The key is to make these as challenging as possible while representing users’ interactions with the system.” \n\n\nAnd remember this very important advice on interpreting the worthiness of your evals …\n\n\n\n\n\n\nThe pass rate is a product decision\n\n\n\n“One signal you are writing good tests and assertions is when the model struggles to pass them - these failure modes become problems you can solve with techniques like fine-tuning later on … unlike traditional unit tests, you don’t necessarily need a 100% pass rate. Your pass rate is a product decision, depending on the failures you are willing to tolerate.” \n\n\nLucky for us there is a really great tool that does all of this. It’s called Braintrust and it’s awesome! Before we get to it, we need to prepare a dataset to generate some predictions from."
  },
  {
    "objectID": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#eval-preparation",
    "href": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#eval-preparation",
    "title": "LLM Workshop #4 - L1 Evals and Dataset Curation (Part I)",
    "section": "Eval Preparation",
    "text": "Eval Preparation\nBefore we start eval’ng, we need to define our tools and a few methods for building a dataset we can pipe into Braintrust to get some initial feedback.\n\n\n\n\n\n\nImportant\n\n\n\nAt this stage it’s important to note that we don’t have any “targets” or “expected” outputs to measure our “outputs” by. As this is where we really want to get too, we’ll use these iteration to build up such a dataset one experiment at at time.\n\n\n\nTools\nThese are the same tools you saw in the previous post. As we build our our eval pipeline, we’ll get a better idea of how we might alter the definitions of these tools to improve performance. But since our “vibe check” passed, we’ll start with what we know already seems to work pretty well.\n\n\nPydantic classes for the core tools we want to use in evals\n# Translation\nclass TranslationTask(BaseModel):\n    \"\"\"Determine the original language the document is written in and translate it into English.\n\n    This tool should be used anytime the user provides a non-English document.\n    \"\"\"\n\n    english_translation: str = Field(\n        ...,\n        description=\"The text translated into English\",\n    )\n    source_language: str = Field(\n        ...,\n        description=\"The language of the original text (e.g., English, Spanish, French, Chinese, German, etc.)\",\n    )\n\n\n# Document summarization\nclass DocumentSummaryTask(BaseModel):\n    \"\"\"Provide a short summary of the document and any broad themes.\n\n    This tool should be used anytime the user asks to summarize a document or identify the\n    high-level themes in a single document.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"A concise, one-sentence summary of the document\",\n    )\n    themes: list[str] | None = Field(\n        ...,\n        description=\"A list of up to 5 concise themes, each 1 to 3 words long\",\n        max_items=5,\n    )\n\n\nclass NamedEntityType(str, Enum):\n    \"\"\"Valid types of named entities to extract.\"\"\"\n\n    PERSON = \"PERSON\"\n    NORP = \"NORP\"\n    FAC = \"FAC\"\n    ORG = \"ORG\"\n    GPE = \"GPE\"\n    LOC = \"LOC\"\n    PRODUCT = \"PRODUCT\"\n    EVENT = \"EVENT\"\n    WORK_OF_ART = \"WORK_OF_ART\"\n    LAW = \"LAW\"\n    LANGUAGE = \"LANGUAGE\"\n    DATE = \"DATE\"\n    TIME = \"TIME\"\n    PERCENT = \"PERCENT\"\n    MONEY = \"MONEY\"\n    QUANTITY = \"QUANTITY\"\n    ORDINAL = \"ORDINAL\"\n    CARDINAL = \"CARDINAL\"\n    OTHER = \"OTHER\"\n\n\nclass NamedEntity(BaseModel):\n    \"\"\"A named entity result.\"\"\"\n\n    def convert_str_to_named_entity_type(v: str | NamedEntityType) -&gt; NamedEntityType:\n        \"\"\"Ensure entity type is a valid enum.\"\"\"\n        if isinstance(v, NamedEntityType):\n            return v\n        else:\n            try:\n                match, score = fuzzy_process.extractOne(\n                    v.upper(), [e.value for e in list(NamedEntityType)]\n                )\n                return NamedEntityType(match) if score &gt;= 60 else NamedEntityType.OTHER\n            except ValueError:\n                return NamedEntityType.OTHER\n\n    entity_type: Annotated[str, BeforeValidator(convert_str_to_named_entity_type)]\n    entity_mention: str = Field(..., description=\"The named entity recognized.\")\n\n\nclass DocumentNERTask(BaseModel):\n    \"\"\"Extracts the named entities found in the document.\n\n    This tool should be used anytime the user asks for named entity recognition (NER)\n    or wants to identify named entities.\n    \"\"\"\n\n    named_entities: list[NamedEntity] = Field(\n        ...,\n        description=f\"Perform Named Entity Recognition that finds the following entities: {', '.join([x.name for x in NamedEntityType])}\",\n    )\n\n\n# Sentiment\nclass DocumentSentimentTask(BaseModel):\n    \"\"\"Information about the sentiments expressed in a document.\n\n    This tool should be used anytime the user asks for sentiment analysis.\n    \"\"\"\n\n    positivity: int = Field(\n        ...,\n        description=\"How positive or negative is the author on a scale between 1 and 5 (1=Very Low, 2=Moerately Low, 3=Neutral, 4=Moderately Strong, 5=Very Strong)?\",  # noqa: E501\n        ge=1,\n        le=5,\n    )\n    positive_statements: list[str] = Field(\n        ...,\n        description=\"A list of the author's positive statements\",\n    )\n    negative_statements: list[str] = Field(\n        ...,\n        description=\"A list of the author's negative statements\",\n    )\n\n    has_suggestions: bool = Field(\n        ...,\n        description=\"Does the author make any suggestions?\",\n    )\n    suggestions: list[str] = Field(\n        ...,\n        description=\"A list of any suggestions the author makes\",\n    )\n\n    feels_threatened: bool = Field(\n        ...,\n        description=\"Does the author feel fearful, harmed, intimidated, harassased, discriminated against, or threatened in any way?\",\n    )\n    feels_threatened_examples: list[str] = Field(\n        ...,\n        description=\"A list of how and why the author feels physically/emotionally/mentally threatened, uncomfortable, harassaed\",\n    )\n\n    profanity: bool = Field(\n        ...,\n        description=\"Is there any profanity?\",\n    )\n\n    is_nonsense: bool = Field(\n        ...,\n        description=\"Is the text uninformative or only contain nonsense? Set to `True` if the document is it too short to be meaningful or only says something like 'N/A', 'None', 'I have nothing to add', 'No suggestions', or 'No comment'.\",\n    )\n\n\n# Topic summarization\nclass TopicSummaryTask(BaseModel):\n    \"\"\"Extract the theme and an action plan from a collection of related documents.\n\n    This tool should be used anytime the user asks to identify the theme and an action plan and\n    several documents are provided as context.\n    \"\"\"\n\n    theme_name: str = Field(\n        ...,\n        description=\"A concise, 5-10 word phrase representing the theme of the documents\",\n    )\n    action_plan: list[str] = Field(\n        ...,\n        description=\"A list of 3-5 specific actions derived from the documents\",\n        max_items=5,\n    )\n\n\n\n\nDataset\nWe need to be able to build representative sampled datasets of various sizes at this stage. We want to create a varied set of examples each time we pull a subset of data, where each example has the following attributes:\n\nA prompt to call one or more tools\nThe document(s) to act on as context\nThe number of tools we expect to be called\nA list of the tools we expect to be called.\n\nWhen building the “prompt” for each example, I randomly grab a task specific prompt for each tool that should be called and merge them all together to make up the final “prompt”.\n\n\n\n\n\n\nTip: Use an LLM for synthetic data generation\n\n\n\nIn retrospect, it probably would have been better if I had used an LLM to generate these prompts synthetically so as to make them more grammatically correct.\n\n\nAnyhow, I’m not going to get into this step as its so use case specific. The code is posted below. Feel free to take a look (or not) for some inspiration on ideas you may be able to apply in your own work. As always, if you see improvements I could make … let me know. :)\n\n\nCode to create evaluation datasets\n# For each task/tool, I include a number of prompts I imagine a user might use to implore the AI to perform\n# that task.  When I build the actually examples, I randomly grab a single item for each task that should\n# be called to build the final user prompt\nexample_translation_asks = [\n    \"If the text below is not in English, translate it into English; otherwise, return the text as is\",\n    \"Translate the text below into English if it is not already in English; if it is, return it unchanged\",\n    \"If the provided text is in a language other than English, translate it to English; if it's already in English, return it without changes\",  # noqa: E501\n    \"Translate the following text to English if it is not written in English; otherwise, leave it unaltered\",\n    \"If the text below is not in English, convert it to English; if it is in English, return it without modification\",\n]\n\nexample_summarization_asks = [\n    \"Provide a short summary of the text below and identify any overarching themes\",\n    \"Summarize the document below and highlight any broad themes present\",\n    \"Create a concise summary of the following document and identify any general themes\",\n    \"Extract a brief 1-2 sentence summary from the text below and determine any broad themes\",\n    \"Summarize the text below in 1-2 sentences and identify any major themes within it\",\n]\nexample_ner_asks = [\n    \"Extract the named entities from the author's statement below\",\n    \"Determine the named entities in the document below\",\n    \"Identify the specific named entities in the statement below\",\n    \"Perform NER on the text below\",\n    \"Do NER on the document below\",\n]\nexample_sentiment_asks = [\n    \"Determine the sentiment expressed in the document below\",\n    \"Do sentiment analysis on the text below\",\n    \"Identify and classify the sentiments expressed in the following document\",\n    \"Perform sentiment analysis on the document below\",\n    \"Extract the sentiment from the document below\",\n]\nexample_theme_asks = [\n    \"Identify the theme in the documents below and create an action plan based on them\",\n    \"Determine the theme in the provided documents and develop an action plan accordingly\",\n    \"Extract the theme in the documents below and formulate an action plan based on your findings\",\n    \"Based on the documents provided, extract their theme and an action plan based on them\",\n    \"What is a good theme that describes the text below? Also include an action plan based on the provided documents\",\n]\n\n\n# Given a list of tools to call and whether our context is a single document or a colleciton of documents,\n# this utility function does the work of coallesicing all the asks into a single human prompt.  Note how\n# for \"translation\" tasks we sometimes ask for translation specifically and at other times we don't because\n# I hope that ultimately, the model can just see the text is not in English and do the translation task\n# without being specifically asked too.\ndef build_hypothetical_prompt(tool_names, is_doc: bool = False):\n    # Vary the overall format of the prompt for different styles by which a user may ask for\n    # different tasks\n    if random.choice([True, False]):\n        queries = []\n        for tool_name in tool_names:\n            if tool_name == \"TranslationTask\":\n                # Vary this being asked explicity so model can learn to call this tool whenver\n                # it detects a non-English document\n                if random.choice([True, False]):\n                    queries.append(random.choice(example_translation_asks))\n            elif tool_name == \"DocumentSummaryTask\":\n                queries.append(random.choice(example_summarization_asks))\n            elif tool_name == \"DocumentNERTask\":\n                queries.append(random.choice(example_ner_asks))\n            elif tool_name == \"DocumentSentimentTask\":\n                queries.append(random.choice(example_sentiment_asks))\n            elif tool_name == \"TopicSummaryTask\":\n                queries.append(random.choice(example_theme_asks))\n\n        prompt = queries[0]\n        for q in queries[1:]:\n            prompt += f\". {q}\"\n    else:\n        tasks = []\n        for tool_name in tool_names:\n            if tool_name == \"DocumentSummaryTask\":\n                tasks.append(\"summarization\")\n            elif tool_name == \"DocumentNERTask\":\n                tasks.append(\"named entity recognition (ner)\")\n            elif tool_name == \"DocumentSentimentTask\":\n                tasks.append(\"sentiment analysis\")\n            elif tool_name == \"TopicSummaryTask\":\n                tasks.append(\"thematic analysis/action planning\")\n\n        random.shuffle(tasks)\n\n        if is_doc:\n            tasks.insert(0, \"translation (if the document is not in English)\")\n\n        prompt = f\"Tasks: {', '.join(tasks)}\"\n\n    return prompt.strip()\n\n\n# When building examples, I use these weights to ensure some tasks are more frequently asked for than\n# others. I'm doing this because it's my observation that this is what is more likely to be seen at\n# inference time.\ndef get_tool_option_weights(tool_options: list[str]):\n    weights = [\n        0.1 if i == 1 else 0.9 / (len(tool_options) - 1)\n        for i in range(1, len(tool_options) + 1)\n    ]\n    return weights\n\n\n# This is the function that will be called to build a sampled dataset. It's designed to give the\n# developer the ability to determine how many, and of what type, of data it should get from the\n# document datasets created earlier. It also determines a random number of tools to call and\n# includes that information in the dataset to be used for evals (e.g., testing that all expected\n# tools were called, etc...)\ndef get_sample(\n    docs_df: pd.DataFrame,\n    topics_df: pd.DataFrame,\n    n_docs: int = 5,\n    n_docs_non_english: int = 2,\n    n_chunks: int = 5,\n    n_topics: int = 5,\n    random_state: int | None = None,\n):\n    random.seed(random_state)\n\n    # Add in some full and chunked documents for single document analysis\n    df = docs_df.copy()\n    test_df = df[df[\"AnswerLang\"] == \"Spanish\"].sample(\n        n=n_docs_non_english, random_state=random_state\n    )\n    test_df[\"AnswerText\"] = test_df[\"AnswerText_NonEnglish\"]\n    test_df = pd.concat(\n        [\n            test_df,\n            df[df[\"AnswerLang\"] == \"English\"].sample(\n                n=n_docs, random_state=random_state\n            ),\n        ]\n    )\n    test_df[\"_seq_id\"] = -1\n    test_df[\"_chunk_id\"] = -1\n    test_df = test_df.rename(columns={\"AnswerText\": \"_text\", \"AnswerLang\": \"_lang\"})\n    test_df = test_df[[\"MLVerbatimId\", \"_seq_id\", \"_chunk_id\", \"_text\", \"_lang\"]]\n\n    chunk_df = df.sample(n=n_chunks, random_state=random_state)\n    chunk_df = chunk_df.rename(columns={\"_chunk\": \"_text\", \"AnswerLang\": \"_lang\"})\n    chunk_df[\"_lang\"] = \"English\"\n    chunk_df = chunk_df[[\"MLVerbatimId\", \"_seq_id\", \"_chunk_id\", \"_text\", \"_lang\"]]\n\n    test_df = pd.concat([test_df, chunk_df])\n    test_df = test_df.drop_duplicates(subset=[\"_text\"], keep=\"first\")\n\n    # Get a list as of dicts\n    test_data_d = test_df.to_dict(orient=\"records\")\n\n    # Randomize the tools and the number of tools being called for variety\n    tool_options = [\"DocumentSummaryTask\", \"DocumentNERTask\", \"DocumentSentimentTask\"]\n    tool_option_weights = get_tool_option_weights(tool_options)\n    for example in test_data_d:\n        tools_to_call = []\n        if example[\"_lang\"] != \"English\":\n            tools_to_call.append(\"TranslationTask\")\n\n        n_tools = random.choices(\n            range(1, len(tool_options) + 1), weights=tool_option_weights, k=1\n        )[0]\n        tools_to_call += random.sample(tool_options, n_tools)\n\n        if len(example[\"_text\"]) &gt;= 940:\n            # For longer documents, vary the inclusion of 'TopicSummary'\n            if random.choice([True, False]):\n                tools_to_call.append(\"TopicSummaryTask\")\n\n        example[\"_n_tools\"] = len(tools_to_call)\n        example[\"_tools\"] = tools_to_call\n        example[\"ask\"] = build_hypothetical_prompt(tools_to_call, is_doc=True)\n\n    # Add in some summaries for related document analsysis\n    df = topics_df.copy()\n    test_df = df[df[\"pred_theme_id\"] != -1].sample(\n        n=n_topics, random_state=random_state\n    )\n    test_df = test_df.rename(columns={\"_chunk\": \"_text\"})\n    test_df = test_df[[\"pred_theme_id\", \"_text\"]]\n\n    test_df = test_df.drop_duplicates(subset=[\"_text\"], keep=\"first\")\n\n    # Get a list as of dicts\n    topics_test_data_d = test_df.to_dict(orient=\"records\")\n\n    # Randomize the tools and the number of tools being called for variety\n    for example in topics_test_data_d:\n        tools_to_call = [\"TopicSummaryTask\"]\n        # Randomly throw in the sentiment task 'DocumentSentimentTask'\n        if random.choice([True, False]):\n            tools_to_call.append(\"DocumentSentimentTask\")\n\n        example[\"_n_tools\"] = len(tools_to_call)\n        example[\"_tools\"] = tools_to_call\n        example[\"ask\"] = build_hypothetical_prompt(tools_to_call, is_doc=False)\n\n    return test_data_d + topics_test_data_d"
  },
  {
    "objectID": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#build-evals-with-braintrust",
    "href": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#build-evals-with-braintrust",
    "title": "LLM Workshop #4 - L1 Evals and Dataset Curation (Part I)",
    "section": "Build Evals with Braintrust",
    "text": "Build Evals with Braintrust\nTo get started, log into braintrust, click the dropdown in the top-left (will have your username in there) and select “+ Create Project”. Give that project a name (e.g., ftcourse_project) and you’re good to go. You’ll need to create an API key and add it to your environmental variables to interact with that project in your code.\n\n\n\n\n\n\nNote\n\n\n\nOne of the things I love about Braintrust is how clean and intuitive the product is. You can tell it is built by folks who have themselves been in the trenches. It isn’t cluttered with a bunch of information I don’t care about, navigation is easy, and I can usually figure out where I need to go and what I need to do without have to consult the documentation. It provides the kind of experience every vendor should aspire too!\nTools like this go a long way in minimizing friction and allowing us to iterate faster.\n\n\n\nIteration 0: Initial Evals\nObjective: Generate a small set of examples (&lt; 25) to validate the evaluation pipeline with some simple scoring functions since we don’t have any expected values to compare our predictions with. We will remedy this by using the output from this iteration to begin building a “golden dataset” (reviewed/corrected inputs and outputs). After review, we’ll be able to incorporate this dataset in further evals and as a training dataset if we decide to finetune our own model.\n\n\n\n\n\n\nImportant\n\n\n\nJust like with training a neural network, the recommendation her is to start with a very small datasent with which you can verify your eval pipeline works as expected.\n\n\nSo lets define a “great” eval. For that we need some data, a task, and some scoring functions.\n\nData\nSee the get_sample() function in the eval prep section above if you’re curious about how I’m building sampled datasets for this project.\n\ndocs_df = pd.read_parquet(\n    f\"{DATA_DIR}/clean/{DATA_FILENAME}_sample_14k_chunked.parquet\"\n)\nprint(len(docs_df))\n# 18955\n\ntopics_df = pd.read_parquet(\n    f\"{DATA_DIR}/clean/{DATA_FILENAME}_sample_14k_topics.parquet\"\n)\nprint(len(topics_df))\n# 976\n\ntest_data = get_sample(\n    docs_df,\n    topics_df,\n    n_docs=3,\n    n_docs_non_english=2,\n    n_chunks=5,\n    n_topics=5,\n    random_state=None,\n)\nprint(len(test_data))\n# 15\n\n\n\n\n\n\n\nNote\n\n\n\nLook at your data … ALOT! I think Hamel is the “look at your data” guy but honestly, I heard it from so many folks in the workshop there might be many “look at your data” guys. Either way, this is solid advice for any ML/AI application.\n\n\nHere you can see the user prompt dynamically created for this example, the number of tools it should call, what tools it should specifically call, along with the text we want to operate on and a unique ID for this collection of documents.\n\nr = test_data[-1]\nr\n\n\n{'pred_theme_id': 159,\n '_text': array([&lt;a list of strings&gt;],dtype=object),\n '_n_tools': 1,\n '_tools': ['TopicSummaryTask'],\n 'ask': 'Based on the documents provided, extract their theme and an action plan based on them'}\n\n\n\nTask\nFor any functions we wanted traces for, we simply decorate those functions with @braintrust.traced. A “trace” is anything you wan’t logged in your experiment tracking system. After we run the acutal eval experiment below I’ll show you where these traces show up in the Braintrust UI.\nHere’s a utility function we can use to engage our LLM. Note the decorator.\n\n@braintrust.traced\ndef ask_ai(\n    client,\n    is_doc: bool,\n    system_msg: str,\n    human_msg: str,\n    model: str = \"gpt-4o\",\n    instructor_kwargs: dict = {},\n) -&gt; DocumentAnalysis | RelatedDocumentAnalysis:\n    try:\n        return client.chat.completions.create(\n            model=model,\n            response_model=DocumentAnalysis if is_doc else RelatedDocumentAnalysis,\n            max_retries=3,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": system_msg,\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": human_msg,\n                },\n            ],\n            **instructor_kwargs,\n        )\n    except Exception as e:\n        print(e)\n\nconvert_analysis_call_to_json takes the tool calls captured in a Pydantic object and formats them into a representation that will make it easier for us to build more complex scoring functions later.\n\ndef convert_analysis_call_to_json(\n    analysis_obj: DocumentAnalysis | RelatedDocumentAnalysis,\n):\n    j = [\n        {\"function_call\": task.__class__.__name__, \"args\": task.model_dump()}\n        for task in analysis_obj.tasks\n    ]\n    return j\n\ngenerate_tool_calls uses Instructor to make our LLM calls and I’ve included options for either using OpenAI (the default) or Anthropic models here.\nThe dictionary return represents the “output” included in our Braintrust traces.\n\n@braintrust.traced\nasync def generate_tool_calls(\n    item_d: dict,\n    ask_attr: str = \"ask\",\n    text_attr: str = \"_text\",\n    tools_attr: str = \"_tools\",\n    model_vender: str = \"openai\",\n    model_name: str = \"gpt-4o\",\n):\n    is_doc = isinstance(item_d[text_attr], str)\n\n    # Prep messages and defined required tools\n    required_tools = item_d[tools_attr]\n    ask = item_d[ask_attr]\n    doc = item_d[text_attr] if is_doc else \"\\n\\n\".join(item_d[text_attr])\n\n    if is_doc:\n        system_msg = dedent(\"\"\"\\\n            Execute each analysis task.\n            Always translate any non-English documents into English before executing other tasks.\"\"\")\n        human_msg = f\"{ask}. Document: {doc}\"\n    else:\n        system_msg = dedent(\"\"\"\\\n            Execute each analysis task.\"\"\")\n        human_msg = f\"{ask}. Documents:\\n{doc}\"\n\n    # Generate the response\n    tool_calls = None\n    error = None\n    try:\n        if model_vender == \"openai\":\n            client = instructor.from_openai(OpenAI())\n            results = ask_ai(\n                client,\n                is_doc,\n                system_msg,\n                human_msg,\n                instructor_kwargs={\"temperature\": 0.0},\n            )\n\n        elif model_vender == \"anthropic\":\n            client = instructor.from_anthropic(Anthropic())\n            results = ask_ai(\n                client,\n                is_doc,\n                system_msg,\n                human_msg,\n                model=\"claude-3-5-sonnet-20240620\",\n                instructor_kwargs={\"max_tokens\": 1024, \"temperature\": 0.0},\n            )\n\n        tool_calls = convert_analysis_call_to_json(results)\n    except Exception as e:\n        error = e\n\n    return {\n        \"prompt\": ask,\n        \"required_tools\": required_tools,\n        \"tool_calls\": tool_calls or [],\n        \"error\": error,\n    }\n\nOne of the many things I love about Braintrust is how easy it is to test the individual parts of your evals locally. Here, we’ll test our “task” function defined above\n\noutput = await generate_tool_calls(\n    item_d=r\n)  # , model_vender=\"anthropic\", model_name=\"claude-3-5-sonnet-20240620\")\noutput\n\n\n{\n    \"prompt\": \"Based on the documents provided, extract their theme and an action plan based on them\",\n    \"required_tools\": [\"TopicSummaryTask\"],\n    \"tool_calls\": [\n        {\n            \"function_call\": \"TopicSummaryTask\",\n            \"args\": {\n                \"theme_name\": \"Campus Recreation and Fitness Programs\",\n                \"action_plan\": [\n                    \"Promote the variety of fitness classes available to all levels.\",\n                    \"Highlight the benefits of free and low-cost classes to students.\",\n                    \"Encourage participation in 'try it before you buy it' promotional weeks.\",\n                    \"Emphasize the positive impact of fitness classes on physical and mental well-being.\",\n                    \"Showcase the ease of registration and the supportive staff.\",\n                ],\n            },\n        }\n    ],\n    \"error\": None,\n}\n\n\n\nScoring\nSince we have no targets, our initial scoring functions will simply inform us if all the expected analysis tasks are being called, if the structured output is valid JSON, and if any errors occurred.\n\ndef overall_no_error(output):\n    return output[\"error\"] is None\n\n\ndef overall_is_valid_json(output):\n    try:\n        json.loads(json.dumps(output[\"tool_calls\"]))\n        return True\n    except ValueError:\n        return False\n\n\ndef overall_called_all_tools_acc(output):\n    required_tools = output[\"required_tools\"]\n\n    tool_calls = output[\"tool_calls\"]\n    actual_tools_used = (\n        [item[\"function_call\"] for item in tool_calls] if tool_calls else []\n    )\n\n    correct_matches = len(set(required_tools).intersection(set(actual_tools_used)))\n    total_possible_matches = len(required_tools)\n\n    accuracy = correct_matches / total_possible_matches\n    return accuracy\n\nAnd just like our “task”, we can test our “scoring” functions locally to ensure all is working as expected before running our full experiment.\n\nprint(overall_no_error(output))\nprint(overall_is_valid_json(output))\nprint(overall_called_all_tools_acc(output))\n\n\nTrue\nTrue\n1.0\n\n\n\nEval\nOk, sick!\nWe have some data, a defined and tested task, and several defined and tested scoring fucntions, its time to run our evals over the entire dataset. All our inputs, outputs, metadata, and metrics will be uploaded to Braintrust for review. Note the inclusion of some “metadata” so we can track what model and what kind of context is used for the examples in this dataset. More on how this can be used in later blog posts.\n\nawait Eval(\n    EVAL_PROJECT_NAME,\n    experiment_name=\"it_000: Initial Dataset\",\n    data=[\n        {\n            \"input\": d,\n            \"metadata\": {\n                \"model_vendor\": \"openai\",\n                \"model_name\": \"gpt-4o\",\n                \"is_single_doc\": isinstance(d[\"_text\"], str),\n            },\n        }\n        for d in test_data\n    ],\n    task=partial(\n        generate_tool_calls\n    ),  # , model_vender=\"anthropic\", model_name=\"claude-3-sonnet-20240229\"),\n    scores=[overall_no_error, overall_is_valid_json, overall_called_all_tools_acc],\n)\n\nWhen this finishes, we’ll see a print out of the results and a link directly to the experiment.\n=========================SUMMARY=========================\n100.00% 'overall_called_all_tools_acc' score\n100.00% 'overall_is_valid_json' score\n100.00% 'overall_no_error' score\n\n29.03s duration\n\nSee results for it_000: Initial Dataset at &lt;link to experiment here&gt;\nCongrats! You just ran your first set of evals. Let’s see what it looks like in BrainTrust."
  },
  {
    "objectID": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#review-results",
    "href": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#review-results",
    "title": "LLM Workshop #4 - L1 Evals and Dataset Curation (Part I)",
    "section": "Review Results",
    "text": "Review Results\nAfter your evals finish running, you can click on the summary link and/or just navigate to the “Experiments” tab of your project in BrainTrust to see the results. Out initial iteration above looks like this:\n\n\n\nExperiment Overview\n\n\nYou can see for our initial experiment against 15 examples, our model performed very well with perfect scores across our three metrics. Clicking on our experiment name will allows us to look at how each example did.\n\n\n\nExperiment Detail\n\n\nFrom here we can look at specific examples by clicking on any one of them.\n\n\n\nExample Details\n\n\nHere we can see both the input provided to the task and it’s output. Notice how each of the methods we decorated with the @braintrust.traced got included here in our trace. This allows us to look at each individually if we want too.\nWhat I’m going to do is select all 15 examples, and add them to a dataset I named “golden dataset”. You can do that pretty easily as pictured below.\n\n\n\nAdding Experiment Data to a Dataset\n\n\nNavigating to the “Datasets” tab, I can click on my “golden dataset” and see the rows I just added. I decided to review each one of these examples and correct their outputs by updating the “Expected” section for each.\n\n\n\nA Dataset\n\n\nIn the configuration sectoin of Braintrust I created a “reviewed-wg” tag I can apply to any rows in a dataset that I have personally reviewed/corrected. I can use this to know what examples need review and also for filtering out non-reviewed rows in this dataset when I use it in subsequent evals."
  },
  {
    "objectID": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#takeaways",
    "href": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#takeaways",
    "title": "LLM Workshop #4 - L1 Evals and Dataset Curation (Part I)",
    "section": "Takeaways",
    "text": "Takeaways\nWe’ve only just begun, but hopefully you can see that Braintrust provides a comprehensive platform for everything we need to build out our evaluation pipeline! It provides …\n\nA mechanism to evaluate performance via scoring functions.\nA mechanism to look at our data and see where things are working well or not so well overall and with specific examples via the Braintrust UI where we can look at overall runs and example-specific metrics.\nThe ability to make modifications to our system and measure how well they improved things or not via subsequent experiments and the ability to look at improvements and regressions on a case-by-case basis.\nThe ability to curate datasets that can be used for better evals and fine tuning via the ability to add the results of experiments to datasets that can be reviewed, corrected, and used for more complex evals and fine tunes\n\nI’m a big fan of the platform and encourage y’all to watch Ankur’s talk from the workshop included below to learn more (I’ve already watched it 3x)."
  },
  {
    "objectID": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#next-steps",
    "href": "posts/2024-07-25-llm-workshop-evals-dataset-curation.html#next-steps",
    "title": "LLM Workshop #4 - L1 Evals and Dataset Curation (Part I)",
    "section": "Next Steps",
    "text": "Next Steps\nInitially, I was planning on showing y’all a couple more iterations in the eval pipeline before I realized this post is already long as hell. Braintrust is an awesome tool and I want to do it justice, and so my next blog post will cover two more experiments I ran before moving on to generate a dataset for fine tuning. In particular, I’ll cover the following:\n\nHow to merge the “golden dataset” with more sampled data to build up our dataset along with better and more task specific scoring functions.\nHow to really “look at the data” and review both improvements and regressions on a case-by-case basis.\nHow to improve our prompts and tool definitions based on what we see and then see how things measure up by comparing results to previous runs.\n\nAnyways, I know this was a lot but I hope it encourages you to adopt a “evals first” type of thinking and also to give Braintrust a go. If you made it this far, thanks for reading!"
  },
  {
    "objectID": "posts/2024-07-03-llm-workshop-intro.html",
    "href": "posts/2024-07-03-llm-workshop-intro.html",
    "title": "LLM Workshop #1 - How to take a course that never ends",
    "section": "",
    "text": "These days, it goes by many names, but officially the “workshop” is known simply as “Mastering LLMs: A Conference For Developers & Data Scientists”.\nInitially envisioned as a four-week course, it quickly evolved into something much more dynamic — a conference brimming with talks, office hours, and a wealth of insights shared on their Discord. Participants also received a generous amount of credits from various companies in the field to experiment on their own.\nAlthough the event has technically concluded, I’m remain only 95% convinced that it’s truly over (a new “30 minute” 90 minute session could pop-up in my calendar at any moment).\nAt it’s core, the event is about equipping participants with the necessary tools and techniques to create a comprehensive pipeline for building generative NLP solutions, from dataset curation to deployment. Its spans 4 core workshops, 20 conference sessions, 6 office hours, and a bunch of async discussions via discord. If that sounds like a lot of content, good … because it is! My friend Sanyam summed my feelings in week 3 with this message I got from him on Discord …\n\nAre you feeling overwhelemed by the course as well? I feel this is fastai by at 1000x speed … but for real world, energy. It’s impossible for me to just keep up with the lectures, barely getting time to play with examples or run some code.\n\nSo yah, to start, I want to share my thoughts on the major lessons learned and help folks figure out how to navigate and succeed in course/workshop/whatever of this scale. This is an opinionated take from someone who’s been around awhile, but I hope an experienced take worth consideration."
  },
  {
    "objectID": "posts/2024-07-03-llm-workshop-intro.html#mastering-llms-a-conference-for-developers-data-scientists",
    "href": "posts/2024-07-03-llm-workshop-intro.html#mastering-llms-a-conference-for-developers-data-scientists",
    "title": "LLM Workshop #1 - How to take a course that never ends",
    "section": "",
    "text": "These days, it goes by many names, but officially the “workshop” is known simply as “Mastering LLMs: A Conference For Developers & Data Scientists”.\nInitially envisioned as a four-week course, it quickly evolved into something much more dynamic — a conference brimming with talks, office hours, and a wealth of insights shared on their Discord. Participants also received a generous amount of credits from various companies in the field to experiment on their own.\nAlthough the event has technically concluded, I’m remain only 95% convinced that it’s truly over (a new “30 minute” 90 minute session could pop-up in my calendar at any moment).\nAt it’s core, the event is about equipping participants with the necessary tools and techniques to create a comprehensive pipeline for building generative NLP solutions, from dataset curation to deployment. Its spans 4 core workshops, 20 conference sessions, 6 office hours, and a bunch of async discussions via discord. If that sounds like a lot of content, good … because it is! My friend Sanyam summed my feelings in week 3 with this message I got from him on Discord …\n\nAre you feeling overwhelemed by the course as well? I feel this is fastai by at 1000x speed … but for real world, energy. It’s impossible for me to just keep up with the lectures, barely getting time to play with examples or run some code.\n\nSo yah, to start, I want to share my thoughts on the major lessons learned and help folks figure out how to navigate and succeed in course/workshop/whatever of this scale. This is an opinionated take from someone who’s been around awhile, but I hope an experienced take worth consideration."
  },
  {
    "objectID": "posts/2024-07-03-llm-workshop-intro.html#how-to-take-this-course",
    "href": "posts/2024-07-03-llm-workshop-intro.html#how-to-take-this-course",
    "title": "LLM Workshop #1 - How to take a course that never ends",
    "section": "How to take this course",
    "text": "How to take this course\nHere I want to share 4 tips to getting the most of this course.\n\n\n\n\n\n\nTip 1: Focus Your Learning\n\n\n\nMy advice is to definitely watch the main conference workshops … TWICE.\nThis is how I do the fastai courses as well. I watch the lecture more as an active participant the first time, watching it live if possible, asking (and answering question), jotting down thoughts or other parts of the lecture I want to dive deeper into later. On my second watch, I usually have a “printed” version of the session notes/slides on my iPad that I annotate as I got through it again a bit slower. I usually do this 2nd watch a few days after to give my brain time to rest and reflect.\nFrom there I suggest following the same approach with any conference talks you’re interested in … but only doing that 2nd watch for those you care about. Honestly, I still haven’t watched all of them myself and have been content to really spend time with those most close to the things I’m working with IRL and/or I want to learn (e.g., I’ve watched Ankur Goyal’s talk on using BrainTrust about 3-4 times because I like the tool and I want to explore it as something I use in both my personal and professional life).\nAs for the office hours, I’ve tried to attend as many as I could and ask a lot of questions. I don’t watch these a 2nd time and I don’t do a lot of detailed note taking except where something really lands as important or useful to me personally (like an answer to one of the really great questions that come up in these).\nTime is precious. One of the keys to succeeding in this course and walking away without a mental breakdown is to manage it by spending your time on the things that matter to you.\n\n\n\n\n\n\n\n\nTip 2: The Discord Is The Alpha\n\n\n\nThe like $3,500 in free credits is great, but for me, the real alpha of this course … and the real bang for your buck … is in the Discord. Spend a lot of time there!\nYou are literally able to interact with other practitioners and course instructors who charge serveral hundered dollars an hour as consultants for the same kind of expertise and guidance you get for your one-time payment of $500. For me, this puts the value of the course into the tens of thousands easily, making my call to signup easily one of the best decision I’ve made in 2024.\nAgain, there is A LOT of content in the discord so my recommendation is to focus on the channels dedicated to things you care about vs. trying to read it all. Some of my favorites include:\n\nWorkshops (all of them)\nDebugging Help / training-runs (lots of learning from folks like Zach Mueller and others via real fine-tuning attempts; very helpful)\nDebugging Help / axolotl\nTalks / kylecorbitt_prompt_to_model\nTalks / ankurgoyal_textsql_llms\nTalks / jason_improving_rag\nTalks / clavie_beyond_rag_basics (deserved way more than 30 minutes, 😁)\nTalks / pawel-function-calling\nTalks / whitaker_napkin_math\nOffice Hours / charles-modal\nOffice Hours / replicate\n\nAgain, time is precious. Since my interests were with evals, structured outputs, RAG, and deployment … the above were the channels I spent most of my time in. Believe it or not, there are a few channels that remain unexplored for the time being (so much great stuff).\n\n\n\n\n\n\n\n\nTip 3: How time works in this course\n\n\n\nTime works different in the workshop. For example, a “30 minute” session is really about 75 minutes, a “45 minute” session about 90 minutes, and any of the “2-hour” workshop will likely go an additonal hour.\nNow that this course is “over”, this may not be that big of a deal. But, when I was blocking out time in my calendar these were the heuristics I followed. Think on this when that next calendar invite pops up or you end up taking some similar course in the future.\n\n\n\n\n\n\n\n\nTip 4: Build something\n\n\n\nIf you can do only one thing … build something and blog it!\nThis is really the golden rule of course like the LLM Workshop and fast.ai. If you really want to gain some mastery in the subject, you have to build something and explain it to others.\nA great example of this in practice can be found by following Alex Strick van Linschoten’s blog posts regarding his course project. Exceptional content and an example of what it looks like to succeed in a course like this. Its a win-win for Alex and the community at large.\nThis is the direction I’ll be going in future posts in this series."
  },
  {
    "objectID": "posts/2024-07-03-llm-workshop-intro.html#key-takeaways",
    "href": "posts/2024-07-03-llm-workshop-intro.html#key-takeaways",
    "title": "LLM Workshop #1 - How to take a course that never ends",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nI’ve learned a lot to say the least. Below are some of the key takeways I’ve taken away from the course that I hope will aid your navigation and success with it as well.\n\nTrust fucking no one (TFNO)\n\n\n“Trust fucking no one!” - Charles Frye (also winner of quote of the conference)\nIndeed, when it comes to fine-tuning … “If you’re not nervous, you don’t understand.”\nIf you weren’t scared as hell about getting your prompt templates to match up at both training and inference time already, be prepared to be terrified. And if you’re not carefully looking at examples of your fleshed out prompts during both phases, prepare to get F’d.\n\n 99% of errors happen with the template\n\n\n\n\nLook at your data\nYou need to spend time looking at your data to understand what is going on, gain intuiton on how to improve things, and actually make things better. Automated metrics can only take you so far in the generative game and even high-quality LLMs as judges will fail. Look at your data a lot!\n\n\nBuild an Evals-First Development Mindset\nEvals are a big part of this course and likely one of the most ignore aspects in building generative solutions where the normal course seems to be for folks to just slap a react app on top of a fastapi backend that makes calls out to gpt-4 with a prompt and RAG mechanism that seems to work. But how can you tell if it is really “working”? How can you tell it is still working and working for others besides yourself? How can you tell changes you make to your model, prompt, RAG, whatever is having a positive or negative effect?\nThe answer is you can’t without good evals, and good evals are something built up over time and a project in and of themselves. They aren’t static. Having a good system to build and continuously improve on them is one of the most important ways you can ensure you are delivering a quality AI solution.\nAlso, don’t forget to include human experts where possible!\n\n\nProve You Need to Finetune\n\n Try not to finetune first … you need to prove to yoruefl that you should finetune [using] some minimal evaluation system and you hit a wall and can’t make progress by just prompting\n\nGood use cases for finetuning:\n\nOwning your own model\nData privacy\nDomain specific things that models like GPT and Claude haven’t been trained on\n\nThe narrower the domain/problem … the better.\n\n\nDon’t Try and Build a General Purpose Chatbot\nBeing asked to build a chatbot is a smell because the surface area is very large and unscoped, making it difficult to make progress on. We simply do not have the time, money, data, and resources companies like OpenAI and Anthropic have to make a chatbot that can perform as well as theirs in general … and no one wants to use an AltaVista like chatbot when you got Google.\nI’m watching this happen in real-time at work and its not pretty.\n\n\nUse your Eval Pipeline to Curate Training Datasets\nIn the real-world, we can’t just start with pre-processed dataset from Hugging Face. In the real-world, we’ll often have to find creative and out of the box ways to curate our own dataset to evaluating our AI systems. Having a good evals framework is how you get this done, especially at the beginning where you probably don’t have access to many, if any, human experts providing examples for you to use. Starting with a good LLM from OpenAI or Anthropic for example, along with a growing set of scoring functions and some time of your own for review, is a great way to curate an initial training dataset you can build on going forward.\nBreakdown use cases into specific examples, log the traces, and use them in your finetunes.\nWhat LLMs should I use in generating synthetic data?\n\n I like mistral large … use the most powerful model you can”\n\n\n\nEval/Logging Tools I like (so far)\nI’m going to give BrainTrust and LangSmith a go. I’m most familiar with the later, but I really liked the UX and explict-ness of running evals with BrainTrust.\n\n\nMinimize Friction\nJust like analysis paralysis, there’s an eval paralysis where you might be so consumed on having a fully fleshed out eval pipeline or the feeling you have to pick out the right tooling that you never really get started. Don’t let that happen. Start with simple assertions, use the high-quality LLMs, use notebooks … do whatever you can to get a quality dataset you can get going with quickly\n\n\nGet Up and Running Quick\nNo one wants to deploy a training job with 100k samples for fine-tuning a Llama3 70B only to find out that it fails after a day or so of running because you misspelled your HuggingFace username. Here’s my approach to quickly iterate and verify your finetuning strategy will work from beginning to end (learned from my time with the fast.ai course btw)\n\n Use a subset of my dataset to train a model. Verify my inputs/targets look right, training works end-to-end, that my metrics are being logged to wanb correctly, and that my artifacts can be used and optionally uploaded to HF. I make my sample really small … like maybe 300-500 examples so I can iterate fast here. Once my setup is golden and everything is checked into github, login into Jarvis, grab yourself some big GPUS, clone your repo, install any libraries you need to, and train on your full dataset. Pour yourself a glass of a good scotch or whiskey.\n\nTry to get it down to 15-30 mins max to verify your training works from start to finish. If you are using axolotl, take set max_steps to something small like 500.\nHow much data do you need?\nI’ve heard as little as 50-100 examples can get you far.\n\n\nPrefer Cloud over Local Training\nI had a hell of a time getting CUDA installed and getting things to run on my 2x3090s. I don’t regret it, in fact, I get some sick satisfaction out of it. There’s nothing like launching a training job on my two blower GPUs as my wife sits down to watch an episode of Bridgerton on the TV downstairs (under which sits my DL rig).\nHowever, I see lots of students struggling so hard on trying to get things working locally that they become beaten down before getting to the real meat of things. So my 2 cents for most folks in genera …\n\n Its probably better to use a cloud provider like jarvis or modal rather than battling with your local rig. Of course, I didn’t follow my own advice because I’m OCD about things like this and can’t let things go until I understand the whys, hows, and resolutions for problems like this.\n\nI’ve had excellent experience working with both of these platforms, along with superb support and help from folks like Vishnu (founder at Jarvis), and Charles (AI engineer at Modal Labs and speaker at every AI conference).\nAlso, use Linux. I see so many folks trying to get things working on Windows or their slick M3 Macbook pro and that is just a pathway to misery that will make you question whether you want to even be in the ML field. Don’t do it. Linux FTW\n\n\nPrefer the Base model over the Instruct Version When Fine-tuning\nMost LLMs come in two flavors, base and instruct. The former focuses simply on next-word prediction tasks whereas the later is focused on conversation. In general, the recommendation in the course is to use the base version to build on top of as it gives you complete control of the prompt template without having to worry about following the “instruct” template of whatever model architecture you choose to the tee. Also, as these instruct tempaltes vary widely from one LLM to another, its seems using the base model make it easier to run fine-tunes against a nubmer of LLMs without a lot of code gymnastics.\nInsturction models have been finetuned for conversation … something you want to chat with.\nStart with the basse model where possible … with the smallest model possible (e.g, the 7/8B parameter range)\n\n\nFinetuning Examples Should Mimic what your LLM will see in production\nIf RAG is being used with your properitery LLMs, include it in your training examples for use in finetuning.\n\n\nFor Structured Output, Use Instructor or Outlines\n\n For open models you should use outlines. for closed models APIs you should use instructor.\n\n\n Instructor uses prompting and retries to achieve the desired output (and with openai it will prompt intelligently via the function schema definitions). Outlines clamp down the model predictions to only alllow permissible tokens according to your grammar. Outlines is going to have much stronger guarantees that it will work. No retries are even necessary. If outlines is compatible with your model (you have access to the forward pass because you own the model) you should use outlines"
  },
  {
    "objectID": "posts/2024-07-03-llm-workshop-intro.html#next-steps",
    "href": "posts/2024-07-03-llm-workshop-intro.html#next-steps",
    "title": "LLM Workshop #1 - How to take a course that never ends",
    "section": "Next Steps",
    "text": "Next Steps\nBuild something.\nFollowing Hamel’s course project repo as a guide, I’m going to explore improving a ML pipeline I built for work that allows users to upload a collection of survey comments, interview answers, etc… and have a variety of NLP analysis performed on the documents collectively and individually (eg., translation, sentiment, summarization, NER, and thematic analysis). The data and responses are styled in a way familiar to faculty, staff, and students in higher ed.\nThe current system makes a call out to gpt-4 for every document and for every collection of related documents to perform each task. It can get pricey and can take a long time depending on the number of documents being processed. I did this because I couldn’t get reliable results from asking the LLM to perform a number of tasks, whether the document/s where short or long.\nFor this course project, I’m going to start from the beginning and try to improve things by:\n\nCurating a dataset from both generated data and local traces I have from the existing pipeline\nFollow and “eval first” development structure, so I can evaluate the proprietary LLMs (e.g., gpt-4, gpt-4o, gpt-3.5-turbo, etc..) I’m using right now.\nUse tool calling and more complex pydantic models to reduce the number of calls I need to make and lower costs.\nFinetune a variety of smaller models on a curated datset to see if I can improve the quality of my predictions while also lowering costs and latency even more.\n\nIf folks have any suggestions or ideas to improve this approach, please let me know in the comments or on twitter. I’d especially love to hear from people who have gone down a similar path and what worked (or didn’t) for them."
  },
  {
    "objectID": "posts/2020-05-23-text-generation-with-blurr.html",
    "href": "posts/2020-05-23-text-generation-with-blurr.html",
    "title": "Summarization with blurr",
    "section": "",
    "text": "# only run this cell if you are in collab\n# !pip install transformers -Uqq\n# !pip install datasets -Uqq\n# !pip install bert-score -Uqq\n# !pip install sacremoses\n# !pip install ohmeow-blurr -Uqq\n\nRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.53)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\nRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\nimport datasets\nimport pandas as pd\nfrom fastai.text.all import *\nfrom transformers import *\n\nfrom blurr.text.data.all import *\nfrom blurr.text.modeling.all import *\n\nimport nltk\nnltk.download('punkt', quiet=True)\n\nTrue"
  },
  {
    "objectID": "posts/2020-05-23-text-generation-with-blurr.html#data-preparation",
    "href": "posts/2020-05-23-text-generation-with-blurr.html#data-preparation",
    "title": "Summarization with blurr",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe’re going to use to use the datasets library from huggingface to grab your raw data. This package gives you access to all kinds of NLP related datasets, explanations of each, and various task specific metrics to use in evaluating your model. The best part being everything comes down to you in JSON! This makes it a breeze to get up and running quickly!\nWe’ll just use a subset of the training set to build both our training and validation DataLoaders\n\nraw_data = datasets.load_dataset('cnn_dailymail', '3.0.0', split='train[:1%]')\n\nReusing dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n\n\n\ndf = pd.DataFrame(raw_data)\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\narticle\nhighlights\nid\n\n\n\n\n0\nLONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something simila...\nHarry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\n42c027e4ff9730fbb3de84c1af0d2c506e41c3e4\n\n\n1\nEditor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most severe mental illnesses are incarcerated until they're ready to appear in court. Most often, they...\nMentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he's fighting for change .\nee8871b15c50d0db17b0179a6d2beab35065f1e9\n\n\n2\nMINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the other just completely gave way, fell all the way down,\" survivor Gary Babineau told CNN. \"I probably had a 30-, 35-foot free fall. And there's cars in the water, there's cars on fire. The whole bridge is down.\" He said his back was injured but he determined he could move around. \"I realized there was a school bus right next to me, and me and a couple of other guys went over and started lifting the kids off t...\nNEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collapsed during rush hour Wednesday .\n06352019a19ae31e527f37f7571c6dd7f0c5da37\n\n\n3\nWASHINGTON (CNN) -- Doctors removed five small polyps from President Bush's colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent to the National Naval Medical Center in Bethesda, Maryland, for routine microscopic examination, spokesman Scott Stanzel said. Results are expected in two to three days. All were small, less than a centimeter [half an inch] in diameter, he said. Bush is in good humor, Stanzel said, and will resume his activities at Camp David. During the procedure Vice President Dick Cheney assumed presidential power. Bu...\nFive small polyps found during procedure; \"none worrisome,\" spokesman says .\\nPresident reclaims powers transferred to vice president .\\nBush undergoes routine colonoscopy at Camp David .\n24521a2abb2e1f5e34e6824e0f9e56904a2b0e88\n\n\n4\n(CNN) -- The National Football League has indefinitely suspended Atlanta Falcons quarterback Michael Vick without pay, officials with the league said Friday. NFL star Michael Vick is set to appear in court Monday. A judge will have the final say on a plea deal. Earlier, Vick admitted to participating in a dogfighting ring as part of a plea agreement with federal prosecutors in Virginia. \"Your admitted conduct was not only illegal, but also cruel and reprehensible. Your team, the NFL, and NFL fans have all been hurt by your actions,\" NFL Commissioner Roger Goodell said in a letter to Vick....\nNEW: NFL chief, Atlanta Falcons owner critical of Michael Vick's conduct .\\nNFL suspends Falcons quarterback indefinitely without pay .\\nVick admits funding dogfighting operation but says he did not gamble .\\nVick due in federal court Monday; future in NFL remains uncertain .\n7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nWe begin by getting our hugginface objects needed for this task (e.g., the architecture, tokenizer, config, and model). We’ll use blurr’s get_hf_objects helper method here.\n\npretrained_model_name = \"facebook/bart-large-cnn\"\nhf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, model_cls=BartForConditionalGeneration)\n\nhf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n\n('bart',\n transformers.models.bart.configuration_bart.BartConfig,\n transformers.models.bart.tokenization_bart_fast.BartTokenizerFast,\n transformers.models.bart.modeling_bart.BartForConditionalGeneration)\n\n\nNext we need to build out our DataBlock. Remember tha a DataBlock is a blueprint describing how to move your raw data into something modelable. That blueprint is executed when we pass it a data source, which in our case, will be the DataFrame we created above. We’ll use a random subset to get things moving along a bit faster for the demo as well.\nNotice that the blurr DataBlock as been dramatically simplified given the shift to on-the-fly batch-time tokenization. All we need is to define a single Seq2SeqBatchTokenizeTransform instance, optionally passing a list to any of the tokenization arguments to differentiate the values for the input and summary sequences. In addition to specifying a custom max length for the inputs, we can also do the same for the output sequences … and with the latest release of blurr, we can even customize the text generation by passing in text_gen_kwargs.\nWe pass noop as a type transform for our targets because everything is already handled by the batch transform now.\n\ntext_gen_kwargs = default_text_gen_kwargs(hf_config, hf_model, task='summarization'); text_gen_kwargs\n\n{'bad_words_ids': None,\n 'bos_token_id': 0,\n 'decoder_start_token_id': 2,\n 'diversity_penalty': 0.0,\n 'do_sample': False,\n 'early_stopping': True,\n 'encoder_no_repeat_ngram_size': 0,\n 'eos_token_id': 2,\n 'exponential_decay_length_penalty': None,\n 'forced_bos_token_id': 0,\n 'forced_eos_token_id': 2,\n 'length_penalty': 2.0,\n 'max_length': 142,\n 'min_length': 56,\n 'no_repeat_ngram_size': 3,\n 'num_beam_groups': 1,\n 'num_beams': 4,\n 'num_return_sequences': 1,\n 'output_attentions': False,\n 'output_hidden_states': False,\n 'output_scores': False,\n 'pad_token_id': 1,\n 'remove_invalid_values': False,\n 'repetition_penalty': 1.0,\n 'return_dict_in_generate': False,\n 'temperature': 1.0,\n 'top_k': 50,\n 'top_p': 1.0,\n 'typical_p': 1.0,\n 'use_cache': True}\n\n\n\nhf_batch_tfm = Seq2SeqBatchTokenizeTransform(\n    hf_arch, hf_config, hf_tokenizer, hf_model, max_length=256, max_tgt_length=130, text_gen_kwargs=text_gen_kwargs\n)\n\nblocks = (Seq2SeqTextBlock(batch_tokenize_tfm=hf_batch_tfm), noop)\ndblock = DataBlock(blocks=blocks, get_x=ColReader('article'), get_y=ColReader('highlights'), splitter=RandomSplitter())\n\n\ndls = dblock.dataloaders(df, bs=2)\n\n\nlen(dls.train.items), len(dls.valid.items)\n\n(2297, 574)\n\n\nIt’s always a good idea to check out a batch of data and make sure the shapes look right.\n\nb = dls.one_batch()\nlen(b), b[0]['input_ids'].shape, b[1].shape\n\n(2, torch.Size([2, 256]), torch.Size([2, 66]))\n\n\nEven better, we can take advantage of blurr’s TypeDispatched version of show_batch to look at things a bit more intuitively. We pass in the dls via the dataloaders argument so we can access all tokenization/modeling configuration stored in our batch transform above.\n\ndls.show_batch(dataloaders=dls, max_n=2)\n\n\n\n\n\ntext\ntarget\n\n\n\n\n0\n&lt;s&gt; You wanted to know more about greenwashing, and Scot Case, from environmental marketing firm TerraChoice, answered. Greenwashing expert Scot Case of TerraChoice. \"Why are green products often more expensive than ones that don't say they are green or environmentally friendly? Is it just because green has become a new form of 'premium brand'? Isn't this bad news if we want to make more people environmentally aware when they go shopping?\" Harriet Gladwell. Case: First, it should be noted that not all greener products are more expensive. The remanufactured toner cartridges I purchase at a nationwide office-supply store, for example, carry the same warranty as other cartridges at a 30-percent lower cost. This greener option is less expensive because the manufacturer avoids the cost of manufacturing the plastic and electronic components. They simply reuse the parts from recycled cartridges. There are also greener products that do not cost extra. There are cleaning products and paints, for example, that have been certified as meeting tough environmental standards by EcoLogo or Green Seal that deliver the same high-quality performance one expects without costing any extra. Other greener products might be slightly more expensive initially, but generate substantial savings for the consumer. Energy-efficient compact fluorescent lightbul&lt;/s&gt;\nScot Case answers your questions on greenwashing.\\nHas green become a new form of \"premium brand\"?\\nWhat green words and phrases should raise a red flag?\\nClick here to read more answers to your questions.\n\n\n1\n&lt;s&gt; (CNN) -- Commentators who have watched the conflict in Northern Ireland play out for decades call the peace process a miracle. Various leaders negotiated for years to bring an end to Northern Ireland's \"troubles.\" Culminating in a power sharing deal between Ulster's unionists, led by Ian Paisley, and Sinn Fein, the political arm of the IRA (nationalists), led by Gerry Adams, the road to peace has been a torturous one characterized by violence, set-backs and numerous false starts. Only recently the Ulster Defence Association, Northern Ireland's largest loyalist group, said it will cease to be an armed paramilitary group, starting at midnight on November 11, saying the \"war is over.\" \"All weaponry will be put beyond use,\" Colin Halliday of the Ulster Political Research Group, which is linked to the group, said in a speech in Belfast aired by RTE, Ireland's state-owned broadcaster. \"The struggle to maintain the union is on a new and more complex battlefield.\" The Irish Republican Army (IRA) disarmed two years ago, helping to restore the province's government in Belfast. Irish Prime Minister Bertie Ahern said the most recent moves of groups to disarm was \"significant and hopefully signals a further step&lt;/s&gt;\nSome commentators have called the peace process in Northern Ireland a \"miracle\"\\nIt culminated in a power sharing deal between the Ulster's unionists and Sinn Fein.\\nPeace and prosperity would not have occurred without diplomacy."
  },
  {
    "objectID": "posts/2020-05-23-text-generation-with-blurr.html#training",
    "href": "posts/2020-05-23-text-generation-with-blurr.html#training",
    "title": "Summarization with blurr",
    "section": "Training",
    "text": "Training\nWe’ll prepare our BART model for training by wrapping it in blurr’s BaseModelWrapper object and using the callback, BaseModelCallback, as usual. A new Seq2SeqMetricsCallback object allows us to specify Seq2Seq metrics we want to use, things like rouge and bertscore for tasks like summarization as well as metrics such as meteor, bleu, and sacrebleu for translations tasks. Using huggingface’s metrics library is as easy as specifying a metrics configuration such as below.\nOnce we have everything in place, we’ll freeze our model so that only the last layer group’s parameters of trainable. See here for our discriminitative learning rates work in fastai.\nNote: This has been tested with ALOT of other Seq2Seq models; see the docs for more information.\n\nseq2seq_metrics = {\n        'rouge': {\n            'compute_kwargs': { 'rouge_types': [\"rouge1\", \"rouge2\", \"rougeL\"], 'use_stemmer': True },\n            'returns': [\"rouge1\", \"rouge2\", \"rougeL\"]\n        },\n        'bertscore': {\n            'compute_kwargs': { 'lang': 'en' },\n            'returns': [\"precision\", \"recall\", \"f1\"]\n        }\n    }\n\n\nmodel = BaseModelWrapper(hf_model)\nlearn_cbs = [BaseModelCallback]\nfit_cbs = [Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)]\n\nlearn = Learner(dls, \n                model,\n                opt_func=ranger,\n                loss_func=CrossEntropyLossFlat(),\n                cbs=learn_cbs,\n                splitter=partial(blurr_seq2seq_splitter, arch=hf_arch)).to_fp16()\n\nlearn.create_opt() \nlearn.freeze()\n\nStill experimenting with how to use fastai’s learning rate finder for these kinds of models. If you all have any suggestions or interesting insights to share, please let me know. We’re only going to train the frozen model for one epoch for this demo, but feel free to progressively unfreeze the model and train the other layers to see if you can best my results below.\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=9.120108734350652e-05)\n\n\n\n\n\n\n\n\n\nIt’s also not a bad idea to run a batch through your model and make sure the shape of what goes in, and comes out, looks right.\n\nb = dls.one_batch()\npreds = learn.model(b[0])\nlen(preds),preds[0], preds[1].shape\n\n(3,\n tensor(3.7155, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;),\n torch.Size([2, 60, 50264]))\n\n\n\nlearn.fit_one_cycle(1, lr_max=3e-5, cbs=fit_cbs)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nrouge1\nrouge2\nrougeL\nbertscore_precision\nbertscore_recall\nbertscore_f1\ntime\n\n\n\n\n0\n1.717729\n1.666989\n0.390802\n0.173328\n0.270079\n0.881220\n0.898129\n0.889528\n16:32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now we can look at the generated predictions using our text_gen_kwargs above\n\nlearn.show_results(learner=learn, max_n=2)\n\n\n\n\n\n\n\n\n\n\n\n\ntext\ntarget\nprediction\n\n\n\n\n0\n(CNN) -- To some, she was a lifelong role model. Others call Pakistan's former prime minister and the first female prime minister of any Muslim nation a source of inspiration to women everywhere. One I-Reporter acknowledges, \"I never was a political supporter of Benazir Bhutto but now after her death I feel that her loss is a loss for Pakistan, not just her political supporters.\" Anthony G. Moore photographed Benazir Bhutto with her husband Asif Ali Zardari in New York in 2006. Benazir Bhutto was assassinated during a suicide bombing on December 27, 2007, and I-Reporters from all over the world responded with their memories and condolences. Below are selections, some of which have been edited for length and clarity. Farhad Sethi of Lahore, Pakistan Breathing in the air of grief and sadness, the nation suffers the loss of our beloved leader Benazir Bhutto, an institution in herself withstanding pressures at times when suicide bombing has become an unstoppable enigma. A sniper pierced a bullet through her neck and our enthusiastic leader couldn't even make it to the hospital, her last words God knows what were they but her face and inspirational personality will always be remembered. A\nFormer Pakistani Prime Minister Benazir Bhutto is assassinated.\\nI-Reporters from around the world offer condolences, memories.\\n\"We Pakistanis are proud to acknowledge her,\" says one I-Reporter.\\nI-Report: Share your memories, condolences, photos of Benazir Bhutto.\n[ Benazir Bhutto was assassinated during a suicide bombing on December 27, 2007 .\\nI-Reporters from all over the world responded with their memories and condolences .\\n\"Her loss is a loss for Pakistan, not just her political supporters,\" one I-Reporter said ., Men's college basketball season comes to an end with the NCAA Tournament .\\nDuke University in Durham, North Carolina, and the University of North Carolina in Chapel Hill are two of the top basketball towns .\\nCollege basketball is deeply rooted in North Carolina culture, thanks to the success of the two universities .\\nNorth Carolina and Duke fans are passionate and passionate about the sport .\\nThe sport has its roots in the YMCA, where James Naismith invented basketball .]\n\n\n\n\n\nEven better though, blurr augments the fastai Learner with a blurr_summarize method that allows you to use huggingface’s PreTrainedModel.generate method to create something more human-like.\n\ntest_article = \"\"\"\nThe past 12 months have been the worst for aviation fatalities so far this decade - with the total of number of people killed if airline \ncrashes reaching 1,050 even before the Air Asia plane vanished. Two incidents involving Malaysia Airlines planes - one over eastern Ukraine and the other in the Indian Ocean - led to the deaths of 537 people, while an Air Algerie crash in Mali killed 116 and TransAsia Airways crash in Taiwan killed a further 49 people. The remaining 456 fatalities were largely in incidents involving small commercial planes or private aircraft operating on behalf of companies, governments or organisations. Despite 2014 having the highest number of fatalities so far this decade, the total number of crashes was in fact the lowest since the first commercial jet airliner took off in 1949 - totalling just 111 across the whole world over the past 12 months. The all-time deadliest year for aviation was 1972 when a staggering 2,429 people were killed in a total of 55 plane crashes - including the crash of Aeroflot Flight 217, which killed 174 people in Russia, and Convair 990 Coronado, which claimed 155 lives in Spain. However this year's total death count of 1,212, including those presumed dead on board the missing Air Asia flight, marks a significant rise on the very low 265 fatalities in 2013 - which led to it being named the safest year in aviation since the end of the Second World War. Scroll down for videos. Deadly: The past 12 months have been the worst for aviation fatalities so far this decade - with the total of number of people killed if airline crashes reaching 1,158 even before the Air Asia plane (pictured) vanished. Fatal: Two incidents involving Malaysia Airlines planes - one over eastern Ukraine (pictured) and the other in the Indian Ocean - led to the deaths of 537 people. Surprising: Despite 2014 having the highest number of fatalities so far this decade, the total number of crashes was in fact the lowest since the first commercial jet airliner took off in 1949. 2014 has been a horrific year for Malaysia-based airlines, with 537 people dying on Malaysia Airlines planes, and a further 162 people missing and feared dead in this week's Air Asia incident. In total more than half the people killed in aviation incidents this year had been flying on board Malaysia-registered planes. In January a total of 12 people lost their lives in five separate incidents, while the same number of crashes in February killed 107. \n\"\"\"\n\nWe can override the text_gen_kwargs we specified for our DataLoaders when we generate text using blurr’s Learner.blurr_generate method\n\noutputs = learn.blurr_summarize(test_article, early_stopping=True, num_beams=4, num_return_sequences=3)\n\nfor idx, o in enumerate(outputs):\n    print(f'=== Prediction {idx+1} ===\\n{o}\\n')\n\n=== Prediction 1 ===\n{'summary_texts': [' 2014 has been worst year for aviation fatalities so far this decade - with 1,158 deaths .\\nTotal death count of 1,212, including those presumed dead on board missing Air Asia flight, marks a significant rise on the very low 265 fatalities in 2013 .\\nThe total number of crashes was in fact the lowest since the first commercial jet airliner took off in 1949 - totalling just 111 across the whole world over the past 12 months .\\nTwo incidents involving Malaysia Airlines planes - one over eastern Ukraine and the other in the Indian Ocean - led to the deaths of 537 people .', ' 2014 has been worst year for aviation fatalities so far this decade - with 1,158 deaths .\\nTotal death count of 1,212, including those presumed dead on board missing Air Asia flight, marks a significant rise on the very low 265 fatalities in 2013 .\\nThe total number of crashes was in fact the lowest since the first commercial jet airliner took off in 1949 - totalling just 111 across the whole world over the past 12 months .\\n2014 has been a horrific year for Malaysia-based airlines, with 537 people dying on Malaysia Airlines planes .', ' 2014 has been worst year for aviation fatalities so far this decade - with 1,158 deaths .\\nTotal death count of 1,212, including those presumed dead on board missing Air Asia flight, marks a significant rise on the very low 265 fatalities in 2013 .\\nThe total number of crashes was in fact the lowest since the first commercial jet airliner took off in 1949 - totalling just 111 across the whole world over the past 12 months .']}\n\n\n\nWhat about inference? Easy!\n\nlearn.metrics = None\nlearn.export(fname='ft_cnndm_export.pkl')\n\n\ninf_learn = load_learner(fname='ft_cnndm_export.pkl')\ninf_learn.blurr_summarize(test_article)\n\n[{'summary_texts': ' 2014 has been worst year for aviation fatalities so far this decade - with 1,158 deaths .\\nTotal death count of 1,212, including those presumed dead on board missing Air Asia flight, marks a significant rise on the very low 265 fatalities in 2013 .\\nThe total number of crashes was in fact the lowest since the first commercial jet airliner took off in 1949 - totalling just 111 across the whole world over the past 12 months .\\nTwo incidents involving Malaysia Airlines planes - one over eastern Ukraine and the other in the Indian Ocean - led to the deaths of 537 people .'}]"
  },
  {
    "objectID": "posts/2020-05-23-text-generation-with-blurr.html#thats-it",
    "href": "posts/2020-05-23-text-generation-with-blurr.html#thats-it",
    "title": "Summarization with blurr",
    "section": "That’s it",
    "text": "That’s it\nblurr supports a number of huggingface transformer model tasks in addition to summarization (e.g., sequence classification , token classification, and question/answering, causal language modeling, and transation). The docs include examples for each of these tasks if you’re curious to learn more.\nFor more information about ohmeow or to get in contact with me, head over to ohmeow.com for all the details.\nThanks!"
  },
  {
    "objectID": "posts/2021-06-02-how-to-submit-a-pr-to-fastai.html",
    "href": "posts/2021-06-02-how-to-submit-a-pr-to-fastai.html",
    "title": "Contributing to fastai: Setup your local development environment & submit a PR",
    "section": "",
    "text": "Install the github CLI, gh. Instructions for all the OS flavor are here!\n\nThe CLI makes it trivial to work on open source projects. In particular, it really shines when making a PR as you’ll see below. Here’s the link to the cli’sexcellent documentation which you’ll likely be referring to again and again (so keep it handy).\n\nClone the fastai repo locally\n\ngh repo clone https://github.com/fastai/fastai.git\ncd ./fastai\n\nBuild your conda environment (I’m using mamba based on Jeremy Howard’s recommendation)\n\nMamba is a makes issuing conda commands faster! Basically just replace conda with mamba whenever you are working with packages in your environment.\nmamba env create -f environment.yml\n\nActivate the environment (you want to make sure you’re in the fastai environment going forward)\n\nconda activate fastai\n\nInstall Jupyter and extensions into your fastai environment (I do this all the time because it leads to less problems when trying to use a base install of jupyter notebook for everything)\n\nmamba install -c conda-forge notebook\nmamba install -c conda-forge jupyter_contrib_nbextensions\n\nInstall nbdev and run the nbdev_install_git_hooks script per the fastai docs\n\nmamba install -c fastai nbdev\nnbdev_install_git_hooks\n\nCreate a symlink from /nbs/fastai to fastai to make sure the notebooks can find the fastai library which is up one level from the notebooks\n\ncd ./nbs\nln -s ../fastai fastai\ncd ..\nAt this point your local development environment is good to go! Run jupyter notebook, open your browser, and head over to the /nbs folder to begin.\n\n\n\nWith the github CLI, its amazingly easy! Once you’ve made your changes and added your unit tests all you have to do is:\n\nMake sure you’re local repo is up-to-date BEFORE you start working. In fact, this is a good command to run periodically so you don’t have to deal with any conflicts once you make your PR.\n\ngit pull\n\nCommit your changes to git\n\ngit commit -am 'My amazing addition to fastai here'\n\nSubmit a PR using gh\n\ngh pr create --title \"My amaizing change\" --body \"Here's what you need to know about it!\"\nThere are actually a few ways to issue the pr, but the above is the easiest. Check out the gh pr create docs for more options.\nOnce you do this, you’ll be asked some questions about where to push the branch and offer an option to fork the base repository under your own account. Easy peasy friends.\nCongrats! You’re a contributor now."
  },
  {
    "objectID": "posts/2021-06-02-how-to-submit-a-pr-to-fastai.html#steps",
    "href": "posts/2021-06-02-how-to-submit-a-pr-to-fastai.html#steps",
    "title": "Contributing to fastai: Setup your local development environment & submit a PR",
    "section": "",
    "text": "Install the github CLI, gh. Instructions for all the OS flavor are here!\n\nThe CLI makes it trivial to work on open source projects. In particular, it really shines when making a PR as you’ll see below. Here’s the link to the cli’sexcellent documentation which you’ll likely be referring to again and again (so keep it handy).\n\nClone the fastai repo locally\n\ngh repo clone https://github.com/fastai/fastai.git\ncd ./fastai\n\nBuild your conda environment (I’m using mamba based on Jeremy Howard’s recommendation)\n\nMamba is a makes issuing conda commands faster! Basically just replace conda with mamba whenever you are working with packages in your environment.\nmamba env create -f environment.yml\n\nActivate the environment (you want to make sure you’re in the fastai environment going forward)\n\nconda activate fastai\n\nInstall Jupyter and extensions into your fastai environment (I do this all the time because it leads to less problems when trying to use a base install of jupyter notebook for everything)\n\nmamba install -c conda-forge notebook\nmamba install -c conda-forge jupyter_contrib_nbextensions\n\nInstall nbdev and run the nbdev_install_git_hooks script per the fastai docs\n\nmamba install -c fastai nbdev\nnbdev_install_git_hooks\n\nCreate a symlink from /nbs/fastai to fastai to make sure the notebooks can find the fastai library which is up one level from the notebooks\n\ncd ./nbs\nln -s ../fastai fastai\ncd ..\nAt this point your local development environment is good to go! Run jupyter notebook, open your browser, and head over to the /nbs folder to begin.\n\n\n\nWith the github CLI, its amazingly easy! Once you’ve made your changes and added your unit tests all you have to do is:\n\nMake sure you’re local repo is up-to-date BEFORE you start working. In fact, this is a good command to run periodically so you don’t have to deal with any conflicts once you make your PR.\n\ngit pull\n\nCommit your changes to git\n\ngit commit -am 'My amazing addition to fastai here'\n\nSubmit a PR using gh\n\ngh pr create --title \"My amaizing change\" --body \"Here's what you need to know about it!\"\nThere are actually a few ways to issue the pr, but the above is the easiest. Check out the gh pr create docs for more options.\nOnce you do this, you’ll be asked some questions about where to push the branch and offer an option to fork the base repository under your own account. Easy peasy friends.\nCongrats! You’re a contributor now."
  },
  {
    "objectID": "posts/2021-06-02-how-to-submit-a-pr-to-fastai.html#summary",
    "href": "posts/2021-06-02-how-to-submit-a-pr-to-fastai.html#summary",
    "title": "Contributing to fastai: Setup your local development environment & submit a PR",
    "section": "Summary",
    "text": "Summary\nBut that’s not all! There are all kinds of cool things you can do using the github CLI including monitoring the status of your PRs and also fixing them (which you’ll likely have to do when the base repo owners ask you to make changes of one sort or another). It’s all in the docs and it’s all fairly straightforward!"
  },
  {
    "objectID": "posts/2024-07-18-llm-workshop-prompt-only.html",
    "href": "posts/2024-07-18-llm-workshop-prompt-only.html",
    "title": "LLM Workshop #3 - How Far Can We Get With Prompting Alone?”",
    "section": "",
    "text": "“you should prototype with a powerful model to see what is possible” \n\nThis is our chance to do a “vibe check” and create confidence that our objectives are doable. It’s also a good time to get a sense of which knobs we can turn to potentially improve results, understand likely primary failure modes, and see how far we can push the bigger models without fine tuning. As the general advice from the workshop is to avoid fine tuning until you can prove you need it, this step will also provide the base on top of which we begin building our evaluation pipeline for that very purpose.\n\n\n\n\n\n\nTip: Miminize friction\n\n\n\nThe best way to learn if something will work is to build it. A common theme from the workshop revolves around “minimizing friction”, and in a nutshell you can read that as being encouraged to “get going and build!” The best way to do this in the generative space is to try out one or more of the high quality models available to us via an API.\n\n\nWhat do we want our model to do again?\nBefore we begin, let’s distill our objectives into a few clear bullet points:\n\nIt should work on a single document or a collection of related documents in the domain of higher education surveys.\n(See the “Data Refinement” section in the previous post in this series where we’ve curated a set of documents representative of what we expect to see generally)\nWe should be able to provide it NLP task-specific tools to use to perform analysis on those documents\nCore tools for translation, summarization, sentiment, NER, and thematic analysis will always be provided and designed to work against English texts\nA translation tool is required and need to be executed first if any documents are not in English\nThe results are formatted as structured output.\n\nWith that, let’s define some tools. In partuclar, we will define the “core” tools that should should always be made available to the LLM."
  },
  {
    "objectID": "posts/2024-07-18-llm-workshop-prompt-only.html#in-the-beginning",
    "href": "posts/2024-07-18-llm-workshop-prompt-only.html#in-the-beginning",
    "title": "LLM Workshop #3 - How Far Can We Get With Prompting Alone?”",
    "section": "",
    "text": "“you should prototype with a powerful model to see what is possible” \n\nThis is our chance to do a “vibe check” and create confidence that our objectives are doable. It’s also a good time to get a sense of which knobs we can turn to potentially improve results, understand likely primary failure modes, and see how far we can push the bigger models without fine tuning. As the general advice from the workshop is to avoid fine tuning until you can prove you need it, this step will also provide the base on top of which we begin building our evaluation pipeline for that very purpose.\n\n\n\n\n\n\nTip: Miminize friction\n\n\n\nThe best way to learn if something will work is to build it. A common theme from the workshop revolves around “minimizing friction”, and in a nutshell you can read that as being encouraged to “get going and build!” The best way to do this in the generative space is to try out one or more of the high quality models available to us via an API.\n\n\nWhat do we want our model to do again?\nBefore we begin, let’s distill our objectives into a few clear bullet points:\n\nIt should work on a single document or a collection of related documents in the domain of higher education surveys.\n(See the “Data Refinement” section in the previous post in this series where we’ve curated a set of documents representative of what we expect to see generally)\nWe should be able to provide it NLP task-specific tools to use to perform analysis on those documents\nCore tools for translation, summarization, sentiment, NER, and thematic analysis will always be provided and designed to work against English texts\nA translation tool is required and need to be executed first if any documents are not in English\nThe results are formatted as structured output.\n\nWith that, let’s define some tools. In partuclar, we will define the “core” tools that should should always be made available to the LLM."
  },
  {
    "objectID": "posts/2024-07-18-llm-workshop-prompt-only.html#tools",
    "href": "posts/2024-07-18-llm-workshop-prompt-only.html#tools",
    "title": "LLM Workshop #3 - How Far Can We Get With Prompting Alone?”",
    "section": "Tools",
    "text": "Tools\nIf you didn’t know this already, apparently “Pydantic is all you need”. There’s even this video by Jason Liu to prove it.\n\n\n\n\n\n\n\nTip: Use Pydantic to define your structured outputs\n\n\n\nPydantic is is my preferred way to define tools for structured output for a number of reasons. In particular, I like how …\n\nIt’s widely supported across most IDEs and libraries (e.g, FastAPI, LangChain, Instructor, OpenAI, etc.., etc…).\nIt makes your intentions and expectations crystal clear, especially for any developer with OO experience (which is everyone).\nIt has error handling baked in.\nIt provides all kinds of hooks you can use in the pre/post-processing of your class attributes.\nIt enables you to build complex structured outputs composed on nested objects quite intuitively. Even a non-developer can look at your object hierarchy and understand what it is supposed to produce.\n\n\n\nLet’s use it to build out our core tools that we’ll want to always make available for our users.\n\nCore Tools\n\n\nPydantic classes for the core tools\n# Translation\nclass TranslationTask(BaseModel):\n    \"\"\"The translation of a document to English and the original language.\"\"\"\n\n    english_translation: str = Field(..., description=\"The English tranlsation\")\n    source_language: str = Field(\n        ..., description=\"The language of the original text (e.g., English, Spanish, French, Chinese, German, etc...)\"\n    )\n\n# Document summarization\nclass DocumentSummaryTask(BaseModel):\n    \"\"\"A summary and themes to extract from a document.\"\"\"\n\n    summary: str = Field(..., description=\"A concise and short 1 sentence summary of the author's statements.\")\n    themes: list[str] = Field(..., description=\"A list of no more than 5 concise 1 to 3 word themes discovered in the text\", max_items=5)\n\n# NER\nclass NamedEntityType(str, Enum):\n    \"\"\"A named entity type.\"\"\"\n\n    PERSON = \"PERSON\"\n    NORP = \"NORP\"\n    FAC = \"FAC\"\n    ORG = \"ORG\"\n    GPE = \"GPE\"\n    LOC = \"LOC\"\n    PRODUCT = \"PRODUCT\"\n    EVENT = \"EVENT\"\n    WORK_OF_ART = \"WORK_OF_ART\"\n    LAW = \"LAW\"\n    LANGUAGE = \"LANGUAGE\"\n    DATE = \"DATE\"\n    TIME = \"TIME\"\n    PERCENT = \"PERCENT\"\n    MONEY = \"MONEY\"\n    QUANTITY = \"QUANTITY\"\n    ORDINAL = \"ORDINAL\"\n    CARDINAL = \"CARDINAL\"\n    OTHER = \"OTHER\"\n\n\nclass NamedEntity(BaseModel):\n    \"\"\"The type of named entity and it's value.\"\"\"\n\n    entity_type: NamedEntityType\n    entity_mention: str = Field(..., description=\"The named entity recognized.\")\n\n\nclass DocumentNERTask(BaseModel):\n    \"\"\"Information about named entities to extract.\"\"\"\n\n    named_entities: list[NamedEntity] = Field(\n        ...,\n        description=f\"Perform Named Entity Recognition that finds the following entities: {', '.join([x.name for x in NamedEntityType])}\",\n    )\n\n# Sentiment\nclass DocumentSentimentTask(BaseModel):\n    \"\"\"Information about the sentiments expressed in a document.\"\"\"\n\n    positivity: int = Field(\n        ...,\n        description=\"How positive or negative is the author on a scale between 1 and 5 (1=Very Low, 2=Moerately Low, 3=Neutral, 4=Moderately Strong, 5=Very Strong)?\",\n    )\n    positive_statements: list[str] = Field(\n        ...,\n        description=\"A list of the author's positive statements\",\n    )\n    negative_statements: list[str] = Field(\n        ...,\n        description=\"A list of the author's negative statements\",\n    )\n\n    has_suggestions: bool = Field(\n        ...,\n        description=\"Does the author make any suggestions?\",\n    )\n    suggestions: list[str] = Field(\n        ...,\n        description=\"A list of any suggestions the author makes\",\n    )\n\n    feels_threatened: bool = Field(\n        ...,\n        description=\"Does the author feel fearful, harmed, intimidated, harassased, discriminated against, or threatened in any way?\",\n    )\n    feels_threatened_examples: list[str] = Field(\n        ...,\n        description=\"A list of how and why the author feels physically/emotionally/mentally threatened, uncomfortable, harassaed\",\n    )\n\n    profanity: bool = Field(\n        ...,\n        description=\"Is there any profanity?\",\n    )\n\n    is_nonsense: bool = Field(\n        ...,\n        description=\"Is the text uninformative or only contain nonsense? Set to `True` if the document is it too short to be meaningful or only says something like 'N/A', 'None', 'I have nothing to add', 'No suggestions', or 'No comment'.\",\n    )\n\n# Topic summarization\nclass TopicSummaryTask(BaseModel):\n    \"\"\"A summary and action plan to extract from a list of related documents.\"\"\"\n\n    theme_name: str = Field(..., description=\"A 5 to 10 word concise summary of the list of documents\")\n    action_plan: list[str] = Field(\n        ...,\n        description=\"A list of 3-5 specific actions that can be taken based on the documents provided\",\n        max_items=5,\n    )\n```\n\n\n\n\n“Tools” For “Tool Calling”\nWe’re going to use Instructor here because this is what Hamel suggests …\n\nFor open models you should use outlines. for closed models APIs you should use instructor. \n\n\n\n\n\n\n\nWarning\n\n\n\nTBH, I’m a noob relative to Instructor use. If anything I say below is wrong and/or can be improved … please, please let me know!\n\n\nMoving along, as I understand the library, it is designed to work against a single Pydantic class that you pass into it and from which an instance of that class will be returned at the conclusion of your LLM call. BUT, we’ve defined multiple classes as tools and we only want the LLM to call the tools it deems necessary to fulfill the user’s request. What are we going to do?\nMy answer, perhaps not suprisingly, is to use another Pydantic class. It looks like this:\nclass DocumentAnalysis(BaseModel):\n    tasks: list[TranslationTask | DocumentSummaryTask | DocumentNERTask | DocumentSentimentTask | TopicSummaryTask] = Field(\n        ..., description=\"The results of each analysis task the user asked to be performed on a given document as context.\"\n    )\nThis makes it easy to support a flexible tool calling system where a user can create their own Analysis class with whatever “tasks” they want the LLM to operate with and use with Instructor. For example, I’ll use the class below when working with collections of related documents since the the available tools should be limited for this use case:\nclass RelatedDocumentAnalysis(BaseModel):\n    tasks: list[TopicSummaryTask | DocumentSentimentTask | TopicSummaryTask] = Field(\n        ..., description=\"The results of each analysis task the user asked to be performed on a a list of related documents as context.\"\n    )\nWith our Pydantic army above, we can now move to experimenting with the big dogs to see what works and what doesn’t.\n\n\n\n\n\n\nTip: Note\n\n\n\nBecause of the propietary nature of the survey comments, I can’t show you the actual results. What I can show you is the code and my observations. With that in hand you should have everything you need to get going with your own use cases. If anything isn’t clear, drop a comment below or hit me up on X."
  },
  {
    "objectID": "posts/2024-07-18-llm-workshop-prompt-only.html#single-document-analysis",
    "href": "posts/2024-07-18-llm-workshop-prompt-only.html#single-document-analysis",
    "title": "LLM Workshop #3 - How Far Can We Get With Prompting Alone?”",
    "section": "Single Document Analysis",
    "text": "Single Document Analysis\n\nData\nLet’s sample 5 rows from our sampled chunked dataset created in the previous post. We’ll use the full survey comments for exploration here and ensure that 2 of those samples are in Spanish.\n\n\n\n\n\n\nTip: Test with representative examples\n\n\n\nThink about what kind of examples are like to be seen at inference time and test a few out here. Our goal is to get a sense of how well our model will generalize over asks its likely to see in the future.\n\n\ndf = pd.read_parquet(f\"{DATA_DIR}/clean/{DATA_FILENAME}_sample_14k_chunked.parquet\")\n\ntest_df = df[df[\"AnswerLang\"] == \"Spanish\"].sample(n=2)\ntest_df[\"AnswerText\"] = test_df[\"AnswerText_NonEnglish\"]\ntest_df = pd.concat([test_df, df[df[\"AnswerLang\"] == \"English\"].sample(n=3)])\n\n\nLLM Utility\nWe’ll define the following function to make it easy for us to test different APIs with Instructor. I designed this function for use with OpenAI, Antrhopic, and Fireworks specifically though it will likely work with any supported vendor with little or no modification.\ndef ask_ai(\n    client,\n    content: str,\n    query: str | None = None,\n    model: str = \"gpt-4o\",\n    instructor_kwargs: dict = {},\n) -&gt; DocumentAnalysis:\n    if not query:\n        query = \"Tasks: translation (if the document is not in English), summarization, ner, sentiment analysis.\"\n    try:\n        return client.chat.completions.create(\n            model=model,\n            response_model=DocumentAnalysis,\n            max_retries=3,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\nExecute each analysis task.\nAlways translate any non-English documents into English before executing other tasks.\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"{query}. Document: {content}\",\n                },\n            ],\n            **instructor_kwargs,\n        )\n    except Exception as e:\n        print(e)\n\n\nExperiments\n\nOpenAI\nclient = instructor.from_openai(OpenAI())\n\nfor r_idx, r in test_df.iterrows():\n    print(\"==========\")\n    print(\":: Document ::\")\n    print(r[\"AnswerText\"])\n    results = ask_ai(client, r[\"AnswerText\"])\n\n    print(\":: Results ::\")\n    print(results.model_dump_json(indent=2))\n    print(\"==========\")\n\nObservations\n\nPerformed the translation step where needed correctly and used the English translation in the rest of the tools.\nGenerally did an outstanding job of calling the tools and providing results that were as good, and sometimes better, than what I would have done.\nOccasionally ran into validation errors because it couldn’t set various properties correctly. This usually happend with the summarization or sentiment tools where I’d get an error like this:\ntasks.0.DocumentSummaryTask.summary Input should be a valid string [type=string_type, input_value={'summary': 'The author c... 'inadequate planning']}, input_type=dict]\n\n\n\n\nAnthropic\nclient = instructor.from_anthropic(Anthropic())\n\nfor r_idx, r in test_df.iterrows():\n    print(\"==========\")\n    print(\":: Document ::\")\n    print(r[\"AnswerText\"])\n    results = ask_ai(\n        client, r[\"AnswerText\"], model=\"claude-3-5-sonnet-20240620\", instructor_kwargs={\"max_tokens\": 1024}\n    )\n\n    print(\":: Results ::\")\n    print(results.model_dump_json(indent=2))\n    print(\"==========\")\n\nObservations\n\nPerformed the translation step where needed correctly and used the English translation in the rest of the tools.\nGenerally did an outstanding job of calling the tools and providing results that were as good, and sometimes better, than what I would have done.\nOccassionally, it would struggle with the NER task for some reason. I talk about this more in my Structuring Enums for Flawless LLM results with Instructor post. I can’t say if this is an Instructor issue and/or something particular to this model itself.\n\n\n\n\nFireworks\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"https://api.fireworks.ai/inference/v1\",\n        api_key=os.environ[\"FIREWORKS_API_KEY\"],\n    ),\n)\n\nfor r_idx, r in test_df.iterrows():\n    print(\"==========\")\n    print(\":: Document ::\")\n    print(r[\"AnswerText\"])\n    results = ask_ai(client, r[\"AnswerText\"], model=\"accounts/fireworks/models/firefunction-v2\")\n\n    print(\":: Results ::\")\n    print(results.model_dump_json(indent=2))\n    print(\"==========\")\n\nObservations\n\nDid not reliably perform the translation step where required\nDid not reliably call all the tools\nWhere called, the results of each tool call were not as accurate as either the OpenAI or Anthropic models\nEncountered validation errors more frequently.\nIt is FAST! Like really fast to run.\n\nThe speed difference is pretty noticeable, but I wonder if it comes at the cost of being able to use Fireworks for any complex structured output?"
  },
  {
    "objectID": "posts/2024-07-18-llm-workshop-prompt-only.html#related-document-analysis",
    "href": "posts/2024-07-18-llm-workshop-prompt-only.html#related-document-analysis",
    "title": "LLM Workshop #3 - How Far Can We Get With Prompting Alone?”",
    "section": "Related Document Analysis",
    "text": "Related Document Analysis\n\nData\nLet’s sample 5 rows (topics) from our topics dataset created in the previous post.\ndf = pd.read_parquet(f\"{DATA_DIR}/clean/{DATA_FILENAME}_sample_14k_topics.parquet\")\ntest_df = df.sample(n=5)\n\n\nLLM Utility\nThis is essentially the same utility function used above. The only bits we’ve modified are the prompt template and the response_model.\ndef ask_ai(\n    client,\n    content: str,\n    query: str | None = None,\n    model: str = \"gpt-4o\",\n    instructor_kwargs: dict = {},\n) -&gt; RelatedDocumentAnalysis:\n    if not query:\n        query = \"Tasks: thematic analysis/action planning and sentiment analysis.\"\n    try:\n        return client.chat.completions.create(\n            model=model,\n            response_model=RelatedDocumentAnalysis,\n            max_retries=3,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": dedent(\"\"\"\\\n                        Execute each analysis task.\"\"\"),\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"{query}. Documents:\\n{content}\",\n                },\n            ],\n            **instructor_kwargs,\n        )\n    except Exception as e:\n        print(e)\n\n\nExperiments\n\nOpenAI\nclient = instructor.from_openai(OpenAI())\n\nfor r_idx, r in test_df.iterrows():\n    print(\"==========\")\n    print(\":: Document ::\")\n    docs = \"\\n\\n\".join(r[\"_chunk\"])\n    print(docs)\n\n    results = ask_ai(client, docs)\n\n    print(\":: Results ::\")\n    print(results.model_dump_json(indent=2))\n    print(\"==========\")\n\nObservations\n\nGenerally did an outstanding job of calling the tools and providing results that were as good, and sometimes better, than what I would have done.\nI didn’t encounter any errors!\n\nPerhaps, GPT-4o simply struggles when too many tools are made available to it?\n\n\n\nAnthropic\nclient = instructor.from_anthropic(Anthropic())\n\nfor r_idx, r in test_df.iterrows():\n    print(\"==========\")\n    print(\":: Document ::\")\n    docs = \"\\n\\n\".join(r[\"_chunk\"])\n    print(docs)\n\n    results = ask_ai(\n        client, docs, model=\"claude-3-5-sonnet-20240620\", instructor_kwargs={\"max_tokens\": 1024}\n    )\n\n    print(\":: Results ::\")\n    print(results.model_dump_json(indent=2))\n    print(\"==========\")\n\nObservations\n\nGenerally did an outstanding job of calling the tools and providing results that were as good, and sometimes better, than what I would have done.\nI didn’t encounter any errors!\n\n\n\n\nFireworks\nclient = instructor.patch(\n    OpenAI(\n        base_url=\"https://api.fireworks.ai/inference/v1\",\n        api_key=os.environ[\"FIREWORKS_API_KEY\"],\n    ),\n)\n\nfor r_idx, r in test_df.iterrows():\n    print(\"==========\")\n    print(\":: Document ::\")\n    docs = \"\\n\\n\".join(r[\"_chunk\"])\n    print(docs)\n\n    results = ask_ai(client, docs, model=\"accounts/fireworks/models/firefunction-v2\")\n\n    print(\":: Results ::\")\n    print(results.model_dump_json(indent=2))\n    print(\"==========\")\n\nObservations\n\nGenerally did an outstanding job of calling the tools and providing results that were as good, and sometimes better, than what I would have done.\nI didn’t encounter any errors!\n\nPerhaps, GPT-4o simply struggles when too many tools are made available to it?"
  },
  {
    "objectID": "posts/2024-07-18-llm-workshop-prompt-only.html#exprimenting-with-replicate",
    "href": "posts/2024-07-18-llm-workshop-prompt-only.html#exprimenting-with-replicate",
    "title": "LLM Workshop #3 - How Far Can We Get With Prompting Alone?”",
    "section": "Exprimenting with Replicate",
    "text": "Exprimenting with Replicate\nReplicate is quickly becoming a favorite tool in my LLM arsenal. Hamel has a really nice example/tutorial for getting tool calling to work with the Llama3-70B Instruct model out of the box (not fine tuned). I stole some of his ideas and put together a simple workflow that allows me to incorporate testing with Llama3 variants here.\nBelow, I share running the single document analysis experiment against both the 70B and 8B variants.\ndoc_analysis_schema = DocumentAnalysis.model_json_schema()\n\n# doc_analysis_schema\ndef format_tools():\n    doc_analysis_schema = DocumentAnalysis.model_json_schema()\n\n    result = \"\"\n    for k, v in doc_analysis_schema[\"$defs\"].items():\n        result += f\"&lt;function name='{k}'&gt;\\n\"\n        result += json.dumps(v, indent=2)\n        result += f\"\\n&lt;/function name='{k}'&gt;\\n\"\n    return result.strip()\nPROMPT_TEMPLATE = \"\"\"\\\n&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n{system_message}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n&lt;query&gt;{prompt}&lt;/query&gt;&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n{prefill}\"\"\"\n\n# print(PROMPT_TEMPLATE)\nSYSTEM_PROMPT = \"\"\"\\\n{system_persona}\n\nYou are given a set of tasks to perform and a document inside &lt;query&gt; tags and a set of possible functions inside &lt;function-definitions&gt; tags.\nCalling these functions are optional. Carefully consider the question and determine if one or more functions can be used to answer the question. Place your thoughts and reasoning behind your decision in &lt;function-thoughts&gt; tags.\nBelow is a list of function definitions:\n&lt;function-definitions&gt;\n{tools}\n&lt;/function-definitions&gt;\n\nFor each function you want to call, execute the function and use the answer to provide values to each function parameter in a way that conforms to that function's schema. Include the function name and parameter values inside the &lt;function-calls&gt; tag.\nFunction calls MUST be in this format: &lt;function-thoughts&gt;Calling func1 would be helpful because of ...&lt;/function-thoughts&gt;&lt;function-calls&gt;[func1_name(params_name=params_value, params_name2=params_value2...), func2_name(params)]&lt;/function-calls&gt;, WITHOUT any answer.\nIf the query is not in English, always translate it into English first and then proceed to call any other functions using the English translation.\n\nIf you do not wish to call any functions, say so in the &lt;function-thoughts&gt; tags followed by &lt;function-calls&gt;None&lt;/function-calls&gt;\"\"\"\n\nsystem_persona = \"\"\"\\\nYou are an expert NLP data scientist, skilled in machine translation, text summarization, NER, thematic analysis, strategic planning, sentiment analysis, and classification tasks.\"\"\"\n\n# print(SYSTEM_PROMPT)\ntools = format_tools()  # .replace(\"\\n\", \"\")\nsystem_message = SYSTEM_PROMPT.format(system_persona=system_persona, tools=tools)\n\n# print(system_message)\nfor r_idx, r in test_df.iterrows():\n    print(\"==========\")\n    print(f\"Document:\\n{r['AnswerText']}\\n\")\n\n    prompt = f\"Translate the document below into English if necessary. After that, perform the following tasks on the document below: summarize, perform named entity recognition, and sentiment analysis. Document:\\n{r['AnswerText']}\"\n\n    input = {\n        \"max_tokenx\": 1024,\n        \"temperature\": 0,\n        \"top_p\": 0.9,\n        \"top_k\": 50,\n        \"presence_penalty\": 0,\n        \"frequency_penalty\": 0,\n        \"system_prompt\": system_message,\n        \"prompt\": f\"&lt;query&gt;{prompt}&lt;/query&gt;\",\n    }\n    print(\"--- Example ---\")\n    print(f\"prompt: {prompt}\")\n    print(\"\")\n\n    print(\"===== Llama3-70B-Instruct =====\")\n    output = replicate.run(\"meta/meta-llama-3-70b-instruct\", input=input)\n    print(\"\".join(output))\n    print(\"\")\n\n    print(\"===== Llama3-8B-Instruct =====\")\n    output = replicate.run(\"meta/meta-llama-3-8b-instruct\", input=input)\n    print(\"\".join(output))\n    print(\"\")\n\nObservations\n\nGenerally the 70B model did an outstanding job of calling the tools and providing results that were as good, and sometimes better, than what I would have done.\nThe 8B model would return decent results most of the time, but the formatting was all over the place.\n\nThis gave me a lot of confidence that open source models are a contender to consider. Even though the 8B model didn’t produce reliable results, the fact that the correct information was returned along with how well the 70B model performed, definitely make me think it might do really well in fine tuning."
  },
  {
    "objectID": "posts/2024-07-18-llm-workshop-prompt-only.html#takeaways",
    "href": "posts/2024-07-18-llm-workshop-prompt-only.html#takeaways",
    "title": "LLM Workshop #3 - How Far Can We Get With Prompting Alone?”",
    "section": "Takeaways",
    "text": "Takeaways\nAfter all this, I’m confident that my ideas are worthy to pursue. Based on the results above, it seems worthwhile to explore turning the following knobs to boost performance with these models:\n\nImproved descriptions in the Pydantic classes\nImprove the sytem prompt\nFormulate varied human messages to simulate different ways the user may prompt the model\n\nCan you think of any other adjustments that might be beneficial? If so, drop a comment below!"
  },
  {
    "objectID": "posts/2024-07-18-llm-workshop-prompt-only.html#next-steps",
    "href": "posts/2024-07-18-llm-workshop-prompt-only.html#next-steps",
    "title": "LLM Workshop #3 - How Far Can We Get With Prompting Alone?”",
    "section": "Next Steps",
    "text": "Next Steps\nWith some insights about what I can play with to improve my results, it’s time for some experimentation. But how can I know whether or not the changes I’m making present meaningful progress? How can I know specificially where the model is struggling and look at those examples to inform future experiments?\nThe answer: I need to set up an initial evaluation pipeline along with some scoring functions.\nAnd that is exactly what we’ll do in this next post in this series … stay tuned."
  },
  {
    "objectID": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html",
    "href": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html",
    "title": "Multilingual Sequence Classifaction with the MBart Family",
    "section": "",
    "text": "!pip install ohmeow-blurr -q\n!pip install datasets -q\nfrom fastai.text.all import *\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification\n\nfrom blurr.utils import BLURR\nfrom blurr.data.core import *\nfrom blurr.modeling.core import *\nfrom blurr import __version__ as blurr_version\nfrom fastai import __version__ as fa_version\nfrom torch import __version__ as pt_version\nfrom transformers import __version__ as hft_version\n\nprint(f'Using blurr {blurr_version}')\nprint(f'Using pytorch {pt_version}')\nprint(f'Using fastai {fa_version}')\nprint(f'Using transformers {hft_version}')\n\nUsing blurr 0.0.25\nUsing pytorch 1.8.1+cu101\nUsing fastai 2.3.1\nUsing transformers 4.6.1"
  },
  {
    "objectID": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html#data",
    "href": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html#data",
    "title": "Multilingual Sequence Classifaction with the MBart Family",
    "section": "Data",
    "text": "Data\n\ntrn_ds, val_ds, tst_ds = load_dataset(\"amazon_reviews_multi\", \"de\", split=['train', 'validation', 'test'])\n\nReusing dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n\n\n\ntrn_df = pd.DataFrame(trn_ds)\nval_df = pd.DataFrame(val_ds)\ntst_df = pd.DataFrame(tst_ds)\n\n\n\n\n\n\n\nImportant\n\n\n\nUse a subset of the data when building your model to speed up developement time!\n\n\nAfter you got everything, throw the full dataset at it and go get some coffee :)\n\n# testing with a subset ... (comment out to train on full dataset!!!)\n\n# this won't work because the rows are ordered by our targets!\n# trn_ds, val_ds = load_dataset(\"amazon_reviews_multi\", \"de\", split=['train[:10%]', 'validation[:10%]'])\n\ntrn_df = trn_df.sample(frac=0.05)\nval_df = val_df.sample(frac=0.05)\n\n\ntrn_df['is_valid'] = False; val_df['is_valid'] = True\ndf = pd.concat([trn_df, val_df])\n\nprint(len(trn_df), len(val_df), len(df))\n\n10000 250 10250\n\n\n\ntrn_df.head()\n\n\n\n\n\n\n\n\nlanguage\nproduct_category\nproduct_id\nreview_body\nreview_id\nreview_title\nreviewer_id\nstars\nis_valid\n\n\n\n\n115812\nde\napparel\nproduct_de_0727194\nfür so einen Preis muss ich dazusagen, es hält was es verspricht, natürlich ist hier keine hohe Qualität zu erwarten, aber für solchen Preis recht gut. ist. Die Nähte gehen mit Zeit auf\nde_0580512\nPreis Leistung Ok\nreviewer_de_0542714\n3\nFalse\n\n\n58074\nde\ntoy\nproduct_de_0014211\nDie Qualität ist nicht gut. Nur nach kurzem nutzen sind die plastikverkleidungen abgefallen. Der Ball lässt sich schlecht für Kinder entfernen. Die Scheiben sehen nach kurzer Nutzung aus als wären sie Jahre im Gebrauch. Würde ich nicht wieder kaufen.\nde_0100470\nSchlechte Qualität\nreviewer_de_0719961\n2\nFalse\n\n\n122195\nde\ntoy\nproduct_de_0304268\nErfüllt seinen Zweck und sie gut aus.\nde_0811174\nToller Lederbeutel\nreviewer_de_0999635\n4\nFalse\n\n\n75151\nde\nother\nproduct_de_0720910\nIch mach es kurz; Kauft euch das Album nicht und behaltet Rise Against so on Erinnerung wie sie früher waren. Alles nach Appeal to reason ist generisch, einfallslos und irgendwie langweilig geworden. The Violence ist der einzige Track der mir vielleicht im Gedächtnis bleiben könnte.\nde_0713777\nLangweilg\nreviewer_de_0130191\n2\nFalse\n\n\n94760\nde\ndigital_video_download\nproduct_de_0847576\nFand die Twists im Film ziemlich vorhersehbar. Dennoch fand ich den Film ansich ganz cool, da ich 1. Den Schauspieler sehr mag und 2. Diese Thematik immer wieder spannend finde. Für abends zuhause mit dem Partner auf der Couch ein solider Film, den man durchaus mal schauen kann, aber auch kein Super hollywood Kino erwartet\nde_0471539\nSolider Film für zuhause\nreviewer_de_0223020\n3\nFalse\n\n\n\n\n\n\n\n\nunique_tgt_vals = trn_df.stars.value_counts()\nprint(unique_tgt_vals)\n\nlabels = sorted(list(df.stars.unique()))\nprint(labels)\n\n4    2054\n3    2054\n5    2025\n2    1946\n1    1921\nName: stars, dtype: int64\n[1, 2, 3, 4, 5]"
  },
  {
    "objectID": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html#huggingface-objects",
    "href": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html#huggingface-objects",
    "title": "Multilingual Sequence Classifaction with the MBart Family",
    "section": "huggingface objects",
    "text": "huggingface objects\n\nmodel_name = \"facebook/mbart-large-50\"\nmodel_cls = AutoModelForSequenceClassification\nhf_tok_kwargs = {'src_lang': 'de_DE', 'tgt_lang': 'de_DE'}\n\nhf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(model_name,  \n                                                                  model_cls=model_cls,  \n                                                                  tokenizer_kwargs=hf_tok_kwargs, \n                                                                  config_kwargs={'num_labels': len(labels)})\n\nprint('arch: ', type(hf_arch))\nprint('config: ', type(hf_config))\nprint('tokenizer: ', type(hf_tokenizer))\nprint('model: ', type(hf_model))\n\narch:  &lt;class 'str'&gt;\nconfig:  &lt;class 'transformers.models.mbart.configuration_mbart.MBartConfig'&gt;\ntokenizer:  &lt;class 'transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast'&gt;\nmodel:  &lt;class 'transformers.models.mbart.modeling_mbart.MBartForSequenceClassification'&gt;\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAlways good to look into the config as you’ll often find good defaults to use in your training and inference!\n\n\n\nhf_config\n\nMBartConfig {\n  \"_name_or_path\": \"facebook/mbart-large-50\",\n  \"_num_labels\": 3,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_final_layer_norm\": true,\n  \"architectures\": [\n    \"MBartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4\n  },\n  \"max_length\": 200,\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"mbart\",\n  \"normalize_before\": true,\n  \"normalize_embedding\": true,\n  \"num_beams\": 5,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": true,\n  \"static_position_embeddings\": false,\n  \"tokenizer_class\": \"MBart50Tokenizer\",\n  \"transformers_version\": \"4.6.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 250054\n}"
  },
  {
    "objectID": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html#dataloaders",
    "href": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html#dataloaders",
    "title": "Multilingual Sequence Classifaction with the MBart Family",
    "section": "DataLoaders",
    "text": "DataLoaders\n\n# 3 lines!  Nice!!!\nblocks = (HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=256), CategoryBlock)\ndblock = DataBlock(blocks=blocks, get_x=ColReader('review_body'), get_y=ColReader('stars'), splitter=ColSplitter())\n\ndls = dblock.dataloaders(df, bs=4)\n\n\n\n\n\n\n\nImportant\n\n\n\nIt’s almost always useful to look at the shape of things in your batches (esp. when debugging)\n\n\nFor example, when running on a colab GPU I kept getting CUDA OOM even with a batch size of just 4. So I looked at the input_ids and saw that they were over 1,000 tokens long in some batches. So I adjusted the max_length above to ensure they weren’t longer that 128 characters and voila, you have the tutorial before you now.\nOf course, you should run this with the biggest batch size and sequence size your GPU(s) will support.\n\nxb, yb = dls.one_batch()\nxb['input_ids'].shape\n\ntorch.Size([4, 256])\n\n\n\n# what does human friendly data look like?\ndls.show_batch(dataloaders=dls, max_n=2, trunc_at=1500)\n\n\n\n\n\ntext\ncategory\n\n\n\n\n0\nIch habe mir für mein neues Apple iPad (2018er Modell) diese Folie zugelegt. Das ausschlaggebende Kriterium hierbei war natürlich die durch den Hersteller beworbene besondere Oberflächenstruktur der Folie. Ich benutze das iPad sehr oft zusammen mit dem Apple Pen und fand die Idee, dass sich der Stift beim Schreiben wie ein Bleistift auf einem Blatt Papier anfühlt, sehr verheißungsvoll. Dementsprechend war ich natürlich sehr gespannt darauf, ob die Folie denn auch hält, was der Hersteller verspricht. Dank Amazon kam die Lieferung wie gewohnt zügig an und nach dem ersten Auspacken zeigte sich, dass der Hersteller nicht zu viel versprochen hatte. Die Oberfläche der Folie fühlte sich in der Tat an wie ein Blatt Papier und auch die ersten \"Trockenübungen\" mit dem Apple Pen fühlten sich beinahe an wie echt. Das anbringen der Folie war dann aber leider (wie bei allen Folien) eine Herausforderung. Obwohl ich mich an die Anleitung gehalten hatte und besonders auf eine staubfreie Umgebung achtete schaffte ich es nicht die erste Folie gänzlich ohne Luft- und Staubeinschlüsse auf meinem iPad anzubringen\n2\n\n\n1\nDer erste Punkt das Design ist schlicht und schick. Die Uhr kann man zum Sport wie auch auf der Arbeit und in der Freizeit ohne Probleme anziehen. Das Laden der Fitnessuhr gestaltet sich ebenfalls intuitiv und schnell. Durch die USB- Schnittstelle kann die Uhr mit vorhanden Geräten geladen werden. Das waren dann auch schon die positiven Punkte der Uhr. Die Kopplung mit dem Handy funktioniert nur bedingt. Eine direkte Bluetooth Verbindung zwischen Handy und Uhr ist unmöglich. Durch die App können die Geräte zwar gekoppelt werden, diese weist aber erhebliche Fehlerstellen auf. Oft verbindet sich das Handy erst wieder mit der Uhr nach einem Neustart der Geräte. Die Uhr besitzt keinen eigenen Knopf zum An- und Ausschalten. Das automatische Aufblenden der Anzeige funktioniert ebenfalls nur bedingt und gefühlt nach dem Zufallsprinzip. Die Hilfestellungen durch Bedingungsanleitung und App bringen einen nicht wirklich weiter. Wer ausschließlich eine Uhr zum tracken der Schritte und Herzfrequenz sucht ist mit der Uhr gut aufgehoben. Zum täglichen Gebrauch als Uhrenersatz nicht zu empfehlen.\n2"
  },
  {
    "objectID": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html#training",
    "href": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html#training",
    "title": "Multilingual Sequence Classifaction with the MBart Family",
    "section": "Training",
    "text": "Training\nPrint out the model so we can build a custom set of parameter groups for an MBart + Sequence Classification task\n\n# hf_model\n\n\ndef mbart_splitter(m):\n  model = m.hf_model if (hasattr(m, 'hf_model')) else m\n  \n  embeds_modules = [\n    model.model.encoder.embed_positions, \n    model.model.encoder.embed_tokens,\n    model.model.decoder.embed_positions, \n    model.model.decoder.embed_tokens\n  ]\n \n  embeds = nn.Sequential(*embeds_modules)\n  groups = L(embeds, model.model.encoder, model.model.decoder, model.classification_head)\n  return groups.map(params).filter(lambda el: len(el) &gt; 0)\n\nConfigure our metrics and callbacks required by blurr\n\n# define our metrics (see the sklearn docs for more info)\nprecision = Precision(average='macro')\nrecall = Recall(average='macro')\nf1 = F1Score(average='macro')\n\nlearn_metrics = [accuracy, precision, recall, f1]\nlearn_cbs = [HF_BaseModelCallback]\n\nConfigure our Learner and train away …\n\n# configure our Learner; 3 lines!\nmodel = HF_BaseModelWrapper(hf_model)\nlearn = Learner(dls, model, opt_func=Adam, loss_func=CrossEntropyLossFlat(), metrics=learn_metrics, cbs=learn_cbs, splitter=mbart_splitter)\nlearn.freeze()\n\nprint(len(learn.opt.param_groups))\n\n4\n\n\n\nlearn.lr_find(suggestions=True)\n\n\n\n\nSuggestedLRs(lr_min=6.918309736647643e-07, lr_steep=0.001737800776027143)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(1, lr_max=7e-5)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nprecision_score\nrecall_score\nf1_score\ntime\n\n\n\n\n0\n1.151728\n0.951163\n0.592000\n0.574056\n0.578043\n0.575234\n05:23\n\n\n\n\n\n\nlearn.show_results(learner=learn, max_n=2, trunc_at=1500)\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n\ntext\ncategory\ntarget\n\n\n\n\n0\nDie Qualität meiner neuen Hausschuhe ist, wenn ich das nach drei Tagen schon sagen kann, sehr gut. Durch den sehr weichen Stoff \"schmeicheln\" sie regelrecht den Füßen. Es lässt sich wunderbar darauf laufen und -besonders wichtig für mich- sie halten auch schön warm. Was mir nicht gefällt, und deshalb ziehe ich auch einen Stern ab, ist die hintere Umrandung des Obermaterials. Hausschuhe sind ja hinten eigentlich flach, sodass ich schön in den Schuh rein- und rausschlüpfen kann. Durch die 1 cm Kante, die wahrscheinlich deshalb eingearbeitet wurde, um mehr Halt im Schuh zu haben, macht mich beim Treppensteigen sehr unsicher. Obwohl der Schuh an sich wunderbar passt, gibt mir die hintere Kante das Gefühl, jederzeit beim Laufen aus dem Schuh zu rutschen. Und beim Treppensteigen verstärkt sich dieses Gefühl. Ein klarer Minuspunkt; obwohl das sehr subjektiv ist. Manche werden wahrscheinlich nicht verstehen, was ich damit meine. Für knapp 20 Euro habe ich jedenfalls einen tollen Gegenwert erhalten. Meine letzten Hausschuhe, für die ich hier bei Amazon seinerzeit für knapp 40 Euro bezahlt habe, sind nach drei Jahren total aus\n4\n3\n\n\n1\nDa ich unsere Frühstücksbrötchen gerne selber backe, habe ich mir das Baguetteblech besorgt. Das Blech ist wie abgebildet und funktioniert super. Passt genau auf ein Backblech. Reinigung ist sehr leicht, ich habe übrig gebliebene Mehl einfach mit Wasser abgewaschen. Einfetten war nicht von Nöten. Ich habe das Blech nur mit Mehl bestäubt, bevor ich die Brötchen drauf gelegt habe. Einen Stern gibt es Abzug, da bei der Lieferung bereits einige Kratzer in der Beschichtung waren. Ansonsten bin ich bisher sehr zufrieden! Nachtrag - Änderung der Punktezahl: Mittlerweile bin ich nicht mehr so von dem Backblech angetan. Der Teig bleibt immer öfter kleben und zuletzt waren die Brötchen unten sogar schwarz gefärbt. Das war nicht verbrannt sondern vom Backblech, ich weiß nicht ob sich die Beschichtung löst, oder ob das die Farbe ist. Aber das ist mit Sicherheit nicht gesund. Deswegen nur noch 2 Sterne!\n2\n3\n\n\n\n\n\nWe’ll freeze all the layers with the exception of the decoder and classification_head layers (the last 2)\n\nlearn.freeze_to(-2)\n\n\nlearn.lr_find(suggestions=True)\n\n\n\n\nSuggestedLRs(lr_min=1.4454397387453355e-06, lr_steep=9.12010818865383e-07)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(3, lr_max=slice(2e-8, 2e-7))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nprecision_score\nrecall_score\nf1_score\ntime\n\n\n\n\n0\n1.020526\n0.942416\n0.612000\n0.590443\n0.597485\n0.591900\n10:13\n\n\n1\n1.020601\n0.935697\n0.612000\n0.593317\n0.598285\n0.594764\n10:13\n\n\n2\n1.063076\n0.934123\n0.616000\n0.597113\n0.602207\n0.598684\n10:13\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\n\nlearn.show_results(learner=learn, max_n=2, trunc_at=1500)\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n\ntext\ncategory\ntarget\n\n\n\n\n0\nDie Qualität meiner neuen Hausschuhe ist, wenn ich das nach drei Tagen schon sagen kann, sehr gut. Durch den sehr weichen Stoff \"schmeicheln\" sie regelrecht den Füßen. Es lässt sich wunderbar darauf laufen und -besonders wichtig für mich- sie halten auch schön warm. Was mir nicht gefällt, und deshalb ziehe ich auch einen Stern ab, ist die hintere Umrandung des Obermaterials. Hausschuhe sind ja hinten eigentlich flach, sodass ich schön in den Schuh rein- und rausschlüpfen kann. Durch die 1 cm Kante, die wahrscheinlich deshalb eingearbeitet wurde, um mehr Halt im Schuh zu haben, macht mich beim Treppensteigen sehr unsicher. Obwohl der Schuh an sich wunderbar passt, gibt mir die hintere Kante das Gefühl, jederzeit beim Laufen aus dem Schuh zu rutschen. Und beim Treppensteigen verstärkt sich dieses Gefühl. Ein klarer Minuspunkt; obwohl das sehr subjektiv ist. Manche werden wahrscheinlich nicht verstehen, was ich damit meine. Für knapp 20 Euro habe ich jedenfalls einen tollen Gegenwert erhalten. Meine letzten Hausschuhe, für die ich hier bei Amazon seinerzeit für knapp 40 Euro bezahlt habe, sind nach drei Jahren total aus\n4\n3\n\n\n1\nDie Lampe machte zunächst einen ordentlichen Eindruck. Jedoch waren einige LEDs bereits bei der Auslieferung defekt. Der daraufhin kontaktierte Lieferant ignorierte mein Anschreiben. Die dazugehörige Zeitschaltuhr hat alle paar Wochen den Dienst quittiert. Da ich nicht so häufig in den Keller gehe, in dem die Lampe bei der Überwinterung Licht spenden sollte, stellte ich dies meist - offenbar jeweils einige Tage nach dem Ausfall fest - da die Pflanzen kurz vor dem krepieren waren. Die Zeitschaltuhr ließ sich mehrmals wieder aktivieren, ist nun aber nach wenigen Monaten ganz defekt, die Lampe lässt sich nicht mehr einschalten. Die Pflanzen sind nun alle eingegangen. Ich rate dringend von dem Kauf des Artikels ab.\n1\n1"
  },
  {
    "objectID": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html#inference",
    "href": "posts/2021-05-25-mbart-sequence-classification-with-blurr.html#inference",
    "title": "Multilingual Sequence Classifaction with the MBart Family",
    "section": "Inference",
    "text": "Inference\n\ntxt = tst_df.review_body[0]\nprint(txt)\nlearn.blurr_predict(txt)\n\nLeider, leider nach einmal waschen ausgeblichen . Es sieht super hübsch aus , nur leider stinkt es ganz schrecklich und ein Waschgang in der Maschine ist notwendig ! Nach einem mal waschen sah es aus als wäre es 10 Jahre alt und hatte 1000 e von Waschgängen hinter sich :( echt schade !\n\n\n[(('1',),\n  (#1) [tensor(0)],\n  (#1) [tensor([0.5725, 0.3474, 0.0716, 0.0067, 0.0018])])]\n\n\n\ntxts = list(tst_df.review_body.values[1:10])\nprint(txts)\nlearn.blurr_predict(txts)\n\n['zunächst macht der Anker Halter einen soliden Eindruck. Die Magnethalterung ist auch brauchbar. Was gar nicht geht ist die Tatsache, dass die Halterung für runde Lüftungsdüsen, anders als vom Hersteller beschrieben, nicht geeignet ist! Ständig fällt das Smartphone runter. Durch das häufige Wiederanbringen ist nun auch die Gummierung kaputt, was zur Folge hat, dass die Lüftungsdüse schön zerkratzt wird! Also Schrott, der auch noch mein Auto beschädigt! Für mich ist das nicht brauchbar!', 'Siegel sowie Verpackung war beschädigt und ware war gebraucht mit Verschleiß und Fingerabdrücke. Zurück geschickt und bessere qualitativere Artikel gekauft.', 'Habe dieses Produkt NIE erhalten und das Geld wurde nicht rückerstattet!!!!!!!', 'Die Träger sind schnell abgerissen', 'Druckbild ist leider nicht akzeptabel. Die kompletten seiten werden grau eingefärbt. Verkäufer antwortet nicht auf Emails. Deshalb absolut nicht empfehlenswert.', '🤬🤬🤬 Stoff löst sich nach kurzer Zeit', 'Beim zweiten Gebrauch bereits undicht!!!', 'Die Lieferung war prompt. 2 Gläser sind bereits undicht und Wasser befindet sich in den Zwischenräumen... was nun?', 'Bin überhaupt nicht zufrieden. Das Handy ist mir einmal kurz, aus minimalen Höhe ca 30cm, auf den Tisch gefallen und die Folie ist schon wieder kaputt. MfG Sonja Sax']\n\n\n[(('2',),\n  (#1) [tensor(1)],\n  (#1) [tensor([0.1873, 0.5228, 0.2539, 0.0327, 0.0032])]),\n (('1',),\n  (#1) [tensor(0)],\n  (#1) [tensor([0.6606, 0.2255, 0.0867, 0.0233, 0.0038])]),\n (('1',),\n  (#1) [tensor(0)],\n  (#1) [tensor([9.5275e-01, 4.1451e-02, 5.0995e-03, 3.2897e-04, 3.6943e-04])]),\n (('1',),\n  (#1) [tensor(0)],\n  (#1) [tensor([0.3692, 0.2738, 0.2347, 0.0907, 0.0316])]),\n (('1',),\n  (#1) [tensor(0)],\n  (#1) [tensor([8.1323e-01, 1.5297e-01, 3.0495e-02, 2.6044e-03, 6.9975e-04])]),\n (('4',),\n  (#1) [tensor(3)],\n  (#1) [tensor([0.0447, 0.1097, 0.2021, 0.3402, 0.3033])]),\n (('1',),\n  (#1) [tensor(0)],\n  (#1) [tensor([0.6773, 0.1921, 0.0808, 0.0320, 0.0179])]),\n (('3',),\n  (#1) [tensor(2)],\n  (#1) [tensor([0.1179, 0.2596, 0.4055, 0.1970, 0.0200])]),\n (('1',),\n  (#1) [tensor(0)],\n  (#1) [tensor([0.6736, 0.2547, 0.0619, 0.0078, 0.0020])])]\n\n\nWell that’s it!\nI hope this article helps your fastai, huggingface, blurr out, and hey, if I’m doing something wrong above please let me know! I’m far from perfect :)\nFor more information on the MBart/MBar-50 architecture, see the huggingface docs here."
  },
  {
    "objectID": "posts/2022-02-09-ajtfb-chapter-6-regression.html",
    "href": "posts/2022-02-09-ajtfb-chapter-6-regression.html",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Regression",
    "section": "",
    "text": "Other posts in this series:\nA Journey Through Fastbook (AJTFB) - Chapter 1\nA Journey Through Fastbook (AJTFB) - Chapter 2\nA Journey Through Fastbook (AJTFB) - Chapter 3\nA Journey Through Fastbook (AJTFB) - Chapter 4\nA Journey Through Fastbook (AJTFB) - Chapter 5\nA Journey Through Fastbook (AJTFB) - Chapter 6a\nA Journey Through Fastbook (AJTFB) - Chapter 7\nA Journey Through Fastbook (AJTFB) - Chapter 8\nA Journey Through Fastbook (AJTFB) - Chapter 9"
  },
  {
    "objectID": "posts/2022-02-09-ajtfb-chapter-6-regression.html#regression",
    "href": "posts/2022-02-09-ajtfb-chapter-6-regression.html#regression",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Regression",
    "section": "Regression",
    "text": "Regression\nA regression task is all about predicting a continous value rather than a particular cateogry.\nHere we’ll consider a particular type of regression problem called image regression, where the “independent variable is an image, and the dependent variable is one or more float.” Our model is going to be a key point model that aims to predict a point (e.g., 2 labels … the x and y) on the image, which in our example is the center of a person’s face."
  },
  {
    "objectID": "posts/2022-02-09-ajtfb-chapter-6-regression.html#defining-your-datablock",
    "href": "posts/2022-02-09-ajtfb-chapter-6-regression.html#defining-your-datablock",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Regression",
    "section": "Defining your DataBlock",
    "text": "Defining your DataBlock\nAgain, the DataBlock is a blueprint for everything required to turn your raw data (images and labels) into something that can be fed through a neural network (DataLoaders with a numerical representation of both your images and labels). Below is the one presented in this chapter.\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\npath.ls()\n\n(#50) [Path('/root/.fastai/data/biwi_head_pose/02.obj'),Path('/root/.fastai/data/biwi_head_pose/04'),Path('/root/.fastai/data/biwi_head_pose/10'),Path('/root/.fastai/data/biwi_head_pose/17.obj'),Path('/root/.fastai/data/biwi_head_pose/23.obj'),Path('/root/.fastai/data/biwi_head_pose/21.obj'),Path('/root/.fastai/data/biwi_head_pose/24.obj'),Path('/root/.fastai/data/biwi_head_pose/12.obj'),Path('/root/.fastai/data/biwi_head_pose/10.obj'),Path('/root/.fastai/data/biwi_head_pose/01')...]\n\n\n“There are 24 directories numbered from 01 to 24 (they corresond to the different people photographed), and a corresponding .obj file for each (we won’t need them here)\n\n\n\n\n\n\nTip\n\n\n\nAlways EDA your dataset to make sure you understand how it is organized; this is especially important to ensure you create a good validation set without leakage from the training set.\n\n\n\n\n\n\n\n\nNote\n\n\n\n“… we should not just use a random splitter [so as] to ensure that our model can generalize to people that it hasn’t seen yet; a splitter function that returns True for just one person, resulting in a validation set containing just that one person.”\n\n\n\n(path/'01').ls()\n\n(#1000) [Path('/root/.fastai/data/biwi_head_pose/01/frame_00486_rgb.jpg'),Path('/root/.fastai/data/biwi_head_pose/01/frame_00023_pose.txt'),Path('/root/.fastai/data/biwi_head_pose/01/frame_00122_rgb.jpg'),Path('/root/.fastai/data/biwi_head_pose/01/frame_00165_rgb.jpg'),Path('/root/.fastai/data/biwi_head_pose/01/frame_00167_pose.txt'),Path('/root/.fastai/data/biwi_head_pose/01/frame_00384_rgb.jpg'),Path('/root/.fastai/data/biwi_head_pose/01/frame_00287_rgb.jpg'),Path('/root/.fastai/data/biwi_head_pose/01/frame_00075_rgb.jpg'),Path('/root/.fastai/data/biwi_head_pose/01/frame_00186_pose.txt'),Path('/root/.fastai/data/biwi_head_pose/01/frame_00444_rgb.jpg')...]\n\n\nLooks like each person has multiple images, and for each image there is a text file telling us where the point is. We can write a function to get the .txt file for any given image as such\n\nimg_files = get_image_files(path)\n\ndef img2pose(img_fpath):\n  return Path(f\"{str(img_fpath)[:-7]}pose.txt\")\n\nimg2pose(img_files[0])\n\nPath('/root/.fastai/data/biwi_head_pose/04/frame_00486_pose.txt')\n\n\n\n\n\n\n\n\nTip\n\n\n\nLook at your inputs/targets to verify you’re understanding of them is correct\n\n\n\nimg = PILImage.create(img_files[0])\nprint(img.shape)\nimg.to_thumb(160)\n\n(480, 640)\n\n\n\n\n\n\n\n\n\nAnd the books provides the function to use to extract the x/y (point) which is given as …\n\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\n\ndef get_img_center(img_fpath):\n  ctr = np.genfromtxt(img2pose(img_fpath), skip_header=3)\n  x = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n  y = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n\n  return tensor([x,y])\n\n\nget_img_center(img_files[0])\n\ntensor([416.9190, 250.1563])\n\n\nAnd with the above info and methods, we can now construct our DataBlock\n\ndblock = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_img_center,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)]\n)\n\nLet’s break down our blueprint!\n\nDefine the data types for our inputs and targets via the blocks argument.\n\nHere our targets are of type PointBlock. “This is necessary so that fastai knows that the labels represent coordinates … it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images.”\n\nDefine how we’re going to get our images via get_items.\n\nCan just use the get_image_files since we will be passing the path into our DataBlock.dataloaders() method\n\nDefine how, from the raw data, we’re going to create our labels via get_y.\n\nWill simply use the get_img_center we defined above since we will get getting a bunch of paths to images.\n\nDefine how we’re going to create our validation dataset via splitter\n\nHere we define a custom splitter using FuncSplitter, which gives us complete control in how our validation set is determined. Here it will be all the images associated to person “13”.\n\nDefine things we want to do for each item via item_tfms\n\nNothing for this example\n\nDefine things we want to do for each mini-batch of items via batch_tfms\n\nFor each minibatch of data, we’ll resize each image to 320x240 pixels and apply the default augmentations specified in aug_transforms. We’ll also normalize our images used the ImageNet mean/standard deviations since our pretrained model was trained on ImageNet.\n\n\n\n\n\n\nNote\n\n\n\nIf you want to serialize your Learner, do not use lambda functions for defining your DataBlock methods! They can’t be pickled.\n\n\n\ndls = dblock.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n\n\n\n\n\n\n\n\nTo get a feel for what our item_tfms and batch_tfms are doing, we can show_batch using a single image as we do below.\n\ndls.show_batch(unique=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is recommended to see what the tensors look like as well\n\n\n\nxb, yb = dls.one_batch()\nxb.shape, yb.shape, yb[0]\n\n(torch.Size([64, 3, 240, 320]),\n torch.Size([64, 1, 2]),\n TensorPoint([[0.2772, 0.0728]], device='cuda:0'))"
  },
  {
    "objectID": "posts/2022-02-09-ajtfb-chapter-6-regression.html#train-a-model",
    "href": "posts/2022-02-09-ajtfb-chapter-6-regression.html#train-a-model",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Regression",
    "section": "Train a model",
    "text": "Train a model\n\n\n\n\n\n\nImportant\n\n\n\n“Once you think your data looks right, we generally recommend the next step should be using it to train a simple model” See bottom of p193 for why.\n\n\n\ny_range\n\n\n\n\n\n\nTip\n\n\n\ny_range should be used in regression tasks to narrow down the valid range of our targets.\n\n\nIt’s implementation in fastai is:\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\n… and looks like this when plotted:\n\nplot_function(partial(sigmoid_range,low=-1,high=1), min=-4, max=4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSince we know “coordinates in fastai and PyTorch are always rescaled between -1 and 1, we can use those values when defining our Learner\n\n\n\n\nDefine your loss function\nAs we didn’t define a loss function, fastai will pick one for us based on our task. Here is will be MSELoss (mean squared loss).\n“… when coordinates are used as the dependent variable, most of the time we’re likely to be trying to predict something as close as possible; that’s basically what MSELoss does”\n\ndls.loss_func\n\nFlattenedLoss of MSELoss()\n\n\n\n\nMetrics\n\n\n\n\n\n\nTip\n\n\n\n“… MSE is already a useful metric for this task (although it’s probably more interpretable after we take the square root”\n\n\n\n\nDefine our Learner and start training\n\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\n\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(valley=0.0008317637839354575)\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(5, 2e-2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.049521\n0.008220\n02:13\n\n\n1\n0.007028\n0.001976\n01:56\n\n\n2\n0.003018\n0.001024\n01:56\n\n\n3\n0.001997\n0.000425\n01:56\n\n\n4\n0.001606\n0.000190\n01:56\n\n\n\n\n\n“Generally when we run this we got a loss of around 0.0001, which correspondes to this average coordinate prediction error:”\nmath.sqrt(0.0001) \n# 0.01\nThis is pretty accurate …\n\nlearn.show_results(ds_idx=1, max_n=3, figsize=(6,8))"
  },
  {
    "objectID": "posts/2022-02-09-ajtfb-chapter-6-regression.html#summary",
    "href": "posts/2022-02-09-ajtfb-chapter-6-regression.html#summary",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Regression",
    "section": "Summary",
    "text": "Summary\nPick your loss and metrics according to your task …\nFor single-label classification: nn.CrossEntropyLoss and accuracy, precision, recall, f1, etc…\nFor multi-label classification: nn.BCEWithLogitsLoss and accuracy, precision, recall, f1, etc…\nFor regression: nn.MSELoss and the square root of the validation loss as the metric"
  },
  {
    "objectID": "posts/2022-02-09-ajtfb-chapter-6-regression.html#resources",
    "href": "posts/2022-02-09-ajtfb-chapter-6-regression.html#resources",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Regression",
    "section": "Resources",
    "text": "Resources\n\nhttps://book.fast.ai - The book’s website; it’s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc…"
  },
  {
    "objectID": "posts/2020-04-11-finding-datablock-nirvana-part-1.html",
    "href": "posts/2020-04-11-finding-datablock-nirvana-part-1.html",
    "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
    "section": "",
    "text": "# only run this cell if you are in collab\n# !pip install git+https://github.com/fastai/fastai2 \n# !pip install git+https://github.com/fastai/fastcore\nfrom fastai2.vision.all import *"
  },
  {
    "objectID": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#intoduction",
    "href": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#intoduction",
    "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
    "section": "Intoduction",
    "text": "Intoduction\n\nFlying at 50,000 feet\nAt a high level, most machine learning and deep learning systems can be summed up as consisting of three primary elements. Data, an architecture/model, and a loss function. It can be visually described as such:\n\nThe data describes the information given to the model for learning a specific task, and the loss function provides the feedback necessary for the model to improve in that task via a number that tells it how well it is doing.\n\n\nWhy is thinking about our data pipeline important?\nSimple! You can’t have a good model without a good architecture and proper loss function, but you can’t have anything without data. And getting good data that can be transformed into something modelable isn’t necessarily easy. In the slide deck presentation heard throughout the ML world, Andrej Karpathy, Senior Director of Artifical Intelligence at Tesla, put it this way:\n\nComing from academia and the utopia of prepared datasets ready of modeling, he found that in the real world, the bread and butter of a deep learning system and where the blood, sweat, and tears would be shed, was in the data. Data acquisition, cleaning, preparation, and the day-to-day management thereof. This same sentiment can as much be inferred from any of you that watched Jeremy Howard’s v2 walk through in late 2019{% fn 1 %}… every single session was about getting your data modelable using the new v2 bits. That should tell you a lot!\n\n\nSo how do we do it? How do we prepare our datasets for modeling?\nWhile there are many ways, even with fast.ai, most indicators point to it’s DataBlock API as the answer."
  },
  {
    "objectID": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#what-is-the-datablock-api",
    "href": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#what-is-the-datablock-api",
    "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
    "section": "What is the DataBlock API?",
    "text": "What is the DataBlock API?\nThe DataBlock API is a blueprint for transforming your raw data into something that can fed into a model using the fast.ai framework. It is their high-level data API, one that builds upon their low-level Datasets/DataLoaders API, and also their mid-level Transform based API.\nAll three incorporate some new ideas for getting your data good to go, and the choice isn’t necessary one or the other.\n\nDropping down to 30,000 feet … what is it?\nThe DataBlock API consists of THREE main components: getters, transforms, and a splitters.\n\ngetters tell it how to “get” the raw data (e.g., from the file system as file paths, a Pandas DataFrame).\ntransforms tell it how to “transform” that raw data progressively into something that can be fed into a model (e.g., a numeric representation of your inputs and targets).\nsplitters define various strategies you can implore to create your training and validation datasets.\n\nWe’ll be talking a lot about transforms in this article, but one of their most interesting characteristics is that they can be defined to transform your raw data into a numerical representation (as “block transforms”), to run on your CPU when an item from your dataset is fetched (as an “item transform”) , or on the GPU after a mini-batch of your data has been collated into a square matrix and right before it is ran through your model (as a “batch transform”). In fact, there are all kinds of hooks into the data processing pipeline whereby you can apply transforms!\n\n\nAn example\nLet’s break down one of the DataBlock examples from the documentation:\npets = DataBlock(blocks=(ImageBlock, CategoryBlock), \n                 get_items=get_image_files, \n                 splitter=RandomSplitter(),\n                 get_y=Pipeline([attrgetter(\"name\"), RegexLabeller(pat = r'^(.*)_\\d+.jpg$')]),\n                 item_tfms=Resize(128),\n                 batch_tfms=aug_transforms())\nYour getters here are get_items and get_y. The first tells us that our inputs will be coming in the form of filenames returned by the get_image_files function, while the later tells the API to get the labels, or targets, for the inputs by applying a regex to the filename. There is also a get_x method available should you need to apply more specific instructions for defining your input. get_items, get_x, and get_y are all optional. Which you will need to implement depends on your data source and what you need to do to get your inputs/targets.\nThe splitter parameter tells us that we are going to randomly split this data with 80% for training and 20% for validation. How do I know this? Easy. In your notebook put whatever class/method you are interested followed by two ?? to see it’s source.\n\nRandomSplitter??\n\nSo we got our data and we defined how we’re going to split it for training/validation … but how do we actually turn that into something we can feed a neural network? That is where transforms come into play and there are three primary kinds:\n\nThe data transforms defined in the blocks parameter describe how to “transform” your inputs and targets into what you really want to pass in to your model. Here we apply an ImageBlock to our inputs in order to turn the filenames into numerical representations of our images and a CategoryBlock to turn our targets from string labels to a unique set of numerical indexes for each of the possible labels. Essentially what these transforms do is turn your raw data into numbers because your data HAS to be represented numerically to train any kind of ML or DL model.\nNext we define our item transforms via item_tfms. Our only item transform above will resize all our images to 128x128. We do this here because we’ll need squared matrices to pass our images through our network in mini-batches (e.g., a subset of examples), and we can’t create a mini-batch of items until they are all the same shape. These transforms are applied when we fetch an individual item from one of our datasets.\nLastly, we define our batch transforms via batch_tfms for transforms that will be applied to a “mini-batch” of data. Above we’re saying, “There’s a bunch of cool data augmentations we want you to apply to the images in each mini-batch right before you send it through the model.” Again, these transforms are applied on the GPU against a mini-batch of items.\n\nYou can apply transforms to a variety of places in the data processing loop, but these three will satisfy your needs 90-95% of the time.\nUh, okay … so where’s the data?\nRemember that the pets DataBlock is just a blueprint, a pipeline for making raw data into modelable data. How do we build something based on this blueprint? Easy. We just call our DataBlock’s dataloaders() method, passing in the one argument our get_items function, get_image_files, needs … the directory path all your images files are under.\ndls = pets.dataloaders(path/\"images\")\nOnce your pets DataBlock knows the “source” of your data, it goes to work. It gets your image filenames, derives each image’s label from that name, creates a training and validation dataset, and then applies the appropirate transforms, at the appropriate time, so that when you pull items from your DataLoaders object (your dls variable), you have something your model understands. This is the object you pass into your Learner to do the actual training.\nHere’s some code you can run yourself in colab:\n\npath = untar_data(URLs.PETS)                        # &lt;-- Download our data; returns the path to that data\n\npets = DataBlock(blocks=(ImageBlock, CategoryBlock), \n                 get_items=get_image_files, \n                 splitter=RandomSplitter(),\n                 get_y=Pipeline([attrgetter(\"name\"), RegexLabeller(pat = r'^(.*)_\\d+.jpg$')]),\n                 item_tfms=Resize(128),\n                 batch_tfms=aug_transforms())\n\ndls = pets.dataloaders(path/\"images\")               # &lt;-- Tell our DataBlock where the \"source\" is and off it goes\ndls.show_batch(max_n=9)\n# dls.valid.show_batch()"
  },
  {
    "objectID": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#the-basics---pytorch-datasets-dataloaders",
    "href": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#the-basics---pytorch-datasets-dataloaders",
    "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
    "section": "The Basics - PyTorch Datasets & Dataloaders",
    "text": "The Basics - PyTorch Datasets & Dataloaders\nUsing the DataBlock API seems magical (well, it kinda is). We’ve seen how easy it is to build this DataLoaders object that can be used to train our models, but in order to see what it is actually going on, we need to start at the beginning, we need to see how this is done natively in PyTorch.\nDon’t get confused by the similar concepts and names (e.g., Datasets, DataLoaders, transforms, etc…). Many of these ideas are built into PyTorch and extended to do much more in fast.ai. Just remember … we’re only working with PyTorch right now.\nPyTorch itself provides Dataset and DataLoader classes for getting at our data and being able to iteratively run it through our model via mini-batches. Let’s see how!\n\nDataset\nA Pytorch Dataset (see torch.utils.data.Dataset) is defined as “an abstract class representing a dataset”{% fn 2 %}. That’s just a fancy way to say it represents a collection of our data. We inherit from it and implement two key methods:\n__len__: To return the size of our dataset\n__getitem__: To get at a particular item in our dataset.\nLet’s start breaking down our DataBlock above by converting the underlying data representation as one of these Dataset classes. We’ll import some new packages that will be using and create a PetCategories class that will allow us to map our target labels with their indexes (and vice-versa).\n\nimport pdb, re\nfrom torchvision import transforms\n\n\nclass PetCategories():\n    def __init__(self, image_fpaths, lbl_regex):\n        # not all things are images\n        self.lbl_regex = re.compile(lbl_regex)\n        fpaths = [ f  for f in image_fpaths if self.lbl_regex .match(f.name) ]\n        \n        # build our vocab\n        self.vocab = dict(enumerate(set([self.lbl_regex.match(f.name).groups(0)[0]  \n                                         for f in fpaths if self.lbl_regex.match(f.name) ])))\n        # build a reverse lookup\n        self.o2i = L(self.vocab.values()).val2idx()\n        \n    def get_label(self, fname):\n        return self.lbl_regex.match(fname).groups(0)[0]\n\n\nclass PetsDataset(torch.utils.data.Dataset):\n    def __init__(self, image_fpaths, pet_categories, item_tfms=None):\n        \n        # not all things are images\n        self.fpaths = [ f  for f in image_fpaths if f.name.endswith('.jpg')]\n        \n        # our \"item transforms\"\n        self.tfm_pipeline = item_tfms\n        \n        # our labels vocab\n        self.pet_categories = pet_categories\n            \n    def __len__(self):\n        return len(self.fpaths)\n    \n    def __getitem__(self, idx):\n        img_fpath = self.fpaths[idx]\n        img_label = self.pet_categories.get_label(img_fpath.name)\n        \n        # you can think of this as a \"block\" or an \"data transform\"\n        img = Image.open(img_fpath)\n        lbl_idx = self.pet_categories.o2i[img_label]\n        \n        if self.tfm_pipeline: img = self.tfm_pipeline(img)\n            \n        return img, torch.tensor(lbl_idx)\n\nThere is a lot for you to explore above (step through the code, riddle it with pdb.set_trace statements, change it up and see what happens, etc….), but note the following in particular:\n\n__getitem__ needs to return an “example”, which is two things … your inputs/targets and they both need to be tensors.\nitem_tfms represents the PyTorch (not fast.ai) transforms we need to apply to our inputs/targets. We’re going to use a special class named Compose from torchvision to set these up. For now, these transforms will just make sure our images are resized to the same size and converted to a tensor. Again, there is nothing fast.ai here (with the exception of me using the L class) … we’re just dealing with PyTorch righ now. :)\nNotice how we have to create our own vocab and o2i method so we can return an integer representing the “category” rather than the category name (e.g. “Maine_Coon”) itself. Everything has to be a number!\n\nTIP: Run all this code in colab … do it! Make sure you understand what is going on and why. One of the most valuable techniques I use for learning all things Python, PyTorch, and fast.ai, is using pdb.set_trace() to step through and debug code. It’s great way to build inutition by printing out the shapes of tensors, running parts of the code interactively, etc….\nNow …we’re going to need TWO Datasets … one for training and one for validation. We’ll split our examples up randomly and set aside 20% for our validation set. There’s many ways to do this (most better and more efficient that below).\n\nall_images = (path/'images').ls(); len(all_images)\n\n7393\n\n\n\nrnd_idxs = np.random.permutation(len(all_images)); len(rnd_idxs)\n\n7393\n\n\n\ncut = int(len(rnd_idxs) * .2); cut\n\n1478\n\n\n\ntrain_idxs, valid_idxs = rnd_idxs[cut:], rnd_idxs[:cut]\n\nprint(len(train_idxs), len(valid_idxs), len(train_idxs) + len(valid_idxs))\n\n5915 1478 7393\n\n\nTIP: Notice how I print out lengths and shapes of tensors as I go? Doing that provides both a sanity check and ensure you are seeing what you expect before going further down the rabbit hole.\nNow, we can create our training and validation Datasets.\nAgain, we are NOT using fast.ai transforms here … these are all built into the torchvision package. They serve the same purpose here as the fast.ai “item transforms”, but for now, we’re doing this all using just the PyTorch bits.\n\nitem_tfms = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.ToTensor()\n])\n\n\ncategories = PetCategories(all_images[train_idxs], lbl_regex=r'^(.*)_\\d+.jpg$')\nlen(categories.vocab)\n\n37\n\n\n\ntrain_ds = PetsDataset(all_images[train_idxs], pet_categories=categories, item_tfms=item_tfms)\nvalid_ds = PetsDataset(all_images[valid_idxs], pet_categories=categories, item_tfms=item_tfms)\n\nprint(len(train_ds), len(valid_ds))\nprint(train_ds[20][0].shape, train_ds[20][1])\n\n5913 1477\ntorch.Size([3, 224, 224]) tensor(33)\n\n\n\n\nDataLoader\nWith that we can create a torch.utils.data.DataLoader from each Dataset. The primary reason we need this object is to yield mini-batches of data into our model, but as you can see, it also provides us the ability to do much more (e.g., shuffle data, provide a collate function, etc…). Check out the docs for more info!\nNote: fast.ai has it’s own DataLoader class that extends THIS one from PyTorch. Yah, I know it can seem confusing, but just remember for now, we are only working with functionality built-in to PyTorch.\n\nbsz = 64\n\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=bsz, shuffle=True)\nvalid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=bsz*2, shuffle=False)\n\nAnd voila, we can now iterate through our dataset, mini-batch by mini-batch\n\nb = next(iter(valid_dl))\nlen(b), b[0].shape, b[1].shape\n\n(2, torch.Size([128, 3, 224, 224]), torch.Size([128]))\n\n\nWow … that took quite a bit more work than the 6 lines of code to create a DataBlock, and it’s still not as functional. For example, we haven’t built anything that can decode items, show batches, or allow us to easily adjust/extend the objects we created above.\nSo let’s keep going. Starting with the low-level API, we can take these PyTorch Dataset and DataLoader objects more friendly for fast.ai Learners."
  },
  {
    "objectID": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#using-the-low-level-api---fast.ai-dataloaders",
    "href": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#using-the-low-level-api---fast.ai-dataloaders",
    "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
    "section": "Using the Low-Level API - fast.ai DataLoaders",
    "text": "Using the Low-Level API - fast.ai DataLoaders\nIt’s actually pretty easy to get your PyTorch Dataset class incorporated into fast.ai and get it to play nicely with fast.ai’s custom DataLoaders.\n\nfrom fastai2.data.core import DataLoaders\n\n\ndls = DataLoaders.from_dsets(train_ds, valid_ds)\n\n\nb = dls.one_batch()\n\n\nlen(b), b[0].shape, b[1].shape\n\n(2, torch.Size([64, 3, 224, 224]), torch.Size([64]))\n\n\nI told you it was simple, didn’t I?\nNotice that we didn’t have to change anything in our PyTorch Dataset to create a DataLoaders object we can pass to our Learner for training. This is nice because it means, given a standard PyTorch Dataset, you can use all the wonderful fast.ai bits for training in less than 3 lines of code.\nTip: If you don’t care about being able to show batches, show results, and this satisfies your needs … STOP! You’re good to go. Don’t overthink you’re problem or over-engineer a solution to a problem that doesn’t necessarily exist. Remember: You don’t have to use the mid-level API or DataBlocks to use fast.ai!"
  },
  {
    "objectID": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#using-the-mid-level-api---converting-your-dataset-into-a-transform",
    "href": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#using-the-mid-level-api---converting-your-dataset-into-a-transform",
    "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
    "section": "Using the Mid-Level API - Converting Your Dataset into a Transform",
    "text": "Using the Mid-Level API - Converting Your Dataset into a Transform\nBUT what if we want to apply/change our transforms, or run transforms on the GPU after we have a batch, or be able to visualize our data in our datasets and dataloaders or even our predictions? To begin with, we can convert our Dataset into a Transform by doing 4 things:\n\nInherit from Transform instead of torch.utils.data.Dataset\nChange your __getitem__ into encodes. According to the docs … “a Transform in fastai calls the encodes method when you apply it on an item (a bit like PyTorch modules call forward when applied on something).”{% fn 3 %} Here it will return the numerical representations of our data in the form of tensors.\nChange your return type to be a tuple and optionally use fastai’s semantic types (here we wrap our image in TensorImage which knows how to show itself). From the docs: “If you then return a tuple (or a subclass of a tuple), and use fastai’s semantic type, you can then apply any other fastai’s transform on your data and it will be dispatched properly.”{% fn 4 %} That simply means we can add on more transforms that know how to work with TensorImage objects and they’ll do the right thing.\nGet rid of __len__\n\n\nclass PetsTransform(Transform):\n    def __init__(self, image_fpaths, pet_categories, item_tfms=None):\n        \n        # not all things are images\n        self.fpaths = [ f  for f in all_images if f.name.endswith('.jpg')]\n        \n        # our pytorch \"item transforms\"\n        self.tfm_pipeline = item_tfms\n        \n        # our labels vocab\n        self.pet_categories = pet_categories\n            \n    def __len__(self):\n        return len(self.fpaths)\n    \n    def encodes(self, idx):\n        img_fpath = self.fpaths[idx]\n        img_label = self.pet_categories.get_label(img_fpath.name)\n        \n        # you can think of this as a \"block\" or an \"data transform\"\n        img = Image.open(img_fpath)\n        lbl_idx = self.pet_categories.o2i[img_label]\n        \n        if self.tfm_pipeline: img = self.tfm_pipeline(img)\n            \n        return (TensorImage(img), torch.tensor(lbl_idx))\n\nNow that we are using a Transform, we have to use a new kind of object to build our dataset: TfmdLists\nA TfmdList is “just an object that lazily applies a collection of Transforms on a list.”{% fn 5 %} Think of it as a fancy Dataset object that knows how to work with Transform objects.\n\ntrain_fpaths = all_images[train_idxs]\nvalid_fpaths = all_images[valid_idxs]\n\ntrain_tl= TfmdLists(range(len(train_idxs)), PetsTransform(train_fpaths, \n                                                          pet_categories=categories, \n                                                          item_tfms=item_tfms))\n\nvalid_tl= TfmdLists(range(len(valid_idxs)), PetsTransform(valid_fpaths, \n                                                          pet_categories=categories, \n                                                          item_tfms=item_tfms))\n\nSince this is just another kind of dataset, we can pass these TfmdLists objects to DataLoaders just like before. But notice, we can now add fast.ai transforms to it just like we did in the DataBlock example at the top. We’re already resizing and converting the examples to tensors, so we’ll add some after_batch transforms for normalization and augmentations.\n\ndls = DataLoaders.from_dsets(train_tl, valid_tl, \n                             after_batch=[Normalize.from_stats(*imagenet_stats), *aug_transforms()])\ndls = dls.cuda()\n\n\nb = dls.one_batch()\nlen(b), b[0].shape, b[1].shape\n\n(2, torch.Size([64, 3, 224, 224]), torch.Size([64]))\n\n\nLet’s see if we can show a batch of our data. Uncomment the line below, run it, and yah … it throws an exception. But why?\n\n# dls.show_batch()\n\nIf you guessed it is because show_batch doesn’t know what to do with the target’s numerical index, bingo! You’re right.\nLet’s start to fix that by actually creating our own class that represents our inputs/targets. Notice that besides inheriting from Tuple, all we are providing is a show method that tells a PetImage object how to show itself. According to the docs, “fastai will call [your transforms decodes methods] until it arrives at a type that knows how to show itself, then call the show method on this type.”{% fn 6 %}\nBTW, a lot of this code is just ripped from the “Siamese tutorial” in the docs, so don’t be too impressed. If you want to really do a deep dive and work though all this given a different task, check it out here.\n\nclass PetImage(Tuple):\n    def show(self, ctx=None, **kwargs):\n        img, category_idx = self\n        if not isinstance(img, Tensor):\n            img_tensor = tensor(img)\n            img_tensor = img_tensor.permute(2,0,1)\n        else: \n            img_tensor = img\n\n        return show_image(img_tensor, title=categories.vocab[category_idx], ctx=ctx, **kwargs)\n\nThe show method knows how to work with tensors or PIL images. The last method is a helper method available in fast.ai to actually show an image and print it’s title above it. If you pass in a ctx it will use that to format and place the images appropriate. A context can be something like a matplotlib axis or a DataFrame … it “represents the object where we will show our thing.”{% fn 7 %}\nNow let’s make some changes to our PetsTransform to make it a bit more fastai’sh.\nFirst, we’ll use PILImage.create to create the image in encodes. We do this because that object allows us to apply fast.ai transform liks Resize and ToTensor directly on it.\nSecond, we’re going to move to using fast.ai transforms for everything, so we’ll get rid of the PyTorch transforms!\nThird, notice our encodes now returns a PetsImage. It’s just a tuple … but because its a particular kind of tuple, we can use the typdispatched show_batch and show_results to actually visualize our data/results.\n\nclass PetsTransform2(Transform):\n    def __init__(self, image_fpaths, pet_categories):\n        \n        # not all things are images\n        self.fpaths = [ f  for f in all_images if f.name.endswith('.jpg')]\n        \n        # our labels vocab\n        self.pet_categories = pet_categories\n            \n    def __len__(self):\n        return len(self.fpaths)\n    \n    def encodes(self, img_fpath):\n        img = PILImage.create(img_fpath)\n        img_label = self.pet_categories.get_label(img_fpath.name)\n        lbl_idx = self.pet_categories.o2i[img_label]\n        \n        return PetImage(img, lbl_idx)\n\nBecause of these changes, instead of creating the separate TfmdLists ourselves, we can now further do things the “fast.ai way” by using a splitter to do that for us. Here we’ll use RandomSplitter which gives us that same 80/20 training/validation split.\n\nsplits = RandomSplitter()(all_images)\ntfm = PetsTransform2(all_images, categories)\n\nNow we can get both our datasets in one line of code! When we pass splits to TfmdLists, it takes care of creating our training and validation datasets!\n\ntls = TfmdLists(all_images, tfm, splits=splits)\n\nAnd thanks for our PetImage class, fast.ai can show an item from our dataset.\n\nshow_at(tls.valid, 0)\n\n\n\n\n\n\n\n\nEven better, we can now specify all our transforms using fast.ai in the call to dataloaders(). And because these are fast.ai DataLoader objects, we can add tranforms at any point in our data processing pipeline (not just after_item and after_batch).\n\ndls = tls.dataloaders(after_item=[Resize(224), ToTensor], \n                      after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])\n\nIn the process, notice how we’ve also refactored our code into something much more reusable. For example, if we want to resize our images to something else, its as easy as …\n\nnew_dl = dls.new(after_item=[Resize(64), ToTensor])\nnew_dl.one_batch()[0].shape\n\ntorch.Size([64, 3, 64, 64])\n\n\nAnd what about showing a batch of data? Unfortunately it still won’t work. show_batch is designed primarily to work with the DataBlock API, but here, we’re returning the whole thing as a single transform.\nThe solution is easy: use the @typedispatch mechanism and override show_batch so that our x (our input) is “typed”.\n\nb = dls.one_batch()\n\n\ndls._types, type(b)\n\n({__main__.PetImage: [fastai2.torch_core.TensorImage, torch.Tensor]},\n __main__.PetImage)\n\n\n\n@typedispatch\ndef show_batch(x:PetImage, y, samples, ctxs=None, max_n=6, nrows=None, ncols=3, figsize=None, **kwargs):\n    if figsize is None: figsize = (ncols*6, max_n//ncols * 3)\n    if ctxs is None: ctxs = get_grid(min(x[0].shape[0], max_n), nrows=None, ncols=ncols, figsize=figsize)\n    for i,ctx in enumerate(ctxs): \n        PetImage(x[0][i], x[1][i].item()).show(ctx=ctx)\n\n\ndls.show_batch()\n\n\n\n\n\n\n\n\nWhen dls.show_batch() runs, it will find the closes matching version of show_batch() available to execute given chat the batch is. We could even write a typedispatched show_results() to look at our predictions alongside our targets using the same technique we applied to show_batch().\nUsing the mid-level API, you not only have a Dataloaders object good to go for training … you have one that you can use to show your data and extend by applying/changing as many transforms to wherever you want in the data processing pipeline.\nWhat could be better than this?\nAnswer: Doing all this with &lt; 10 lines of code using the DataBlock API.\nWe’ve already looked at how it works above, now, we’ll look at the questions you need to ask to construct it in accordance with your data and task. Again, if the above gets you where you need to be, you don’t need to use the high-level DataBlock API. There is no right option for every task and there are many ways to get where you need to go."
  },
  {
    "objectID": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#using-the-high-level-api---datablocks",
    "href": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#using-the-high-level-api---datablocks",
    "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
    "section": "Using the High-Level API - DataBlocks",
    "text": "Using the High-Level API - DataBlocks\nHaving looked at the basic data-processing units in PyTorch, then to the low and mid-level APIs available in fast.ai, you’re probably wondering, “Ok, how can I do all that by drawing up a DataBlock blueprint for my task?”\nThe path to enlightment comes in the form of answering 7 questions.\n\nAsking the right questions\nAssuming you understand your task and data, once you’ve answered these 7 questions you’ll know everything you need to construct your own DataBlock. These come right out of the DataBlock tutorial so check that for even more details and example implementations!\n\nWhat are the types of your inputs and targets? (e.g., images/categories)\nWhere is your data? (e.g., filenames in folders, a DataFrame, a database)\nDo we need to do anything special to get our “inputs”? If so, use get_x\nDo we need to do anything special to get our “targets”? If yes, use get_y\nHow do you want to split the data into training and validation sets? Use splitter\nDo we need to do anything when we get an item? If yes, define that in item_tfms\nDo we need to do anything to a “mini-batch” of data? If yes, define that in batch_tfms\n\n\n\nGetting the right answers\nLooking back at our example DataBlock …\npets = DataBlock(blocks=(ImageBlock, CategoryBlock), \n                 get_items=get_image_files, \n                 splitter=RandomSplitter(),\n                 get_y=Pipeline([attrgetter(\"name\"), RegexLabeller(pat = r'^(.*)_\\d+.jpg$')]),\n                 item_tfms=Resize(128),\n                 batch_tfms=aug_transforms())\nWe knew how to construct it as such because:\n1.What are the types of your inputs and targets?\nAnswer: inputs=pet images | targets=37 categories. So we need an ImageBlock to handle the images and a CategoryBlock to handle the labels. Those blocks will add the needed transforms for each of their respective pieces.\n2.Where is your data?\nAnswer: filenames\n3.Do we need to do anything special to get our “inputs”?\nAnswer: No, get_items will get our input images.\n4.Do we need to do anything special to get our “targets”?\nAnswer: Yes, we need to implement a get_y to get our labels from the image file name.\n5.How do you want to split the data into training and validation sets?\nAnswer: We just want a random 80/20 split, so use RandomSplitter\n6.Do we need to do anything when we get an item?\nAnswer: Yes, we need to resize our images so they are the same shape and can be included together in a mini-batch. Do this in item_tfms\n7.Do we need to do anything to a “mini-batch” of data?\nAnswer: Yes, we’d like to add some randomization to the images by applying data augmentations on the GPU. Do this with batch_tfms"
  },
  {
    "objectID": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#tips-tricks-best-practices-a-bunch-of-good-things-to-know",
    "href": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#tips-tricks-best-practices-a-bunch-of-good-things-to-know",
    "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
    "section": "Tips, Tricks, Best Practices, & A Bunch of Good Things to Know",
    "text": "Tips, Tricks, Best Practices, & A Bunch of Good Things to Know\nBelow are some of the more important things and best practices to be aware of when working with the DataBlock API. It’s in no way exhaustive, but anything I’ve had to lookup multiple times is listed here.\n\nWhat happens if I don’t define how to get my targets (my y)?\nIf you don’t specify your labels, the DataBlock API will assume they are the same as your inputs. This is atypical for most tasks, but not entirely useless. According to the docs, “by default, the data block API assumes we have an input and a target, which is why we see our filename repeated twice” whenever you view the results of your datasets/dataloaders without a y specified.{% fn 8 %}\n\n\nCan I have multiple inputs/targets?\nYes! According to the docs … “You can also have more than two blocks (if you have multiple inputs and/or targets), you would just need to pass n_inp to the DataBlock to tell the library how many inputs there are (the rest would be targets) and pass a list of functions to get_x and/or get_y (to explain how to process each item to be ready for his type).”{% fn 9 %} We’ll explore this in Part 2 of this series where I attempt to update my v1 MixedTabluarList object (incorporates tabular + text) into something v2 friendly. In the meantime, here’s a nice example from the docs on setting up a dataset for object detection:\ncoco = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock),\n                 get_items=get_image_files,\n                 splitter=RandomSplitter(),\n                 get_y=[lambda o: img2bbox[o.name][0], lambda o: img2bbox[o.name][1]], \n                 item_tfms=Resize(128),\n                 batch_tfms=aug_transforms(),\n                 n_inp=1)\nYou see that n_inp? It’s saying, “Use the ImageBlock for my inputs (I only have 1), but I’ll need TWO targets this time as I’m trying to predict the location of an object (BBoxBlock) and it’s label (BBoxLblBlock).” Notice also because we are predicting TWO things, our get_y returns a list of, you guessed it, two things. If we didn’t need to do anything special with either of these targets, we’d simply pass noop in it’s place in that list.\n\n\nWhere can I learn about the baked in bits of the DataBlock API?\nThe API already has a lot of useful classes and functions suitable for defining your getters, splitter, and transforms across a number of application types. The full list is here: http://dev.fast.ai/data.transforms\n\n\nWhat if something goes wrong? Or what if I want to make sure my DataBlock is doing what I think it is?\nUse dblock.summary(path). If there is an error, this thing will bomb out where it is encountered … else, you’ll be able to verify that all the wonderful things your 5-10 lines of code above does what you expect.\n\n\nDo I need to always use get_items?\nNo. For example, if your “source” data is a DataFrame …\npascal = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=ColSplitter(),\n                   get_x=ColReader(0, pref=pascal_source/\"train\"),\n                   get_y=ColReader(1, label_delim=' '),\n                   item_tfms=Resize(224),\n                   batch_tfms=aug_transforms())\n                   \ndls = pascal.dataloaders(df)\nAccording to the docs … “we wont have to use a get_items function here because we already have all our data in one place.”{% fn 10 %}\n\n\nWhat are different ways I can get my x and y from a DataFrame?\nUsing ColReader:\nget_x=ColReader(0, pref=pascal_source/\"train\"),\nget_y=ColReader(1, label_delim=' ')\nUsing lambda functions:\nget_x=lambda x:pascal_source/\"train\"/f'{x[0]}',\nget_y=lambda x:x[1].split(' '),\nUsing column names:\nget_x=lambda o:f'{pascal_source}/train/'+o.fname,\nget_y=lambda o:o.labels.split(),\nUsing from_columns:\ndef _pascal_items(x): return (f'{pascal_source}/train/'+x.fname, x.labels.str.split())\n\nvalid_idx = df[df['is_valid']].index.values\n\npascal = DataBlock.from_columns(blocks=(ImageBlock, MultiCategoryBlock),\n                   get_items=_pascal_items,\n                   splitter=IndexSplitter(valid_idx),\n                   item_tfms=Resize(224),\n                   batch_tfms=aug_transforms())\n                  \nAccording to the docs, this is “the most efficient way (to avoid iterating over the rows of the dataframe, which can take a long time) …. It will use get_items to convert the columns in numpy arrays. The drawback is that since we lose the dataframe after extracting the relevant columns, we can’t use a ColSplitter anymore.”{% fn 11 %}\n\n\nWhat about tabular data?\nWe’ll explore the tabular bits in a later part, but as the docs say, the “tabular data doesn’t really use the data block API as it’s relying on another API with TabularPandas for efficient preprocessing and batching.”{% fn 12 %} Of course, where there is a will, there is a way, and so we’ll see a possible solution in Part 2 or 3 of this series :)."
  },
  {
    "objectID": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#summary",
    "href": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#summary",
    "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
    "section": "Summary",
    "text": "Summary\nAs the famous song goes, “we’ve only just begun ….” In future installments we’ll dig into more of the particulars of the entire fast.ai data stack, and see how we can use it to solve some “out-of-the-box” tasks.\nIn the meantime, the best way for you to get a better handle on what’s what, is to mess around with the many examples found in the v2 documentation here."
  },
  {
    "objectID": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#references",
    "href": "posts/2020-04-11-finding-datablock-nirvana-part-1.html#references",
    "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
    "section": "References",
    "text": "References\n\nhttp://dev.fast.ai/tutorial.datablock\nhttp://dev.fast.ai/tutorial.siamese\nhttp://dev.fast.ai/data.block\nhttp://dev.fast.ai/data.transforms\nfastai v2 walk-thru playlist\nZach Mueller’s “A Guided Walk-through of 2.0”: Lesson 1\n\n{{ ‘See full playlist here’ | fndetail: 1 }} {{ ‘https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class’ | fndetail: 2 }} {{ ‘http://dev.fast.ai/tutorial.siamese#Using-the-mid-level-API’ | fndetail: 3 }} {{ ‘Ibid.’ | fndetail: 4 }} {{ ‘http://dev.fast.ai/tutorial.siamese#Using-the-mid-level-API’ | fndetail: 5 }} {{ ‘http://dev.fast.ai/tutorial.siamese#Making-show-work’ | fndetail: 6 }} {{ ‘Ibid.’ | fndetail: 7 }} {{ ‘http://dev.fast.ai/tutorial.datablock’ | fndetail: 8 }} {{ ‘Ibid.’ | fndetail: 9 }} {{ ‘Ibid.’ | fndetail: 10 }} {{ ‘Ibid.’ | fndetail: 11 }} {{ ‘Ibid.’ | fndetail: 12 }}"
  },
  {
    "objectID": "posts/2024-12-13-gemini-part-1.html",
    "href": "posts/2024-12-13-gemini-part-1.html",
    "title": "Gemini (Part I) - Why You Should Consider Gemini",
    "section": "",
    "text": "Gemini is a collection of smaller LLMs designed to deliver faster performance and lower costs while offering capabilities comparable to larger models. The recently released Gemini 2.0 builds on this foundation with enhanced performance, innovative real-time features, and a competitive price point that sets it apart from the competition.\nIn this series, we’ll explore the exciting possibilities with Gemini, including the new real-time features introduced in 2.0. To start, we’ll review the range of Gemini models, their unique capabilities, and how to choose the right one for your needs. From there, we’ll dive into how these models stack up against the competition in terms of both results and cost.\nThe core hypothesis driving this series is that the future lies in agentic systems composed of smaller and more cost efficient models. I believe organizations will achieve greater value by orchestrating smaller and general-use models to meet their goals, rather than by relying solely on large and expensive LLMs to address every use case."
  },
  {
    "objectID": "posts/2024-12-13-gemini-part-1.html#gemini-in-a-nutshell",
    "href": "posts/2024-12-13-gemini-part-1.html#gemini-in-a-nutshell",
    "title": "Gemini (Part I) - Why You Should Consider Gemini",
    "section": "",
    "text": "Gemini is a collection of smaller LLMs designed to deliver faster performance and lower costs while offering capabilities comparable to larger models. The recently released Gemini 2.0 builds on this foundation with enhanced performance, innovative real-time features, and a competitive price point that sets it apart from the competition.\nIn this series, we’ll explore the exciting possibilities with Gemini, including the new real-time features introduced in 2.0. To start, we’ll review the range of Gemini models, their unique capabilities, and how to choose the right one for your needs. From there, we’ll dive into how these models stack up against the competition in terms of both results and cost.\nThe core hypothesis driving this series is that the future lies in agentic systems composed of smaller and more cost efficient models. I believe organizations will achieve greater value by orchestrating smaller and general-use models to meet their goals, rather than by relying solely on large and expensive LLMs to address every use case."
  },
  {
    "objectID": "posts/2024-12-13-gemini-part-1.html#the-gemini-family",
    "href": "posts/2024-12-13-gemini-part-1.html#the-gemini-family",
    "title": "Gemini (Part I) - Why You Should Consider Gemini",
    "section": "The Gemini Family",
    "text": "The Gemini Family\n  Figure 1: The Gemini Famly (google) \n\n\n\n\n\n\nNote\n\n\n\nWhen in doubt, use “Flash” to start and see how far that gets you. Personally, I’m getting great results for structured output tasks with Flash and sometimes even the 8B parameter variant."
  },
  {
    "objectID": "posts/2024-12-13-gemini-part-1.html#performance",
    "href": "posts/2024-12-13-gemini-part-1.html#performance",
    "title": "Gemini (Part I) - Why You Should Consider Gemini",
    "section": "Performance",
    "text": "Performance\nInternal Benchmarks\nTo better understand how the latest Gemini 2.0 Flash Experimental model stacks up against its predecessors, let’s examine the reported benchmark results.\nThe following table highlights the performance improvements compared to the Gemini 1.5 generation models.\n  Figure 2: Reported benchmarks (google) \nThe 1.5 models have worked really well for me across a number of projects and it looks like the 2.0 generation represents a substantial move in the right direction across the board! According to Google’s  announcement post …\n\n“Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed”\n\nHow Does It Compare to the Competition?x\nEvaluating performance across large language models (LLMs) can be challenging due to the abundance of available benchmarks, not all of which provide consistent or reliable insights. Among these options, one resource that I’ve found consistently aligns with my own experiences is lmarena.ai (formerly LMSYS), which I will reference for this comparison.\n  Figure 2: Overall performance comparison across leading LLM models (courtesy of lmarena.ai)    \n  Figure 3: Detailed breakdown of performance metrics across different categories (courtesy of lmarena.ai)    \nThat’s alot of Geminis up there at the top!"
  },
  {
    "objectID": "posts/2024-12-13-gemini-part-1.html#cost",
    "href": "posts/2024-12-13-gemini-part-1.html#cost",
    "title": "Gemini (Part I) - Why You Should Consider Gemini",
    "section": "Cost",
    "text": "Cost\nAs AI becomes an increasingly integral part of application infrastructure, the cost of using these models is becoming a critical consideration. Understanding how costs vary across providers like OpenAI, Anthropic, and others is essential for making informed decisions. So, how do the costs of using these models generally compare, and where does Gemini stand in this landscape?\n\n\n\n\n\n\n\n\n\n\nProvider\nModel\nMax Tokens\nInput Cost (per 1M)\nOutput Cost (per 1M)\n\n\n\n\nOpenAI\nGPT-4o\n128,000\n$2.50\n$10.00\n\n\nOpenAI\nGPT-4o mini\n128,000\n$0.15\n$0.60\n\n\nOpenAI\nGPT-4 Turbo\n128,000\n$10.00\n$30.00\n\n\nOpenAI\nGPT-3.5 Turbo\n16,384\n$0.50\n$1.50\n\n\nAnthropic\nClaude 3.5 Sonnet\n200,000\n$3.00\n$15.00\n\n\nAnthropic\nClaude 3.5 Haiku\n200,000\n$0.25\n$1.25\n\n\nGoogle\nGemini 1.5 Pro\n1,048,576\n$1.25\n$5.00\n\n\nGoogle\nGemini 1.5 Flash\n1,048,576\n$0.075\n$0.30\n\n\n\nNote: Input tokens refer to the tokens in the prompt sent to the model, while output tokens are those generated by the model in response.\n\n\n\n\n\n\nNote\n\n\n\nThe cost of using Gemini is often a fraction of the cost of other models. In fact, for “experimental” models, you aren’t billed at all\n\nYou aren’t billed for the usage of experimental Google models.\nGenerative AI on Vertex AI pricing for experimental models"
  },
  {
    "objectID": "posts/2024-12-13-gemini-part-1.html#conclusion",
    "href": "posts/2024-12-13-gemini-part-1.html#conclusion",
    "title": "Gemini (Part I) - Why You Should Consider Gemini",
    "section": "Conclusion",
    "text": "Conclusion\nIf the future is indeed agentic, as I hypothesize, and cost-efficient, high-performing, general-use models play a central role in that vision, then Gemini emerges as a compelling choice. Based on the comparison of performance between Gemini models and those of other major API providers, combined with its cost-effectiveness, Gemini is undoubtedly worth considering as part of your AI strategy moving forward.\nIn the next blog in this series, we’ll explore the basics of using Gemini and dive into some of its standout features that set it apart from the competition using the new and improved unified SDK in 2.0. This unified SDK is one of the more exicting bits to developers like myself that have had to bounce back-and-forth between using the Gemini API and Vertex API to talk with Gemini. With 2.0, we finally have one SDK to rule them all (even if it still means we need to understand how the two different API options vary and when to use one over the other … something we’ll talk about in the next post as well)."
  },
  {
    "objectID": "posts/2021-06-03-ajtfb-chapter-5.html",
    "href": "posts/2021-06-03-ajtfb-chapter-5.html",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 5: Multiclass classification",
    "section": "",
    "text": "Other posts in this series:\nA Journey Through Fastbook (AJTFB) - Chapter 1\nA Journey Through Fastbook (AJTFB) - Chapter 2\nA Journey Through Fastbook (AJTFB) - Chapter 3\nA Journey Through Fastbook (AJTFB) - Chapter 4\nA Journey Through Fastbook (AJTFB) - Chapter 6a\nA Journey Through Fastbook (AJTFB) - Chapter 6b\nA Journey Through Fastbook (AJTFB) - Chapter 7\nA Journey Through Fastbook (AJTFB) - Chapter 8\nA Journey Through Fastbook (AJTFB) - Chapter 9"
  },
  {
    "objectID": "posts/2021-06-03-ajtfb-chapter-5.html#multiclass-vs-multi-label-classification",
    "href": "posts/2021-06-03-ajtfb-chapter-5.html#multiclass-vs-multi-label-classification",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 5: Multiclass classification",
    "section": "Multiclass vs Multi-label classification …",
    "text": "Multiclass vs Multi-label classification …\nYah, it can be confusing!\nAnyhow, multiclass classification is all about predicting a SINGLE CLASS an object belongs to from a list of two or more classes. It can be a simple as predicting whether an image is a dog or a cat, or as complex as predicting the breed of dog from amongst dozens of potential breeds.\nMulti-label classification (covered in the next chapter) involves predicting MULTIPLE CLASSES to which an object belongs; it can belong to one, some, all, or even none of those classes. For example, you may be looking at satellite photos from which you need to predict the different kinds of terrain (your classes) each contains."
  },
  {
    "objectID": "posts/2021-06-03-ajtfb-chapter-5.html#defining-your-datablock",
    "href": "posts/2021-06-03-ajtfb-chapter-5.html#defining-your-datablock",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 5: Multiclass classification",
    "section": "Defining your DataBlock",
    "text": "Defining your DataBlock\nAgain, the DataBlock is a blueprint for everything required to turn your raw data (images and labels) into something that can be fed through a neural network (DataLoaders with a numerical representation of both your images and labels). Below is the one presented in this chapter.\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\n\n\n\n\n\ndblock = DataBlock(blocks=(ImageBlock, CategoryBlock), \n                   get_items=get_image_files, \n                   get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                   splitter=RandomSplitter(seed=42),   \n                   item_tfms=Resize(460), \n                   batch_tfms=aug_transforms(size=224, min_scale=.75))\n\nLet’s break down our blueprint!\n\nDefine the data types for our inputs and targets via the blocks argument.\n\nThis is defined as a tuple, where we tell our DataBlock that the imputs are images and our targets are a single category (or class or label).\n\nDefine how we’re going to get our raw data via get_items.\n\nWe use get_image_files because we are getting image files from the filesystem. When we kick off the DataBlock to build our DataLoaders, we’ll pass in the path to our images which will in turn be passed to get_image_files to pull the raw data.\n\nDefine how, from the raw data, we’re going to create our labels (e.g., the classes for each image) via get_y.\n\nIn this case, we don’t need to define a get_x because get_items gets the x’s already. However, since we are working with filenames from which we want to define our labels, we do need this fancy get_y function above. using_attr tells the RegexLabeller what attribute of our data to apply itself too, and since our data is filenames, we tell it to use the .name property of each filename object as the thing the RegexLabeller acts against. That will give us our target class.\n\nDefine how we’re going to create our validation dataset via splitter\nDefine things we want to do for each item via item_tfms\n\nitem_tfms are transforms, or things we want to do, to each input individually! Above we only have one which says, “Resize each image to 460 max width/height” one by one when we grab it. For individual images to be collated into mini-batches, they have to be the same size … thus we do this here and not below.\n\nDefine things we want to do for each mini-batch of items via batch_tfms\n\nbatch_tfms are transforms, or things we want to do, to a mini-batch of inputs at once on the GPU. aug_transforms includes a bunch that have proven to be effective in computer vision tasks. With the parameters we’re passing into it above (size=224, min_scale=.75), we’re saying, “Take the mini-batch of images here and randomly crop the 460x460 images to be 224x224 that captures at least 3/4 of the image”. See here for more info on RandomResizedCrop.\n\n\n\n\n\n\nImportant\n\n\n\nIf you can describe your DataBlock like I have above, you understand it!\n\n\n\n\n\n\n\n\nImportant\n\n\n\nVerify your DataBlock works as expected, or else troubleshoot it, by running DataBlock.summary(data)\n\n\n\ndblock.summary(path/'images')\n\nSetting-up type transforms pipelines\nCollecting items from /root/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: partial -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /root/.fastai/data/oxford-iiit-pet/images/Bombay_111.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=604x453\n  Pipeline: partial -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n    starting from\n      /root/.fastai/data/oxford-iiit-pet/images/Bombay_111.jpg\n    applying partial gives\n      Bombay\n    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorCategory(3)\n\nFinal sample: (PILImage mode=RGB size=604x453, TensorCategory(3))\n\n\nCollecting items from /root/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: partial -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\nSetting up after_item: Pipeline: Resize -- {'size': (460, 460), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} -&gt; ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -&gt; Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -&gt; RandomResizedCropGPU -- {'size': (224, 224), 'min_scale': 0.75, 'ratio': (1, 1), 'mode': 'bilinear', 'valid_scale': 1.0, 'max_scale': 1.0, 'p': 1.0} -&gt; Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: Resize -- {'size': (460, 460), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} -&gt; ToTensor\n    starting from\n      (PILImage mode=RGB size=604x453, TensorCategory(3))\n    applying Resize -- {'size': (460, 460), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} gives\n      (PILImage mode=RGB size=460x460, TensorCategory(3))\n    applying ToTensor gives\n      (TensorImage of size 3x460x460, TensorCategory(3))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -&gt; Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -&gt; RandomResizedCropGPU -- {'size': (224, 224), 'min_scale': 0.75, 'ratio': (1, 1), 'mode': 'bilinear', 'valid_scale': 1.0, 'max_scale': 1.0, 'p': 1.0} -&gt; Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False}\n    starting from\n      (TensorImage of size 4x3x460x460, TensorCategory([ 3, 17, 10,  4], device='cuda:0'))\n    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n      (TensorImage of size 4x3x460x460, TensorCategory([ 3, 17, 10,  4], device='cuda:0'))\n    applying Flip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} gives\n      (TensorImage of size 4x3x460x460, TensorCategory([ 3, 17, 10,  4], device='cuda:0'))\n    applying RandomResizedCropGPU -- {'size': (224, 224), 'min_scale': 0.75, 'ratio': (1, 1), 'mode': 'bilinear', 'valid_scale': 1.0, 'max_scale': 1.0, 'p': 1.0} gives\n      (TensorImage of size 4x3x224x224, TensorCategory([ 3, 17, 10,  4], device='cuda:0'))\n    applying Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False} gives\n      (TensorImage of size 4x3x224x224, TensorCategory([ 3, 17, 10,  4], device='cuda:0'))\n\n\nNow we can create our DataLoaders and take a look at our x’s and y’s, our pet images and their label/class\n\ndls = dblock.dataloaders(path/'images')\ndls.show_batch()\n\n\n\n\n\n\n\n\nTo get a feel for what our batch_tfms are doing, we can show_batch using a single image as we do below.\n\ndls.show_batch(unique=True)\n\n\n\n\n\n\n\n\nThe combination of what we’re doing in the item_tfms and batch_tfms is known as presizing.\n“Presizing is a particular way to do iamge augmentation taht is designed to minimize data destruction while maintaining good performance.” After resizing all the images to a larger dimension that we will train on, we perform all our core augmentations on the GPU. This results in both faster and less destructive transformations of the data.\n\n\n\n\n\n\nImportant\n\n\n\nSee pp190-191 for how these augmentations are applied to the training and validation set!"
  },
  {
    "objectID": "posts/2021-06-03-ajtfb-chapter-5.html#train-a-model",
    "href": "posts/2021-06-03-ajtfb-chapter-5.html#train-a-model",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 5: Multiclass classification",
    "section": "Train a model",
    "text": "Train a model\n\n\n\n\n\n\nImportant\n\n\n\n“Once you think your data looks right, we generally recommend the next step should be using it to train a simple model” See bottom of p193 for why.\n\n\n\nDefine your loss function\nTo train a model we need a good loss function that will allow us to optimize the parameters of our model. For multiclassification tasks where we want to predict a single class/label, to go to is cross-entropy loss\nTo understand how this particular loss function operates and its interesting effects, see my prior article “Loss Functions: Cross Entropy Loss and You!” It’s all about how it works, why use it over something like accuracy, and so forth. Pages 194-203 is the place to look in fastbook for more details on the ins and outs of this loss function.\n\n\n\n\n\n\nImportant\n\n\n\nCross-entropy loss is the Highlander of loss functions … “there can only be one”\n\n\n“Intuitively, the softmax function really wants to pick one class … so it’s ideal for training a classifier when we know each piecture has a definite label. (Note taht it may be less ideal during inference, as you might want your model to sometimes tell you it doesn’t recognize any of the classes taht is has seen during training, and not pick a class because it has a slightly bigger activation score. In this case, it might be better to train a model using multiple binary output columns, each using a sigmoid activation.)”\n\n\nTrain a model\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\n\n\n\n\n\n\n\nImportant\n\n\n\nUse the Learning Rate Finder to determine a good LR to use during the optimization step!\n\n\n“One of the most importatn things we can do when training a model is to make sure that we have the right learning rate. If our learning rate is too low, it can take many, many epochs to train our model … also that we may have problems with overfitting, ceacuse every time we do a complete pass through the data, we give our model a chance to memorize it”\nSee p 205-206 for more information on how it works, and also Leslie Smith’s paper on it here. A must read for fastai developers!!!\n\nlr_min, lr_steep = learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\n\nlr_min, lr_steep\n\n(0.010000000149011612, 0.0063095735386013985)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPick either “one order of magnitude less that where the minimum loss was achieved” (lr_min above, which is actually the true minimum, roughly 1e-1 / 10, 0.01 or else 1e-2) -or- “the last point where the loss was clearly decreasing”\n\n\nThese two are likely close to one another, and if you’re not sure which to use, try them both!\n\nlearn.fine_tune(2, base_lr=1e-2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.988194\n0.477657\n0.131258\n01:05\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.990748\n0.932280\n0.228687\n01:09\n\n\n1\n0.537608\n0.289773\n0.089986\n01:07\n\n\n\n\n\nUsing fine_tune gives us a nice and quick baseline we can look back at going forward. Nevertheless, we can likely improve our model by taking more control over what parameters are trained (updated), when, and by how much using fit_one_cycle.\nSo let’s start again, by defining our Learner and finding a good LR for training ONLY the last layer’s parameters (the idea being that the pretrained model we’re finetuning, our backbone, is already pretty good at understanding images … while the last layer’s parameters are random because they are specific to our task at hand).\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlr_min, lr_steep = learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\n\nlearn.fit_one_cycle(2, lr_max=1e-2)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.094374\n0.674852\n0.180650\n01:05\n\n\n1\n0.645389\n0.303023\n0.096752\n01:03\n\n\n\n\n\nNOW … we’re going to “unfreeze” our model, meaning we’re going to make all the parameters trainable. And then we’re going to apply discriminative learning rates, or different base LRs to different parts of our models, with the assumption that earlier layers likely only need to change a little while later layers, especially our classification head, have to change more. This is covered thoroughly in another must read paper, Universal Language Model Fine-tuning for Text Classification\nLook at the bottom of the cell below’s output to see the number of traininable parameters for our currently frozen model.\n\nlearn.summary()\n\n\n\n\nSequential (Input shape: 64)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     64 x 64 x 112 x 112 \nConv2d                                    9408       False     \nBatchNorm2d                               128        True      \nReLU                                                           \nMaxPool2d                                                      \nConv2d                                    36864      False     \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      False     \nBatchNorm2d                               128        True      \nConv2d                                    36864      False     \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      False     \nBatchNorm2d                               128        True      \nConv2d                                    36864      False     \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      False     \nBatchNorm2d                               128        True      \n____________________________________________________________________________\n                     64 x 128 x 28 x 28  \nConv2d                                    73728      False     \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     False     \nBatchNorm2d                               256        True      \nConv2d                                    8192       False     \nBatchNorm2d                               256        True      \nConv2d                                    147456     False     \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     False     \nBatchNorm2d                               256        True      \nConv2d                                    147456     False     \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     False     \nBatchNorm2d                               256        True      \nConv2d                                    147456     False     \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     False     \nBatchNorm2d                               256        True      \n____________________________________________________________________________\n                     64 x 256 x 14 x 14  \nConv2d                                    294912     False     \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     False     \nBatchNorm2d                               512        True      \nConv2d                                    32768      False     \nBatchNorm2d                               512        True      \nConv2d                                    589824     False     \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     False     \nBatchNorm2d                               512        True      \nConv2d                                    589824     False     \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     False     \nBatchNorm2d                               512        True      \nConv2d                                    589824     False     \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     False     \nBatchNorm2d                               512        True      \nConv2d                                    589824     False     \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     False     \nBatchNorm2d                               512        True      \nConv2d                                    589824     False     \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     False     \nBatchNorm2d                               512        True      \n____________________________________________________________________________\n                     64 x 512 x 7 x 7    \nConv2d                                    1179648    False     \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    False     \nBatchNorm2d                               1024       True      \nConv2d                                    131072     False     \nBatchNorm2d                               1024       True      \nConv2d                                    2359296    False     \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    False     \nBatchNorm2d                               1024       True      \nConv2d                                    2359296    False     \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    False     \nBatchNorm2d                               1024       True      \n____________________________________________________________________________\n                     []                  \nAdaptiveAvgPool2d                                              \nAdaptiveMaxPool2d                                              \nFlatten                                                        \nBatchNorm1d                               2048       True      \nDropout                                                        \n____________________________________________________________________________\n                     64 x 512            \nLinear                                    524288     True      \nReLU                                                           \nBatchNorm1d                               1024       True      \nDropout                                                        \n____________________________________________________________________________\n                     64 x 37             \nLinear                                    18944      True      \n____________________________________________________________________________\n\nTotal params: 21,830,976\nTotal trainable params: 563,328\nTotal non-trainable params: 21,267,648\n\nOptimizer used: &lt;function Adam at 0x7fe0e7fb57a0&gt;\nLoss function: FlattenedLoss of CrossEntropyLoss()\n\nModel frozen up to parameter group #2\n\nCallbacks:\n  - TrainEvalCallback\n  - Recorder\n  - ProgressCallback\n\n\nLet’s unfreeze and look at the same …\n\nlearn.unfreeze()\nlearn.summary()\n\n\n\n\nSequential (Input shape: 64)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     64 x 64 x 112 x 112 \nConv2d                                    9408       True      \nBatchNorm2d                               128        True      \nReLU                                                           \nMaxPool2d                                                      \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \n____________________________________________________________________________\n                     64 x 128 x 28 x 28  \nConv2d                                    73728      True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nConv2d                                    8192       True      \nBatchNorm2d                               256        True      \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \n____________________________________________________________________________\n                     64 x 256 x 14 x 14  \nConv2d                                    294912     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    32768      True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \n____________________________________________________________________________\n                     64 x 512 x 7 x 7    \nConv2d                                    1179648    True      \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nConv2d                                    131072     True      \nBatchNorm2d                               1024       True      \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \n____________________________________________________________________________\n                     []                  \nAdaptiveAvgPool2d                                              \nAdaptiveMaxPool2d                                              \nFlatten                                                        \nBatchNorm1d                               2048       True      \nDropout                                                        \n____________________________________________________________________________\n                     64 x 512            \nLinear                                    524288     True      \nReLU                                                           \nBatchNorm1d                               1024       True      \nDropout                                                        \n____________________________________________________________________________\n                     64 x 37             \nLinear                                    18944      True      \n____________________________________________________________________________\n\nTotal params: 21,830,976\nTotal trainable params: 21,830,976\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function Adam at 0x7fe0e7fb57a0&gt;\nLoss function: FlattenedLoss of CrossEntropyLoss()\n\nModel unfrozen\n\nCallbacks:\n  - TrainEvalCallback\n  - Recorder\n  - ProgressCallback\n\n\n… and as you can see, we’re training everything!\nBecause what parameters were training has changed, we also need to run the LR finder again to get some guidance on how to set our LRs.\n\nlr_min, lr_steep = learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\nSo lets see how many parameter/layer groups we have\n\nlen(learn.opt.param_groups)\n\n3\n\n\nWhat we can now do is say, train the first layer group with an LR of 1e-6 … the last with an LR of 1e-4, and “the layers in between will have learning rates that are multiplicatively equidistnat throughout that range.” Since we only have 3, the middle group will be trained with a starting LR of 1e-5\n\n\n\n\n\n\nImportant\n\n\n\nWe usually choose the first number a bit back from where things start to go bad … and the last number 1-2 magnitudes lower than the base LR of the frozen model\n\n\n\nlearn.fit_one_cycle(12, lr_max=slice(1e-6, 1e-4))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.394857\n0.291812\n0.091340\n01:06\n\n\n1\n0.359788\n0.266810\n0.084574\n01:05\n\n\n2\n0.315511\n0.248843\n0.078484\n01:05\n\n\n3\n0.308508\n0.245649\n0.081191\n01:06\n\n\n4\n0.272639\n0.231762\n0.077131\n01:06\n\n\n5\n0.231295\n0.222235\n0.075778\n01:06\n\n\n6\n0.216695\n0.223222\n0.077808\n01:06\n\n\n7\n0.207144\n0.226977\n0.075778\n01:07\n\n\n8\n0.191794\n0.223768\n0.075778\n01:07\n\n\n9\n0.193688\n0.222243\n0.076455\n01:07\n\n\n10\n0.185064\n0.219272\n0.076455\n01:07\n\n\n11\n0.177980\n0.221938\n0.073072\n01:07\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOverfitting does not mean your model isn’t improving!\n\n\n“You will often see that the accuracy continues improving, even as the validation loss gets worse. In the end, what matters is your accuracy [or your chosen metric], not the loss. The loss is just the function we’ve given the computer to help us to optimize”\nHow to choose the number of epochs?\n\n\n\n\n\n\nImportant\n\n\n\n“Your first approach to training should be to simply pick a number of epochs that will train in the amount of time that you are happy to wait for.”\n\n\nIf the model is still getting better, then you haven’t trained your model long enough.\nIf your metric(s) are getting worse, “if you find that you have overfit, what you should do is retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found.”\nWhen to choose a deeper architecture?\n“A larger (more layers and parameters; sometimes described as the capacity of a model) version of ResNet will always be able to give us a better training loss, but it can suffer more from overfitting, because it has more parameters to overfit with. In general, a bigger model has the ability to better capture the real underlying relationships in your data, as well as to capture and memorize the specific details of your individual images.”\nSo consider these if … 1. You aren’t getting the results you need. 2. Have time to experiment and a big enough GPU to experiment with\nYou may need to reduce the size of your batches with these bigger models, and you can also us mixed-precision training, in order to get things to run on your GPU. The later results in faster training and gives you the ability to have bigger batch sizes than you would be able to support otherwise. All you need to do is add to_fp16() to your `Learner.\n\nlearn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=1)\n\nDownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.977287\n0.299639\n0.100812\n01:06\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.371637\n0.253767\n0.080514\n01:07\n\n\n1\n0.375325\n0.300937\n0.079161\n01:07\n\n\n2\n0.281241\n0.309774\n0.089310\n01:07\n\n\n3\n0.149089\n0.216163\n0.059540\n01:07\n\n\n4\n0.093488\n0.176675\n0.054127\n01:07\n\n\n5\n0.063744\n0.169187\n0.050744\n01:07"
  },
  {
    "objectID": "posts/2021-06-03-ajtfb-chapter-5.html#summary",
    "href": "posts/2021-06-03-ajtfb-chapter-5.html#summary",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 5: Multiclass classification",
    "section": "Summary",
    "text": "Summary\nWell, at this point, you know how to train a multiclassification computer vision task. So, go train one for yourself using what you’ve learned!\nMake sure you understand and why we use cross-entropy loss for multiclassification problems. I can’t tell you how many times I’ve responded to questions about why someone’s model wasn’t training only to find out the reason was because they had the wrong loss function. Know it, love it, use it :)\nAlso, so much of what fastai incorporates from the LR finder and the fit_one_cycle, comes from Leslie Smith’s research. Checkout the “Resources” section below for some of his more influential papers, all of which I’ve read and encourage the rest of you to read if you want some inside scoop about why fastai works the way it does. I guarantee, reading and studying those papers will make you a better deep learning practioner and a better fastai developer in particular! You may think you’re not ready to start reading academic papers at chapter 5, but believe me, I’ve been there, and you are :)"
  },
  {
    "objectID": "posts/2021-06-03-ajtfb-chapter-5.html#resources",
    "href": "posts/2021-06-03-ajtfb-chapter-5.html#resources",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 5: Multiclass classification",
    "section": "Resources",
    "text": "Resources\n\nhttps://book.fast.ai - The book’s website; it’s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc…\nCyclical Learning Rates for Training Neural Networks\nSuper-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\nA disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay\nUniversal Language Model Fine-tuning for Text Classification\nfastai: A Layered API for Deep Learning"
  },
  {
    "objectID": "posts/2022-03-31-ajtfb-chapter-8.html",
    "href": "posts/2022-03-31-ajtfb-chapter-8.html",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering",
    "section": "",
    "text": "Other posts in this series:\nA Journey Through Fastbook (AJTFB) - Chapter 1\nA Journey Through Fastbook (AJTFB) - Chapter 2\nA Journey Through Fastbook (AJTFB) - Chapter 3\nA Journey Through Fastbook (AJTFB) - Chapter 4\nA Journey Through Fastbook (AJTFB) - Chapter 5\nA Journey Through Fastbook (AJTFB) - Chapter 6a\nA Journey Through Fastbook (AJTFB) - Chapter 6b\nA Journey Through Fastbook (AJTFB) - Chapter 7\nA Journey Through Fastbook (AJTFB) - Chapter 9"
  },
  {
    "objectID": "posts/2022-03-31-ajtfb-chapter-8.html#collaborative-filtering",
    "href": "posts/2022-03-31-ajtfb-chapter-8.html#collaborative-filtering",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering",
    "section": "Collaborative Filtering",
    "text": "Collaborative Filtering\nWhat is it?\nThink recommender systems which “look at which products the current user has used or liked, find other users who have used or liked similar products, and then recommend other products that those users have used or liked.”\nThe key to making collaborative filtering and tabular models, is the idea of latent factors."
  },
  {
    "objectID": "posts/2022-03-31-ajtfb-chapter-8.html#what-are-latent-factors-and-what-is-the-problem-they-solve",
    "href": "posts/2022-03-31-ajtfb-chapter-8.html#what-are-latent-factors-and-what-is-the-problem-they-solve",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering",
    "section": "What are “latent factors” and what is the problem they solve?",
    "text": "What are “latent factors” and what is the problem they solve?\nRemember that models can only work with numbers, and while something like “price” can be used to accurately reflect the value of a house, how do we represent numerically concepts like the day of week, the make/model of a car, or the job function of an employee?\nThe answer is with latent factors.\nIn a nutshell, latent factors are numbers associated to a thing (e.g., day of week, model of car, job function, etc…) that are learnt during model training. At the end this process, we have numbers that provide a representation of the thing we can use and explore in a variety of ways. These factors are called “latent” because we don’t know what they are beforehand.\n\n\n\n\n\n\nNote\n\n\n\nThe learnt numbers for “a thing” may vary to one degree or another based on the data used during training and your objective. For example, what “Sunday” means may be represented differently when you are trying to forecast how many bottle of scotch will be sold that day than if you were trying to predict the number of options that will be traded for a certain equity.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLatent factors allows us to learn a numerical representation of a thing (especially those for which a single number would not do it justice)\n\n\nIf we had something like this …\n\n… how could we predict what users would rate movies they have yet to see? Let’s take a look.\n\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\n\npath = untar_data(URLs.ML_100k)\n\n\n\n\n\n\n    \n      \n      100.15% [4931584/4924029 00:00&lt;00:00]\n    \n    \n\n\n\nratings_df = pd.read_csv(path/\"u.data\", delimiter=\"\\t\", header=None, names=[\"user\", \"movie\", \"rating\", \"timestamp\"])\nratings_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nuser\nmovie\nrating\ntimestamp\n\n\n\n\n0\n196\n242\n3\n881250949\n\n\n1\n186\n302\n3\n891717742\n\n\n2\n22\n377\n1\n878887116\n\n\n3\n244\n51\n2\n880606923\n\n\n4\n166\n346\n1\n886397596\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nHow do we numerically represent user 196 and movie 242? With latent factors we don’t have to know, we can have such a representation learnt using SGD.\nHow do we set this up?\n\n“… randomly initialized some parameters [which] will be a set of latent factors for each user and movie.”\n“… to calculate our predictions [take] the dot product of each movie with each user.\n“… to calculate our loss … let’s pick mean squared error for now, since that is one reasonable way to represent the accuracy of a prediction”\n\n\n\n\n\n\n\nNote\n\n\n\ndot product = element-wise multiplication of two vectors summed up.\n\n\nWith this in place, “we can optimize our parameters (the latent factors) using stochastic gradient descent, such as to minimize the loss.” In a picture, it looks like this …\n\n\n\n\n\n\n\nImportant\n\n\n\nThe parameters we want to optimize are the latent factors!\n\n\n\nmovies_df = pd.read_csv(path/\"u.item\", delimiter=\"|\", header=None, names=[\"movie\", \"title\"], usecols=(0,1), encoding=\"latin-1\")\nmovies_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nmovie\ntitle\n\n\n\n\n0\n1\nToy Story (1995)\n\n\n1\n2\nGoldenEye (1995)\n\n\n2\n3\nFour Rooms (1995)\n\n\n3\n4\nGet Shorty (1995)\n\n\n4\n5\nCopycat (1995)\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nratings_df = ratings_df.merge(movies_df)\nratings_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nuser\nmovie\nrating\ntimestamp\ntitle\n\n\n\n\n0\n196\n242\n3\n881250949\nKolya (1996)\n\n\n1\n63\n242\n3\n875747190\nKolya (1996)\n\n\n2\n226\n242\n5\n883888671\nKolya (1996)\n\n\n3\n154\n242\n3\n879138235\nKolya (1996)\n\n\n4\n306\n242\n5\n876503793\nKolya (1996)\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndls = CollabDataLoaders.from_df(ratings_df, item_name=\"title\", user_name=\"user\", rating_name=\"rating\")\ndls.show_batch()\n\n\n\n\n\nuser\ntitle\nrating\n\n\n\n\n0\n647\nMen in Black (1997)\n2\n\n\n1\n823\nTwelve Monkeys (1995)\n5\n\n\n2\n894\nTwelve Monkeys (1995)\n4\n\n\n3\n278\nIn & Out (1997)\n2\n\n\n4\n234\nPinocchio (1940)\n4\n\n\n5\n823\nPhenomenon (1996)\n4\n\n\n6\n268\nNell (1994)\n3\n\n\n7\n293\nMrs. Doubtfire (1993)\n3\n\n\n8\n699\nRock, The (1996)\n4\n\n\n9\n405\nBest of the Best 3: No Turning Back (1995)\n1\n\n\n\n\n\nSo how do we create these latent factors for our users and movies?\n“We can represent our movie and user latent factor tables as simple matrices” that we can index into. But as looking up in an index is not something our models know how to do, we need to use a special PyTorch layer that will do this for us (and more efficiently than using a one-hot-encoded, OHE, vector to do the same).\nAnd that layer is called an embedding. It “indexes into a vector using an integer, but has its derivative calcuated in such a way that it is identical to what it would have been if it had done a matric multiplication with a one-hot-encoded vector.”\n\n\n\n\n\n\nImportant\n\n\n\nAn embedding is the “thing that you multiply the one-hot-encoded matrix by (or, using the computational shortcut, inex into directly)”"
  },
  {
    "objectID": "posts/2022-03-31-ajtfb-chapter-8.html#collaborative-filtering-from-scratch-dot-product",
    "href": "posts/2022-03-31-ajtfb-chapter-8.html#collaborative-filtering-from-scratch-dot-product",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering",
    "section": "Collaborative Filtering: From Scratch (dot product)",
    "text": "Collaborative Filtering: From Scratch (dot product)\nA dot product approach\n\nn_users = len(dls.classes[\"user\"])\nn_movies = len(dls.classes[\"title\"])\nn_factors = 5\n\nprint(n_users, n_movies, n_factors)\n\n944 1665 5\n\n\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors):\n      super().__init__()\n      self.users_emb = Embedding(n_users, n_factors)\n      self.movies_emb = Embedding(n_movies, n_factors)\n\n  def forward(self, inp):\n    users = self.users_emb(inp[:,0])\n    movies = self.movies_emb(inp[:,1])\n    return (users * movies).sum(dim=1)\n\n\nmodel = DotProduct(n_users=n_users, n_movies=n_movies, n_factors=n_factors)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n4.443892\n3.739447\n00:17\n\n\n1\n1.088801\n1.125219\n00:12\n\n\n2\n0.960021\n0.994215\n00:09\n\n\n3\n0.917181\n0.959309\n00:09\n\n\n4\n0.903602\n0.957268\n00:09\n\n\n\n\n\n\nTip 1: Constrain your range of predictions using sigmoid_range\n“… to make this model a little bit better … force those predictions to be between 0 and 5. One thing we discovered empirically is that it’s better to have the range go a little bit over 5, so we use (0, 5.5)”\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0, 5.5)):\n      super().__init__()\n      self.users_emb = Embedding(n_users, n_factors)\n      self.movies_emb = Embedding(n_movies, n_factors)\n      self.y_range = y_range\n\n  def forward(self, inp):\n    users = self.users_emb(inp[:,0])\n    movies = self.movies_emb(inp[:,1])\n    return sigmoid_range((users * movies).sum(dim=1), *self.y_range)\n\n\nmodel = DotProduct(n_users=n_users, n_movies=n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n1.012296\n0.994128\n00:09\n\n\n1\n0.855805\n0.903300\n00:09\n\n\n2\n0.707656\n0.875159\n00:09\n\n\n3\n0.482094\n0.879115\n00:09\n\n\n4\n0.369521\n0.884585\n00:09\n\n\n\n\n\n\n\nTip 2: Add a “bias”\n\n\n\n\n\n\nImportant\n\n\n\nA bias allows your model to learn an overall representation of a thing, rather than just a bunch of characteristics.\n\n\n“One obvious missing piece is that some users are just more positive or negative in their recommendations than others, and some movies are just plain better or worse than others. But in our dot product representation, we do not have any way to encode either of these things … **because at this point we have only weights; we don’t have biases”\n\nclass DotProduct(Module):\n  def __init__(self, n_users, n_movies, n_factors, y_range=(0, 5.5)):\n      super().__init__()\n      self.users_emb = Embedding(n_users, n_factors)\n      self.users_bias = Embedding(n_users, 1)\n\n      self.movies_emb = Embedding(n_movies, n_factors)\n      self.movies_bias = Embedding(n_movies, 1)\n\n      self.y_range = y_range\n\n  def forward(self, inp):\n    # embeddings\n    users = self.users_emb(inp[:,0])\n    movies = self.movies_emb(inp[:,1])\n\n    # calc our dot product and add in biases \n    # (important to include \"keepdim=True\" =&gt; res.shape = (64,1), else will get rid of dims equal to 1 and you just get (64))\n    res = (users * movies).sum(dim=1, keepdim=True)\n    res += self.users_bias(inp[:,0]) + self.movies_bias(inp[:,1])\n\n    # return our target constrained prediction\n    return sigmoid_range(res, *self.y_range)\n\n\nmodel = DotProduct(n_users=n_users, n_movies=n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nlearn.fit_one_cycle(5, 5e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.925160\n0.938387\n00:10\n\n\n1\n0.807021\n0.863466\n00:11\n\n\n2\n0.607999\n0.871146\n00:10\n\n\n3\n0.412089\n0.897263\n00:10\n\n\n4\n0.294376\n0.905192\n00:10\n\n\n\n\n\n\n\nTip 3: Add “weight decay”\nAdding in bias has made are model more complex and therefore more prone to overfitting (which seems to be happening here).\n\n\n\n\n\n\nNote\n\n\n\nOverfitting is where your validation stops improving and actually starts to get worse.\n\n\nWhat do you do when your model overfits?\nWe can solve this via data augmentation or by including one or more forms of regularization (e.g., a means to “encourage the weights to be as small as possible”.\nWhat is “weight decay” (aka “L2 regularization”)?\n“… consists of adding to your loss function the sum of all the weights squared.”\nWhy do that?\n“Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.”\nWhy would this prevent overfitting?\n“The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function…. Letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting.\n\n\n\n\n\n\nImportant\n\n\n\n“Limiting our weights from growing too much is going to hinder the training of the model but it will yield a state where it generalizes better”\n\n\nHow do we add weight decay into are training?\n“… wd is a parameter that **controls that sum of squares we add to our loss” as such:\n\nmodel = DotProduct(n_users=n_users, n_movies=n_movies, n_factors=50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.934345\n0.946092\n00:10\n\n\n1\n0.862484\n0.866700\n00:10\n\n\n2\n0.715633\n0.829172\n00:10\n\n\n3\n0.597503\n0.817248\n00:10\n\n\n4\n0.473108\n0.817725\n00:10\n\n\n\n\n\n\n\nCreating our own Embedding Module\npp.265-267 show how to write your own nn.Module that does what Embedding does. Here are some of the important bits to pay attention too …\n“… optimizers require that they can get all the parameters of a module from the module’s parameters method, so make sure to tell nn.Module that you want to treat a tensor as a parameters using the nn.Parameter class like so:\nclass T(Module):\n  def __init__(self):\n    self.a = nn.Parameter(torch.ones(3))\n\n\n\n\n\n\nImportant\n\n\n\n“All PyTorch modules use nn.Parameter for any trainable parameters.\n\n\nclass T(Module):\n  def __init__(self):\n    self.a = nn.Liner(1, 3, bias=False)\n\nt = T()\nt.parameters()   #=&gt; will show all the weights of your nn.Linear\ntype(t.a.weight) #=&gt; torch.nn.parameter.Parameter\nNow, given a method like this …\ndef create_params(size):\n  return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n… we can create randomly initialized parameters, included parameters for our latent factors and biases like this:\nself.users_emb = create_params([n_users, n_factors])\nself.users_bias = create_params([n_users])"
  },
  {
    "objectID": "posts/2022-03-31-ajtfb-chapter-8.html#interpreting-embeddings-and-biases",
    "href": "posts/2022-03-31-ajtfb-chapter-8.html#interpreting-embeddings-and-biases",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering",
    "section": "Interpreting Embeddings and Biases",
    "text": "Interpreting Embeddings and Biases\n\n\n\n\n\n\nNote\n\n\n\n“… interesting to see what parameters it has discovered … easiest to interpret are the biases”\n\n\n\nmovie_bias = learn.model.movies_bias.weight.squeeze() # =&gt; squeeze will get rid of all the single dimensions\nidxs = movie_bias.argsort()[:5]                       # =&gt; \"argsort()\" returns the indices sorted by value\n[dls.classes[\"title\"][i] for i in idxs]               # =&gt; look up the movie title in dls.classes\n\n['Children of the Corn: The Gathering (1996)',\n 'Big Bully (1996)',\n 'Showgirls (1995)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n 'Free Willy 3: The Rescue (1997)']\n\n\n“Think about what this means …. It tells us not just whether a movie is of a kind that people tend not to enjoy watching, but that people tend to not like watching it even if it is of a kind that they would otherwise enjoy!”\nTo get the movies by highest bias:\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes[\"title\"][i] for i in idxs]\n\n['Titanic (1997)',\n \"Schindler's List (1993)\",\n 'As Good As It Gets (1997)',\n 'L.A. Confidential (1997)',\n 'Shawshank Redemption, The (1994)']\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo visualize embeddings with many factors, you “can pull out the most important underlying directions” using a dimensionality reduction model like principal components analysis (PCA).\n\n\nSee p.268 and these three StatQuest videos for more on how PCA works (btw, StatQuest is one of my top data science references so consider subscribing to his channel). Video 1, Video 2, and Video 3"
  },
  {
    "objectID": "posts/2022-03-31-ajtfb-chapter-8.html#collaborative-filtering-using-fastai.collab",
    "href": "posts/2022-03-31-ajtfb-chapter-8.html#collaborative-filtering-using-fastai.collab",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering",
    "section": "Collaborative Filtering: Using fastai.collab",
    "text": "Collaborative Filtering: Using fastai.collab\n\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n0.950606\n0.930099\n00:10\n\n\n1\n0.834664\n0.870282\n00:10\n\n\n2\n0.723968\n0.833274\n00:10\n\n\n3\n0.573679\n0.819824\n00:10\n\n\n4\n0.489258\n0.820394\n00:10\n\n\n\n\n\n\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\n\nmovie_bias = learn.model.i_bias.weight.squeeze() \nidxs = movie_bias.argsort()[:5]                   \n[dls.classes[\"title\"][i] for i in idxs]              \n\n['Children of the Corn: The Gathering (1996)',\n 'Showgirls (1995)',\n 'Barb Wire (1996)',\n 'Island of Dr. Moreau, The (1996)',\n 'Cable Guy, The (1996)']"
  },
  {
    "objectID": "posts/2022-03-31-ajtfb-chapter-8.html#embedding-distance",
    "href": "posts/2022-03-31-ajtfb-chapter-8.html#embedding-distance",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering",
    "section": "Embedding Distance",
    "text": "Embedding Distance\n“Another thing we can do with these learned embeddings is to look at distance.”\nWhy do this?\n“If there were two movies that were nearly identical, their embedding vectors would also have to be nearly identical …. There is a more general idea here: movie similairty can be defined by the similarity of users who like those movies. And that directly means that the distance between two movies’ embedding vectors can define that similarity”\n\nmovie_factors = learn.model.i_weight.weight\nidx = dls.classes[\"title\"].o2i[\"Silence of the Lambs, The (1991)\"]\ndists = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\ntarg_idx = dists.argsort(descending=True)[1]\ndls.classes[\"title\"][targ_idx]\n\n'Dial M for Murder (1954)'"
  },
  {
    "objectID": "posts/2022-03-31-ajtfb-chapter-8.html#bootstrapping",
    "href": "posts/2022-03-31-ajtfb-chapter-8.html#bootstrapping",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nThe bootstrapping problem asks how we can make recommendations when we have a new user for which no data exists or a new product/movie for which no reviews have been made?\nThe recommended approach “is to use a tabular model based on user metadata to construct your initial embedding vector. When a new user signs up, think about what questions you could ask to help you understand their tastes. Then you can create a model in which the dependent variable is a user’s embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup metadata.”\n\n\n\n\n\n\nImportant\n\n\n\nBe aware of the “problem of representation bias” (e.g., where a few very active users end up skewing the results).\n\n\nSee p.271 for more information on how collaborative models may contribute to positive feedback loops and how humans can mitigate by being part of the process."
  },
  {
    "objectID": "posts/2022-03-31-ajtfb-chapter-8.html#collaborative-filtering-from-scratch-nn",
    "href": "posts/2022-03-31-ajtfb-chapter-8.html#collaborative-filtering-from-scratch-nn",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering",
    "section": "Collaborative Filtering: From Scratch (NN)",
    "text": "Collaborative Filtering: From Scratch (NN)\nA neural network approach requires we “take the results of the embedding lookup and concatenate those activations together. This gives us a matrix we can then pass through linear layers and nonlinearities…”\n\n\n\n\n\n\nNote\n\n\n\nBecause “we’ll be concatenating the embedding matrices, rather than taking their dot product, the two embedding matrices can have different sizes (different numbers of latent factors)”\n\n\nHow do we determine the number of latent factors a “thing” should have?\nUse get_emb_sz to return “the recommended sizes for embedding matrices for your data, **based on a heuristic that fast.ai has found tends to work well in practice”\n\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\n\nclass CollabNN(Module):\n  def __init__(self, user_sz, item_sz, y_range=(0, 0.5), n_act=100):\n    self.user_factors = Embedding(*user_sz)\n    self.item_factors = Embedding(*item_sz)\n    self.layers = nn.Sequential(\n      nn.Linear(user_sz[1] + item_sz[1], n_act),\n      nn.ReLU(),\n      nn.Linear(n_act, 1)\n    )\n    self.y_range = y_range\n\n  def forward(self, x):\n    embs = self.user_factors(x[:,0]), self.item_factors(x[:,1])\n    x = self.layers(torch.cat(embs, dim=1))\n    return sigmoid_range(x, *self.y_range)\n\n\nmodel = CollabNN(*embs)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n10.594646\n10.379806\n00:11\n\n\n1\n10.368298\n10.379800\n00:10\n\n\n2\n10.565783\n10.379800\n00:10\n\n\n3\n10.439667\n10.379800\n00:10\n\n\n4\n10.356900\n10.379800\n00:10\n\n\n\n\n\nIf we use the collab_learner, will will calculate our embedding sizes for us and also give us the option of defining how many more layers we want to tack on via the layers parameter. All we have to do is tell it to use_nn=True to use a NN rather than the default dot-product model.\n\nlearn = collab_learner(dls, use_nn=True, y_range=(0,0.5), layers=[100,50])\n\n\nlearn.model\n\nEmbeddingNN(\n  (embeds): ModuleList(\n    (0): Embedding(944, 74)\n    (1): Embedding(1665, 102)\n  )\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (bn_cont): BatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=176, out_features=100, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=100, out_features=50, bias=False)\n      (1): ReLU(inplace=True)\n      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): LinBnDrop(\n      (0): Linear(in_features=50, out_features=1, bias=True)\n    )\n    (3): SigmoidRange(low=0, high=0.5)\n  )\n)\n\n\n\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n10.274985\n10.379872\n00:12\n\n\n1\n10.555515\n10.379800\n00:12\n\n\n2\n10.436097\n10.379800\n00:12\n\n\n3\n10.481320\n10.379800\n00:12\n\n\n4\n10.400410\n10.379800\n00:12\n\n\n\n\n\nWhy use a neural network (NN)?\nBecause “we can now directly incorporate other user and movie information, date and time information, or any other information that may be relevant to the recommendation.”\nWe’ll see this when we look at TabularModel (of which EmbeddingNN is a subclass with no continuous data [n_cont=0] and an out_sz=1."
  },
  {
    "objectID": "posts/2022-03-31-ajtfb-chapter-8.html#kwargs-and-delegates",
    "href": "posts/2022-03-31-ajtfb-chapter-8.html#kwargs-and-delegates",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering",
    "section": "kwargs and @delegates",
    "text": "kwargs and @delegates\nSome helpful notes for both are included on pp.273-274. In short …\n**kwargs:\n\n**kwargs as a parameter = “put any additional keyword arguments into a dict called kwargs”\n**kwargs passed as an argument = “insert all key/value pairs in the kwargs dict as named arguments here.”\n\n@delegates:\n“… fastai resolves [the issue of using **kwargs to avoid having to write out all the arguments of the base class] by providing a special @delegates decorator, which automatically changes the signature of the class or function … to insert all of its keyword arguments into the signature.”"
  },
  {
    "objectID": "posts/2022-03-31-ajtfb-chapter-8.html#resources",
    "href": "posts/2022-03-31-ajtfb-chapter-8.html#resources",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 8: Collaborative Filtering",
    "section": "Resources",
    "text": "Resources\n\nhttps://book.fast.ai - The book’s website; it’s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc…"
  },
  {
    "objectID": "posts/1999-01-01-understanding-fbeta.html",
    "href": "posts/1999-01-01-understanding-fbeta.html",
    "title": "Understanding the F-Beta metric",
    "section": "",
    "text": "scikit-learn describes the F-Beta score “as the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0” with the “beta parameter [determining] the weight of recall in the combined score.” It is one of the most common metrics enlisted in demonstrating the performance of binary, multi-classification, and multi-label classifiers.\nSo what does all that mean?\nIn a nutshell, it says that this metric can be used to help you understand how good your classification model is based on the relative importance you ascribe to precision and recall in making that determination. Common beta values are 0.5 (precision is king), 1 (precision and recall are equally important), and 2 (recall is king).\nWhen you look at the documentation, you’ll notice there are several other interesting arguments you can pass into it. Two of the more mysterious ones being average and sample_weight. Will explore what they mean how you may want to use them based on your dataset.\nThe two metrics, along with other important terms, are described well in this post. Let’s imagine a multi-classification model that tries to determine whether a photo show a picture of a dog, cat, or bird."
  },
  {
    "objectID": "posts/1999-01-01-understanding-fbeta.html#overview",
    "href": "posts/1999-01-01-understanding-fbeta.html#overview",
    "title": "Understanding the F-Beta metric",
    "section": "",
    "text": "scikit-learn describes the F-Beta score “as the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0” with the “beta parameter [determining] the weight of recall in the combined score.” It is one of the most common metrics enlisted in demonstrating the performance of binary, multi-classification, and multi-label classifiers.\nSo what does all that mean?\nIn a nutshell, it says that this metric can be used to help you understand how good your classification model is based on the relative importance you ascribe to precision and recall in making that determination. Common beta values are 0.5 (precision is king), 1 (precision and recall are equally important), and 2 (recall is king).\nWhen you look at the documentation, you’ll notice there are several other interesting arguments you can pass into it. Two of the more mysterious ones being average and sample_weight. Will explore what they mean how you may want to use them based on your dataset.\nThe two metrics, along with other important terms, are described well in this post. Let’s imagine a multi-classification model that tries to determine whether a photo show a picture of a dog, cat, or bird."
  },
  {
    "objectID": "posts/1999-01-01-understanding-fbeta.html#precision-vs.-recall",
    "href": "posts/1999-01-01-understanding-fbeta.html#precision-vs.-recall",
    "title": "Understanding the F-Beta metric",
    "section": "Precision vs. Recall",
    "text": "Precision vs. Recall\nThe two metrics, along with other important terms, are described really well in this post. Let’s imagine a multi-classification model that tries to determine whether a given photo is a picture of a dog, cat, or bird.\n\nPrecision\nDefinition: When your classifier predicted a label, how often was it correct?\nExample: When you predicted ‘cat’, how often were you right?\nFormula: True Positive (TP) / PREDICTED Label (TP + False Positive or FP)\n# TP = number of cat prediction you got right\ntp = 100\n# FP = number of cat predictions you got wrong\nfp = 10\nprecision = tp / (tp + fp)\n# = 0.91\n\n\nRecall\nDefinition: For every actual label in your dataset, how often did your classifier pick the correct one?\nExample: When it’s actually ‘cat’, how often did it predict ‘cat’?\nFormula: True Positive (TP) / ACTUAL Label (TP + False Negative or FN)\n# TP = number of cat prediction you got right\ntp = 100\n# FN = number of actual cats you predicted as something else\nfn = 5\nrecall = tp / (tp + fn)\n# = 0.95\n\n\nOkay, so which one should I use?\nThis depends on your task.\nIf you’re task is to predict whether a patient has cancer given set of symptoms and test results, it’s going to be far more important to you that all actual cancer patients get flagged even at the expense of non-cancer patients being flagged incorrectly. This is recall. In this particular kind of task, you’re also likely going to be facing a dataset were the vast majority of examples are “not cancer.” A case where using metrics like precision and accuracy will likely look really good but be completely misleading. Other examples where you want to maximize recall include fraud and network anomaly detection.\nOn the otherhand, if you’re task is to predict whether an e-mail is spam or not (1=spam|0=not spam), you recognize that it’s not the end of the world if your user gets a junk e-mail. If fact, it would be worse if an actual e-mail got flagged as junk and they didn’t see it. Getting it wrong is more acceptable than making sure all the true cases are gotten right. This is precision. Here, you’re more concerned about your classifiers overall predictive capability in coming up with the right answer, yes or no.\nWhat about our cats, dogs, birds?\nGood question, again it depends on the task. All things be equal, most likely we care more about precision or we care about both equally in this case. Fortunately, the F-Beta metric gives us the power to determine the worth of our model regardless of how we want to weight the two."
  },
  {
    "objectID": "posts/2022-03-28-ajtfb-chapter-7.html",
    "href": "posts/2022-03-28-ajtfb-chapter-7.html",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models",
    "section": "",
    "text": "Other posts in this series:\nA Journey Through Fastbook (AJTFB) - Chapter 1\nA Journey Through Fastbook (AJTFB) - Chapter 2\nA Journey Through Fastbook (AJTFB) - Chapter 3\nA Journey Through Fastbook (AJTFB) - Chapter 4\nA Journey Through Fastbook (AJTFB) - Chapter 5\nA Journey Through Fastbook (AJTFB) - Chapter 6a\nA Journey Through Fastbook (AJTFB) - Chapter 6b\nA Journey Through Fastbook (AJTFB) - Chapter 8\nA Journey Through Fastbook (AJTFB) - Chapter 9"
  },
  {
    "objectID": "posts/2022-03-28-ajtfb-chapter-7.html#imagenette",
    "href": "posts/2022-03-28-ajtfb-chapter-7.html#imagenette",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models",
    "section": "Imagenette",
    "text": "Imagenette\nThe Imagenette is a subset of the ImageNet dataset that “contains a subset of 10 very different categories from the orginal ImageNet dataset, making for quicker training when we want to experiment”\n\n\n\n\n\n\nTip\n\n\n\nStart with small datasets and models for initial experimentation and prototyping. Both will allow you to iterate over your experiments more quickly and verify your code works from beginning to end without having to wait hours for your training/validation loops to finish. “You should aim to have an iteration speed of no more than a couple of minutes …. If it’s taking longer to do an experiment, think about how you could cut down your dataset, or simply your model, to improve your experimentation speed.”\n\n\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.IMAGENETTE)"
  },
  {
    "objectID": "posts/2022-03-28-ajtfb-chapter-7.html#tip-1-use-the-presizing-trick",
    "href": "posts/2022-03-28-ajtfb-chapter-7.html#tip-1-use-the-presizing-trick",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models",
    "section": "Tip 1: Use the “presizing trick”",
    "text": "Tip 1: Use the “presizing trick”\nSee chapter 5, pp.189-191. The idea here is to first crop the image so that further augmentations can be applied without creating empty space (via item_tfms), with further augmentations being applied on the GPU on batches of images for speed (via batch_tfms).\nOn the training set, the initial crop area is chosen randomly with the size set to cover the entire width/height of the image with random crop and other augmentations done on the GPU.\nOn the validation set, a center square is always used in the first step and only a resize is applied on the GPU to get the image width/height equal to the final size needed.\n\ndblock = DataBlock(\n    blocks=(ImageBlock(), CategoryBlock()),\n    get_items = get_image_files,\n    get_y = parent_label,\n    item_tfms = Resize(460),\n    batch_tfms = aug_transforms(size=224, min_scale=0.75)\n)\n\ndls = dblock.dataloaders(path, bs=64)"
  },
  {
    "objectID": "posts/2022-03-28-ajtfb-chapter-7.html#tip-2-create-a-baseline",
    "href": "posts/2022-03-28-ajtfb-chapter-7.html#tip-2-create-a-baseline",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models",
    "section": "Tip 2: Create a “baseline”",
    "text": "Tip 2: Create a “baseline”\n\n\n\n\n\n\nNote\n\n\n\nwe are not using a pretrained model here, we are training one from scratch.\n\n\n\nmodel = xresnet50()\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.654316\n4.794844\n0.320015\n05:21\n\n\n1\n1.217274\n1.211676\n0.612024\n05:19\n\n\n2\n0.964628\n1.417025\n0.617252\n05:06\n\n\n3\n0.736836\n0.677910\n0.787155\n05:12\n\n\n4\n0.596578\n0.539180\n0.833831\n05:05"
  },
  {
    "objectID": "posts/2022-03-28-ajtfb-chapter-7.html#tip-3-normalize-your-data",
    "href": "posts/2022-03-28-ajtfb-chapter-7.html#tip-3-normalize-your-data",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models",
    "section": "Tip 3: Normalize your data",
    "text": "Tip 3: Normalize your data\n\n\n\n\n\n\nTip\n\n\n\n“When training a model, it helps if your input data is normalized - this is, has a mean of 0 and a standard deviation of 1.””\n\n\nFor images we do this over each channel (the 1 dimension) but averaging over all axes with the exception of the channel axis. In fastai, we can utilize the Normalize transform to apply this a batch at a time.\n\n\n\n\n\n\nImportant\n\n\n\nIf we don’t tell this transform what mean/std to use, “fastai will automatically calculate them from a single batch of your data”\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf we are using ImageNet images, we can use imagenet_stats instead of calculating the mean/std ourselves).\n\n\n\n# an example of normalization calculated on a batch of images\n# (because we aren't using normalization yet, you'll see the mean and standard deviation are not very close to\n# 0 and 1 respectively)\nx, y = dls.one_batch()\n\nx.mean(dim=[0,2,3]), x.std(dim=[0,2,3])\n\n(TensorImage([0.4518, 0.4554, 0.4344], device='cuda:0'),\n TensorImage([0.2868, 0.2783, 0.2998], device='cuda:0'))\n\n\n\ndef get_dls(batch_size, image_size):\n  dblock = DataBlock(\n      blocks=(ImageBlock(), CategoryBlock()),\n      get_items = get_image_files,\n      get_y = parent_label,\n      item_tfms = Resize(460),\n      batch_tfms = [*aug_transforms(size=image_size, min_scale=0.75), Normalize.from_stats(*imagenet_stats)]\n  )\n\n  dls = dblock.dataloaders(path, bs=batch_size)\n  return dls\n\n\ndls = get_dls(64, 224)\n\n# an example of normalization calculated on a batch of images\n# (because we are using normalization now, the mean and standard deviation are very close to 0 and 1 respectively)\nx, y = dls.one_batch()\nprint(x.mean(dim=[0,2,3]), x.std(dim=[0,2,3]))\n\n# does this normalization improve our model? Let's see ...\nmodel = xresnet50()\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\nTensorImage([-0.0816, -0.0114,  0.0695], device='cuda:0') TensorImage([1.1806, 1.1762, 1.2825], device='cuda:0')\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.701530\n1.856552\n0.468633\n05:07\n\n\n1\n1.280709\n1.384676\n0.573562\n05:05\n\n\n2\n1.007325\n1.073023\n0.656460\n05:06\n\n\n3\n0.762624\n0.666320\n0.784541\n05:06\n\n\n4\n0.606407\n0.573812\n0.823376\n05:02\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“… when you distribute a model, you need to also distribute the statistics used for normalization, since anyone using it for inference or transfer learning will need to use the same statistics …. If you’re using a model that someone else has trained, make sure you find out what normalization statistics they used and match them.”"
  },
  {
    "objectID": "posts/2022-03-28-ajtfb-chapter-7.html#tip-4-use-progressive-resizing",
    "href": "posts/2022-03-28-ajtfb-chapter-7.html#tip-4-use-progressive-resizing",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models",
    "section": "Tip 4: Use “progressive resizing”",
    "text": "Tip 4: Use “progressive resizing”\n“… start training using small images, and end training using large images. Spending most of the epochs training with small images helps training complete faster.”\n\n\n\n\n\n\nNote\n\n\n\nThink of this as a form of transfer learning\n\n\n“… the kinds of features that are learned by convolutional neural networks are not in any way specific to the size of the image …. So, when we change the image size in the middle of training, it doesn’t mean that we have to find totally different parameters for our model.”\n\n\n\n\n\n\nNote\n\n\n\n“Progressive resizing has an additional benefit: it is another form of data augmentation. Therefore, you should expect to see better generalization”\n\n\n\ndls = get_dls(128,128)\nlearn = Learner(dls, xresnet50(), loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(4, 3e-3)\n\n# simply replace the `Learner.dls` with new `DataLoaders` and continue traning.\nlearn.dls = get_dls(64, 224)\nlearn.fine_tune(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.848093\n1.582196\n0.526512\n03:02\n\n\n1\n1.297791\n1.205059\n0.616878\n03:01\n\n\n2\n0.985249\n1.022758\n0.690067\n02:55\n\n\n3\n0.762485\n0.688779\n0.787155\n02:53\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.845315\n1.171858\n0.650112\n05:08\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.635858\n0.834369\n0.751307\n05:06\n\n\n1\n0.664283\n0.665261\n0.796117\n05:10\n\n\n2\n0.585543\n0.634785\n0.796490\n05:11\n\n\n3\n0.478250\n0.495538\n0.840926\n05:02\n\n\n4\n0.429075\n0.448893\n0.855489\n05:08\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo use the DataLoaders with bigger images, we simply assign it to Learner.dls.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBigger images will require smaller batch sizes. Also, you will not get a benefit of using images sized larger than the size of your images on disk!\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“… for transfer learning, progressive resizing may acutally hurt performance …. This is most likely to happen if your pretrained model was quite similar to your transfer learning task and the dataset and was trained on similar-sized images, so the weights don’t need to be changed much. In that case, training on smaller images may damage the pretrained weights.\n\n\n“On the other hand, if the transfer learning task is going to use images that are of different sizes, shapes, or styles than those used in the pretraining task, progressive resizing will probably help”\n\n\n\n\n\n\nTip\n\n\n\nIf you are unsure, try it!"
  },
  {
    "objectID": "posts/2022-03-28-ajtfb-chapter-7.html#tip-5-use-test-time-augmentation-tta",
    "href": "posts/2022-03-28-ajtfb-chapter-7.html#tip-5-use-test-time-augmentation-tta",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models",
    "section": "Tip 5: Use Test Time Augmentation (TTA)",
    "text": "Tip 5: Use Test Time Augmentation (TTA)\n\n\n\n\n\n\nImportant\n\n\n\nTTA is a form of data augmentation applied to the validation set that adds augmented versions of the images. “During inference or validation, creating multiple versions of each image using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.”\n\n\n“… select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions. In fact, we can do this not just for different crops, but for different values across all of our test time augmentation parameters”\nWhat is the problem TTA addresses and why use it?\n“When we use random cropping, fastai will automatically use center-cropping for the validation set” which can be probelmatic, for example, in multi-label tasks where “sometimes there are small objects toward the edges of an image” that might be cropped out entirely or perhaps features on the fringe that are required for any classification task.\n\n# you can pass any `DataLoaders` to `tta()` (by default it uses your validation `DataLoader`)\npreds, targs = learn.tta()\naccuracy(preds, targs).item()\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\n0.861090362071991\n\n\n“TTA gives us a good boost in performance, with no additional training required."
  },
  {
    "objectID": "posts/2022-03-28-ajtfb-chapter-7.html#tip-6-use-mixup",
    "href": "posts/2022-03-28-ajtfb-chapter-7.html#tip-6-use-mixup",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models",
    "section": "Tip 6: Use MixUp",
    "text": "Tip 6: Use MixUp\n“Mixup … is a powerful data augmentation technique that can provide dramatically higher accuracy, especially when you don’t have much data and don’t have a pretrained model that was trained on data similar to your dataset”\nIt is a dataset-independent form of data augmentation = can be applied without domain knowledge of the dataset to configure other forms of data augmentation (e.g., flipping and to what degree, etc…)\nHow does Mixup work?\n\nSelect another random image\nPick a weight at random\nTake a weighted average of the selected image with your image = Your independent variable\nTake a weighted average of the selected image’s labels with your image’s labels = Your dependent variable\nUse #3 to predict #4\n\nIn pseudocode:\nimg2, targ2 = dataset[randint(0, len(dataset))]\nt = random_float(0.5, 1.0)\nnew_img = t * img1 + (1-t) * img2\nnew_targ = t * targ1 + (1-t) * targ2\n\n\n\n\n\n\nNote\n\n\n\n“For this to work, our targets need to be one-hot encoded”\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“Mixup requires far more epochs”\n\n\n\n\n\n\n\n\nNote\n\n\n\n“One of the reasons Mixup is so exciting is that it can be applied to types of data other than photos. In fact, some people have even shown good results by **using Mixup on activations inside their models, not just on inputs - this allows Mixup to be used for NLP and other data types too.”\n\n\nSee pp.247-249 for a detailed example of how Mixup works and is used in fastai\n\nmodel = xresnet50()\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=MixUp)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.183965\n2.523945\n0.320762\n02:57\n\n\n1\n1.729223\n1.974045\n0.461538\n03:01\n\n\n2\n1.479313\n1.131723\n0.630695\n03:07\n\n\n3\n1.294975\n0.872954\n0.724421\n03:08\n\n\n4\n1.183486\n0.731506\n0.776699\n03:06\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n“… it’s going to be hard to train, because … the model has to predict two labels per image rather than just one …. Overfitting seems less likely to be a problem.”"
  },
  {
    "objectID": "posts/2022-03-28-ajtfb-chapter-7.html#tip-7-use-label-smoothing",
    "href": "posts/2022-03-28-ajtfb-chapter-7.html#tip-7-use-label-smoothing",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models",
    "section": "Tip 7: Use “Label Smoothing”",
    "text": "Tip 7: Use “Label Smoothing”\n“In the theoretical expression of loss, in classification problems, our targets are one hot encoded …. That means the model is trained to return 0 for all categories but one, for which it is trained to return 1…. This encourages overfitting and gives you at inference time a model that is not going to give meaningful probabilities: it will always say 1 for the predicted category even if it’s not too sure, just because it was trained that way.\n\n\n\n\n\n\nImportant\n\n\n\n“This can become very harmful if your data is not perfectly labeled.”\n\n\n“In general, your data will never be perfect. Even if the labels were manually produced by humans, they could make mistakes, or have differences of opinions on images that are harder to label”\nWhat is the solution this this?\n“… we could replace all our 1s with a number a bit less than 1, and our 0s with a number a bit more than 0, and then train. This is” = Label smoothing. “By encouraging your model to be a less confident, label smoothing will make your training more robust, even if there is mislabeled data. The result will be a model that generalizes better at inference.”\nSee pp.249-251 for a detailed explanation and example of how label smoothing operates. To use it, we just have to change our loss function.\n\nmodel = xresnet50()\nlearn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.757509\n3.500999\n0.253921\n03:06\n\n\n1\n2.257501\n2.817133\n0.440627\n03:00\n\n\n2\n1.968483\n2.138581\n0.617625\n02:59\n\n\n3\n1.781833\n1.700527\n0.772591\n03:05\n\n\n4\n1.648491\n1.632251\n0.798357\n03:01\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“As with Mixup, you won’t generally see significant improvements from label smoothing until you train more epochs.”"
  },
  {
    "objectID": "posts/2022-03-28-ajtfb-chapter-7.html#resources",
    "href": "posts/2022-03-28-ajtfb-chapter-7.html#resources",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models",
    "section": "Resources",
    "text": "Resources\n\nhttps://book.fast.ai - The book’s website; it’s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc…\nBag of Tricks for Image Classification with Convolutional Neural Networks discusses a variety of techniques you can use with CNNs\nHow to Train Your ResNet 8: Bag of Tricks discusses a variety of techniques you can use to training ResNets.\nIceVision is a great resource for all things computer vision and a fastai friendly library. You may want to follow these twitter accounts as well: @ai_fast_track and @Fra_Pochetti (creator of IceVision)."
  },
  {
    "objectID": "posts/2024-07-14-llm-workshop-objectives-data-filtration.html",
    "href": "posts/2024-07-14-llm-workshop-objectives-data-filtration.html",
    "title": "LLM Workshop #2 - From Noise to Knowledge: Mastering the Art of Objective Definition and Data Refinement”",
    "section": "",
    "text": "If you want to avoid a lot of needless back-and-forth as you go through the process of curating datasets for evaluation and/or fine tuning, building your eval pipeleines, training models, and so forth, stop and ask yourself, “What do I want to build and why?”. How you answer those questions will inform everything going forward!\nWhy am I making a big deal about this?\nSimple, when I started … I didn’t do it.\nI had a general idea of building a tool calling system for work and just dove in, and as I would get into various aspects of development I’d be like, “Hold on, this isn’t right … why am I trying to do this when all I really need is this?” In the end, I had to start over a few times, and before the last time, I decided to go outside with the dogs and a good cup of coffee and really think about what I wanted to accomplish and why.\nBelow is the result of that quiet time well spent.\n\n\n\n\n\n\nTip: Give your objectices a good think!\n\n\n\nIt’s crucial to deeply consider what you aim to achieve and understand the reasons behind it. Reflecting on your objectives and motivations helps ensure that your efforts are purposeful and aligned with your aspirations. It will also help prevent you from needlessly starting over, as well as, give you confidence that what you “think you want” is realistic, useful, and will add value.\nThings to ask yourself:\n\nWhat do you want the model to do for your end users?\nCan you define specifically what the model is supposed to do, or is it so general that you find it difficult to scope or measure quality?\nHow diverse and varied does your data need to be to get the model to do what you want?"
  },
  {
    "objectID": "posts/2024-07-14-llm-workshop-objectives-data-filtration.html#what-do-you-want-to-build-and-should-you-build-it",
    "href": "posts/2024-07-14-llm-workshop-objectives-data-filtration.html#what-do-you-want-to-build-and-should-you-build-it",
    "title": "LLM Workshop #2 - From Noise to Knowledge: Mastering the Art of Objective Definition and Data Refinement”",
    "section": "",
    "text": "If you want to avoid a lot of needless back-and-forth as you go through the process of curating datasets for evaluation and/or fine tuning, building your eval pipeleines, training models, and so forth, stop and ask yourself, “What do I want to build and why?”. How you answer those questions will inform everything going forward!\nWhy am I making a big deal about this?\nSimple, when I started … I didn’t do it.\nI had a general idea of building a tool calling system for work and just dove in, and as I would get into various aspects of development I’d be like, “Hold on, this isn’t right … why am I trying to do this when all I really need is this?” In the end, I had to start over a few times, and before the last time, I decided to go outside with the dogs and a good cup of coffee and really think about what I wanted to accomplish and why.\nBelow is the result of that quiet time well spent.\n\n\n\n\n\n\nTip: Give your objectices a good think!\n\n\n\nIt’s crucial to deeply consider what you aim to achieve and understand the reasons behind it. Reflecting on your objectives and motivations helps ensure that your efforts are purposeful and aligned with your aspirations. It will also help prevent you from needlessly starting over, as well as, give you confidence that what you “think you want” is realistic, useful, and will add value.\nThings to ask yourself:\n\nWhat do you want the model to do for your end users?\nCan you define specifically what the model is supposed to do, or is it so general that you find it difficult to scope or measure quality?\nHow diverse and varied does your data need to be to get the model to do what you want?"
  },
  {
    "objectID": "posts/2024-07-14-llm-workshop-objectives-data-filtration.html#objective-definition",
    "href": "posts/2024-07-14-llm-workshop-objectives-data-filtration.html#objective-definition",
    "title": "LLM Workshop #2 - From Noise to Knowledge: Mastering the Art of Objective Definition and Data Refinement”",
    "section": "Objective Definition",
    "text": "Objective Definition\nFor this project I’ll be building a system where a user can ask an AI to perform various NLP tasks on one or more text documents. These documents represent survey comments sourced from several kinds of surveys delievered in higher education institutions (e.g., staff, faculty, student satisfaction and engagement surveys). There are core tasks that I need to perform like machine translation and sentiment analysis, but I’d also like to build a system whereby a user can provide it their own tools focused on NLP and the system figure out how to use them correctly.\n\nVersion 1.0\nCurrently I have a pipeline, that for every task, makes a call to GPT-4 for every document. I did this because I couldn’t get good results by giving GPT several somewhat complex tools for it to use based on what the user asked. The tools would get called but the results weren’t great. Evaluation has been left to the eyeballs of myself and other human data scientists, which of course isn’t optimal either.\nGiven that this current system is slow to process any substantial set of survey comments, relatively expensive given the number of calls to OpenAI I’m making, and doesn’t have a real evaluation-first workflow … I figured it was a good candidate for the course project. At the very least I could build an eval framework to really understand how well things were working and maybe fine tune a smaller model that could understand how to properly call all the tools to satisfy an end user’s ask in a single go.\n\n\nThe Surface Area\nIf your objectives are too general, for example a general purpose chatbot or even a general purpose tool calling machine, you are likely headed to some rough times. Considering my objective above, what can I infer about what I really want to build here?\n\nThe model should use NLP tools for specifically understanding survey comments from staff, students, and faculty in higher education. These comments are sourced from a number of surveys delievered to higher education audiences all over the country\nSome clients allow for their surveys to be taken into Spanish, so the machine translation tool will need to be called initially for each of these comments so that the remaining tasks can be performed wholistically on English texts.\nAside from machine translation, the core tools consist of functions for sentiment analysis, NER, summarization, and thematic analysis.\nEach task operations on either whole survey comments or on semantically chunked comments that need to be associated to a “topic” for thematic analysis.\nThe tools should be focused on performing NLP tasks well, particular in the business domain of survey comments in higher educational settings.\n\n\n\nRetrospective\nConsidering the above, this system definitely needs a solid eval workflow but also presents a potential great argument for fine tuning.\n\n\n\n\n\n\nNarrow domains are great for fine tuning\n\n\n\nThe more specific and specialized the task and/or data, the more likely you are to find success in fine tuning\n\n\nBoth the domain and tasks are very specific to higher education and NLP analysis in that setting, I have access to a lot of diverse data, and there is definite business value in creating a model that might reduce costs, speed up processing, and perform at the same level as GPT-4.\nGiven all this, I feel pretty good about proceeding."
  },
  {
    "objectID": "posts/2024-07-14-llm-workshop-objectives-data-filtration.html#data-refinement",
    "href": "posts/2024-07-14-llm-workshop-objectives-data-filtration.html#data-refinement",
    "title": "LLM Workshop #2 - From Noise to Knowledge: Mastering the Art of Objective Definition and Data Refinement”",
    "section": "Data Refinement",
    "text": "Data Refinement\nBased on my objectives, I need to chiefly be able to support two uses cases:\n\nTasks that need to be performed on a single document (e.g., translation, summary, NER, sentiment). Traditionally we have only done sentiment on chunks because it is the chunks that are associated to topics and we want to report sentiment by topic. This still needs to happen but it might also be interesting to predict sentiment across the full comment as well.\nTasks that need to be performed on a collection of related documents (.e.,g thematic analysis, topic sentiment).\n\nIn this section we ask, “How can I curate a set of survey comments and semantically related chunks that are varied enough to support these two use cases across all our survey clients?”\n\nBuild Some Datasets\nI need a diverse set of documents from different survey clients, delivered to different audiences (staff, student, and faculty), and with a subset being in Spanish. I also need to clean up these documents so they are actually useful. If they are too short or represent useless statements like “N/A” or “I don’t know”, they probably won’t be very helpful.\nI don’t need to synthetically create any of the raw documents since I created the survey sytem and have access to the Microsoft SQL Server database it lies in.\nThinking about the use cases I need to support, the following four datasets will be created:\n\n_clean: A cleaned up version of the raw document dataset\n_sample_Xk: A sample of X rows from cleaned (mine will end up to be roughly 14k survey comments)\n_sample_Xk_chunked: A properly chunked version of _sample_Xk that includes predicted topics as well.\n_sample_Xk_topics: The chunks associated to each topic\n\nDocument analysis tasks will use a mix of datasets #2 and #3 so they can learn to operate of both full and chunked comments. Related document analysis will use dataset #4 for thematic analysis and sentiment.\nI am prohibited from sharing the actual data, but I can share the code I wrote to create these datasets for this project.\n\nStep 1: Cleanup\nI have almost 500k comments that span decades of surveys. Before building a representative sample, I want to remove those comments that aren’t likely to be used or even of much use for NLP tasks. For example, comments that are too short or uninformative (e.g., like “N/A”, or “Great”) shouldn’t be part of eval/training datasets. Preprocessing in version 1.0 takes these comments out in current pipeline so we’ll do so here as well.\nAfter loading the full dataset and filtering on the columns I need to build up the datasets above, my DataFrame looks like this: \nIn particular, we’ll run the functions below to slim this dataset down and give us quality comments we can confidently use in building out our sampled datasets.\n\nA. Remove NaNs and Trim Whitespaces\ndef init_document_preprocessing(df: pd.DataFrame, text_col: str):\n    \"\"\"Remove empty documents and clean up whitespace.\"\"\"\n    df = df.copy()\n\n    # Remove all rows where `text_attr` is Nan\n    df = df.dropna(subset=list(set([attr for attr in [text_col] if attr is not None])), how=\"all\")\n\n    # Remove all whitespace from `text_attr`, `language_attr`, `non_english_text_attr`\n    for attr in list(set([text_col, \"AnswerLang\"])):\n        if attr is not None:\n            df[attr] = df[attr].apply(lambda v: str(v).strip())\n\n    return df\n\ndf = init_document_preprocessing(raw_df, text_col=\"AnswerText\")\n\n\nB. Remove Duplicate Documents\nDuplicate comments won’t be helpful and they definitely exist even within a single survey. For example, some users try to game the system by repeating the same comment thoughout the survey in hopes of making sure their points float to the top.\ndef remove_duplicate_documents(df: pd.DataFrame, text_col: str):\n    \"\"\"Eliminate duplicate documents.\"\"\"\n    df = df.copy()\n    unique_attrs = [text_col]\n\n    # Remove duplicate rows (keeping the first instance)\n    return df.drop_duplicates(subset=unique_attrs).copy()\n\ndf = remove_duplicate_documents(verbatims_df, text_col=\"AnswerText\")\n\n\nC. Remove Duplicated Sentences\nThe same folks above also like to try and game the system by repeating the same phrase over and over again. The below code does a decent job at eliminating a string of repetitive content.\ndef remove_duplicate_sentences(text: str):\n    \"\"\"Remove duplicated sentences throughout an entire text sequence.\"\"\"\n    if text is None:\n        text = \"\"\n\n    text = re.sub(r\"(?&lt;=[.!?])\\s*\", \" \", text)\n    sentences = nltk.sent_tokenize(text)\n    unique_sentences = list(dict.fromkeys(sentences))\n\n    # Tokenize the text into sentences and remove duplicates by converting to set and back to list\n    return \" \".join(unique_sentences)\n\n\ndef remove_duplicate_sentences_in_documents(df: pd.DataFrame, text_col: str):\n    df = df.copy()\n    df[text_col] = df[text_col].apply(lambda v: remove_duplicate_sentences(v))\n    return df\n\ndf = remove_duplicate_sentences_in_documents(verbatims_df, text_col=\"AnswerText\")\n\n\nD. Remove Meaningless and Short Documents\nComments that are too short or uninformative aren’t helpful to any NLP tasks. The code below removes much of this based on my examination of the data and running classification models that predicts whether a comment is_nonsense.\nremove_texts = [\n    \"none\",\n    \"nothing\",\n    \"not sure\",\n    \"no suggestions\",\n    \"no comment\",\n    \"no comments\",\n    \"nothing to add\",\n    \"n/a\",\n    \"none at this time\",\n    \"no comments at all\",\n    \"see previous comment\",\n    \"not really\",\n]\n\ndef is_bad_value(s, remove_texts=[]):\n    is_bad = False\n    for rt in remove_texts:\n        is_bad = bool(re.match(rf\"^{rt.lower().strip()}[!.,;:?]*$\", str(s).lower().strip()))\n        if is_bad:\n            break\n\n    return is_bad\n\n\ndef remove_meaningless_documents(df, text_col: str):\n    df = df.copy()\n    df[text_col] = df[text_col].apply(lambda v: str(v) if v is not None and not is_bad_value(v, remove_texts) else \"\")\n    return df\n\n\ndef remove_short_documents(df: pd.DataFrame, text_col: str, min_text_length: int = 5):\n    df = df.copy()\n    df = df[pd.notna(df[text_col]) & (df[text_col].str.len() &gt;= min_text_length)]\n    return df\n\ndf = remove_meaningless_documents(verbatims_df, text_col=\"AnswerText\")\ndf = remove_short_documents(verbatims_df, text_col=\"AnswerText\")\nThat’s it for dataset #1, let’s save it\ndf.to_parquet(f\"{DATA_DIR}/clean/documents_all.parquet\")\n\n\n\nStep 2: Sample\nThere are a lot of documents so we’ll create a subset of &lt; 15k with enough variation for meaningful scoring functions and finetunes\n\nA. Build a Representative Subset\nThere are two core types of surveys, satisfaction and engagement. There are many more instances of the satisfaction surveys so we’ll want to have more of those in our sampled dataset to mimic reality.\nfiltered_df = df[(df[\"BenchmarkSurveyType\"].str.startswith(\"CSS-\"))]\n\n# Calculate the number of samples for each BenchmarkSurveyType\nsample_sizes = filtered_df[\"BenchmarkSurveyType\"].value_counts(normalize=True) * 10000\nsample_sizes = sample_sizes.round().astype(int)\n\n# Sample records proportionally with a bias towards longer comments\nsampled_df = pd.DataFrame()\nfor survey_type, size in sample_sizes.items():\n    subset = filtered_df[filtered_df[\"BenchmarkSurveyType\"] == survey_type]\n    weights = subset[\"AnswerTextCharacterCount\"] / subset[\"AnswerTextCharacterCount\"].sum()\n    sampled_records = subset.sample(n=size, weights=weights, random_state=42)\n    sampled_df = pd.concat([sampled_df, sampled_records])\n\n# Reset index if needed\nsampled_df = sampled_df.reset_index(drop=True)\n\n# This gets us down to 10k examples of customer satisfaction like surveys.\n\nsampled_df = pd.concat([sampled_df, df[(df[\"BenchmarkSurveyType\"].str.startswith(\"SAW\"))]])\n\n# Adding in these staff engagement surveys get us to a total of almost 14k examples\n\n\nB. Add in some Spanish Documents\nI lied when I said there was no syntetically generated data because we’re going to use an LLM to generate some Spanish content from our English survey comments. We’ll use one of my favorite libraries, LangChain, to generate 250 Spanish examples.\nclass SpanishTranslation(BaseModel):\n    \"\"\"The translation of a document from English to Spanish.\"\"\"\n\n    spanish_translation: str = Field(..., description=\"The English tranlsation\")\n\ndef get_openai_translation_messages(domain: str = \"survey comments\"):\n    system_msg = f\"You are a world class translator. Translate the English {domain} below to Spanish. Properly escape strings.\"\n    human_msg = \"{input}\"\n\n    prompt_msgs = [SystemMessage(content=system_msg), HumanMessagePromptTemplate.from_template(human_msg)]\n    return prompt_msgs\n\nllm = ChatOpenAI(model=\"gpt-4\")\nmessages = get_openai_translation_messages()\n\nprompt = ChatPromptTemplate(messages=messages)\nfunctions = [convert_to_openai_function(SpanishTranslation)]\n\nchain = prompt | llm.bind(function_call={\"name\": SpanishTranslation.__name__}, functions=functions) | JsonOutputFunctionsParser()\n\nspanish_sample_df = sampled_df.sample(250, random_state=9)\n\nspanish_translations = []\nfor r_idx, r in spanish_sample_df.iterrows():\n    rsp = chain.invoke({\"input\": str(r[\"AnswerText\"])})\n    spanish_translations.append(rsp[\"spanish_translation\"])\n\nsampled_df[\"AnswerText_NonEnglish\"] = None\nsampled_df[\"AnswerLang\"] = \"English\"\n\nspanish_sample_df[\"AnswerText_NonEnglish\"] = spanish_translations\nspanish_sample_df[\"AnswerLang\"] = \"Spanish\"\n\n# Set 'MLVerbatId' as the index for both DataFrames\nsampled_df.set_index(\"MLVerbatimId\", inplace=True)\nspanish_sample_df.set_index(\"MLVerbatimId\", inplace=True)\n\n# Update df1 with values from df2\nsampled_df.update(spanish_sample_df)\n\n# Reset index if needed\nsampled_df.reset_index(inplace=True)\nThat’s it for dataset #2, let’s save it\ndf.to_parquet(f\"{DATA_DIR}/clean/documents_sample_14k.parquet\")\n\n\n\nStep 3: Chunk\nThe core thematic analysis task we need to support operates on semantically related survey comment “chunks”. In addition to predicting themes for these collections, we also need to report the sentiment for each chunk so that we can visualize sentiment as it relates to each topic.\nWe’ll use the semantic-chunkers library to build semantically related chunks which is something the current pipeline doesn’t use yet, but makes a lot of sense to me after exploring chunking in depth over the past few months or so. I really like this library and it comes with some nifty visualization capabilities for tuning its hyperparameters.\nChunking can be somewhat complex and use case specific, but for the puposes of survey comments we are usually working with paragraphs and sentences sometimes containing bullet points to further deliniate different topics. Given this, I’ve asked chatGPT about general token usage for English paragraphs and sentences to set a few of these hyperparameters and do some basic preliminary chunking before using the semantic-chunkers library.\nAccording to chatGPT, the average number of characters in an English sentence and paragraph can vary based on factors such as writing style, purpose, and medium. However, general estimates are as follows:\n\nAverage Characters in an English Sentence:\n\nAn average English sentence typically contains around 15 to 20 words.\nAssuming an average word length of 5 characters (including spaces and punctuation), an average sentence would be approximately 75 to 100 characters.\n\nAverage Characters in an English Paragraph:\n\nAn average English paragraph usually contains about 3 to 5 sentences.\nUsing the upper bound of 5 sentences and assuming each sentence is 100 characters, an average paragraph would be around 300 to 500 characters.\n\n\nThese averages can fluctuate based on the type of text (e.g., academic writing, casual writing, technical documentation) and individual writing styles but it seems reasonable to assume on average:\n\nA min sentence has 15 * 1.5 = 23 tokens\nA max paragrpah as 20 _ 5 _ 1.5 = 150 tokens\n\nIf you’re interested in learning about the semantic-chunkers, check out these resources:\nSemantic Chunkers Into (Colab)\nSemantic Chunking for RAG (James Briggs)\n\nA. Preliminary Chunking\nWe start with the assumption that paragraphs likely represent distinct topics a user is trying to get at in any given survey comment. If there are bullet points, regardless of format, we also assume that each of these likely represent a distinct idea or “chunk.” Given this, we will do some initial chunking based on those assumptions.\nAlso, I have and continue to look at a lot of this data, so they aren’t really assumptions as much as they simply reflect what I see.\ndef chunk_paragraphs(text):\n    paragraphs = re.split(\n        \"\\n\\n|\\r\\n|\\\\\\\\n\\\\\\\\n|\\\\\\\\r\\\\\\\\n|\" + r\"\\\\n\\s*[-•*o]|\\\\n\\s*\\d+[.)]\", text\n    )  # text.split(\"\\n\\n\")  # Split text by double line breaks to identify paragraphs\n\n    return [chunk.strip() for chunk in paragraphs if len(chunk.strip()) &gt; 4]\n\ndef chunk_texts(df: pd.DataFrame):\n    \"\"\"Chunk the paragraphs keeping any bullet points alongside their context.\"\"\"\n    df = df.copy()\n\n    df.insert(1, \"_seq\", df[\"AnswerText\"].apply(lambda v: chunk_paragraphs(v)))\n\n    df = df.explode(\"_seq\").reset_index(drop=True)\n    df[\"_seq\"] = df[\"_seq\"].str.strip()\n    df.insert(2, \"_seq_id\", df.groupby([\"MLVerbatimId\"]).cumcount())\n    df[\"_seq_length\"] = df[\"_seq\"].str.len()\n\n    return df\n\ndef remove_bullet_points(text):\n    return re.sub(r\"(^|\\n)\\s*[-•*o]\\s*|\\n\\s*\\d+[.)]\\s*\", \" \", text).strip()\n\nchunked_df = chunk_texts(sampled_df)\nchunked_df[\"_seq\"] = chunked_df[\"_seq\"].apply(remove_bullet_points)\n\n\nB. Semantic Chunking\nFrom the semantic-chunkers intro notebook mentioned above:\n\nThe statistical chunking method our most robust chunking method, it uses a varying similarity threshold to identify more dynamic and local similarity splits. It offers a good balance between accuracy and efficiency but can only be used for text documents (unlike the multi-modal ConsecutiveChunker).\n\n\nThe StatisticalChunker can automatically identify a good threshold value to use while chunking our text, so it tends to require less customization than our other chunkers.\n\nI did some review of specific examples and played with the hyperparameters to get what looked like decent results. Here is what I came up with ultimately for perform the final chunking of the dataset.\nencoder = HuggingFaceEncoder(name=\"thenlper/gte-large\")\n\nchunker = StatisticalChunker(\n    encoder=encoder,\n    threshold_adjustment=0.01,\n    dynamic_threshold=True,\n    window_size=5,  # 5,\n    min_split_tokens=23,  # 100,\n    max_split_tokens=300,  # 500\n    split_tokens_tolerance=10,\n    plot_chunks=False,\n    enable_statistics=False,\n)\n\ndef get_semantic_chunks(txt: str, min_chars_to_chunk: int = 90):\n    try:\n        if len(txt.strip()) &lt; min_chars_to_chunk:\n            return [txt.strip()]\n        chunks = chunker(docs=[txt])\n        return [chunk.content.strip() for chunk in chunks[0]]\n    except Exception as e:\n        return [txt.strip()]\n\n\nseqs = chunked_df[\"_seq\"].values.tolist()\nchunked_docs = []\nfor seq in tqdm(seqs):\n    chunked_docs.append(get_semantic_chunks(seq))\n\nchunked_df.insert(1, \"_chunk\", chunked_docs)\nchunked_df = chunked_df.explode(\"_chunk\").reset_index(drop=True)\nchunked_df[\"_chunk\"] = chunked_df[\"_chunk\"].str.strip()\nchunked_df.insert(2, \"_chunk_id\", chunked_df.groupby([\"MLVerbatimId\", \"_seq_id\"]).cumcount())\nchunked_df[\"_chunk_length\"] = chunked_df[\"_chunk\"].str.len()\n\n\nC. Use BertTopic Associate Each Chunk To A Topic\nI’m not going to go into the specifics of this step since I don’t get detract from the focuse of this article anymore than I likely already have. Suffice to say, I use BERTopic to create and assign topics to each “chunk”. This is another great and feature rich library that I’ve been using for a few years.\nThe artifact produced at the conclusion of this step is a DataFrame with topic identiferis associated to each _chunk. It looks like this: \nThat’s it for dataset #3, let’s save it\nchunked_df.to_parquet(f\"{DATA_DIR}/clean/documents_sample_14k_chunked.parquet\")\n\n\n\nStep 4: Topic Summaries\nWe need to evaluate the ability for the model to summarize and define action plans for related “chunks” as identified by a topic model. We’ll use the chunked dataset create above to put something together we can use to predict/evaluate the themes and action plans we assign to topics.\nFortunately for you, the reader, there isn’t alot of code to make this happen :)\ntopics_df = chunked_df.copy()\n\ncols = [\"pred_theme_id\", \"pred_orig_theme_name\"]\n\ntopics_df[\"pred_theme_id\"] = topics_df[\"pred_theme_id\"].astype(int)\ntopics_df[\"_chunk\"] = topics_df[\"_chunk\"].astype(str)\n\nsummaries_df = topics_df.groupby(by=cols)[\"_chunk\"].agg(list).reset_index()\nThat’s it for dataset #4, let’s save it\nsummaries_df.to_parquet(f\"{DATA_DIR}/clean/documents_sample_14k_topics.parquet\")"
  },
  {
    "objectID": "posts/2024-07-14-llm-workshop-objectives-data-filtration.html#next-steps",
    "href": "posts/2024-07-14-llm-workshop-objectives-data-filtration.html#next-steps",
    "title": "LLM Workshop #2 - From Noise to Knowledge: Mastering the Art of Objective Definition and Data Refinement”",
    "section": "Next Steps",
    "text": "Next Steps\nWith some high quality context, we’ll move on to getting a “vibe check” for how likely what we want to build is possible by seeing what kinds of structured outputs we get when using the big dogs. In particular we’ll be running some tests with OpenAI, Anthropic, Fireworks, and Replicate to develop a good intutition of how well to expect things might work with both closed and open source models.\nAgain, I’m using Hamel’s ftcourse repo as a general guide for building this project out so make sure to check it out as y’all start your own journeys. The topic in this blog post isn’t necessarily covered in any of his notebooks so consider this the 00 notebook that is more or less implied in the course.\nAlso, I welcome any ideas on improving anything and everything presented above. Especially if you notice any really egregious and glaring errors in my thinking or workflow, I’d defintely appreciate your thoughts. Either way, thanks for reading to the end :)"
  },
  {
    "objectID": "posts/2024-12-24-gemini-part-2-sdk.html",
    "href": "posts/2024-12-24-gemini-part-2-sdk.html",
    "title": "Gemini (Part II) - The Unified SDK",
    "section": "",
    "text": "In the previous post we looked at how Gemini, espeically 2.0, compares with other models relative to both performance and cost and why you should consider it as a potential “go to” in building AI powered applications of almost any kind. Today, we’ll discuss how to get started with their new unified SDK that both Gemini API and Vertex API users can use to build applications.\n\n\nIs it just me, but I'm more hyped about this than all the new sick features in @google Gemini 2.0 …https://t.co/mMiba2pwMq\"The new Google Gen AI SDK provides a unified interface … through both the Gemini Developer API and the Gemini Enterprise API ( Vertex AI)\"\n\n— Wayde Gilliam (@waydegilliam) December 11, 2024\n\n\nAmongst the biggest complaints from Gemini devs, has been the inconsistency of the SDKs for both the Gemini API and Vertex API. Well it looks like Google has finally heard our cries and as part of the Gemini 2.0 release, they’ve released a new unified SDK that both Gemini API and Vertex API users can use to build applications. This makes is super easy to bounce back-and-forth between the two API options without having to understand the nuances of two somewhat different SDKs and what might break when you switch between them."
  },
  {
    "objectID": "posts/2024-12-24-gemini-part-2-sdk.html#hallelujah",
    "href": "posts/2024-12-24-gemini-part-2-sdk.html#hallelujah",
    "title": "Gemini (Part II) - The Unified SDK",
    "section": "",
    "text": "In the previous post we looked at how Gemini, espeically 2.0, compares with other models relative to both performance and cost and why you should consider it as a potential “go to” in building AI powered applications of almost any kind. Today, we’ll discuss how to get started with their new unified SDK that both Gemini API and Vertex API users can use to build applications.\n\n\nIs it just me, but I'm more hyped about this than all the new sick features in @google Gemini 2.0 …https://t.co/mMiba2pwMq\"The new Google Gen AI SDK provides a unified interface … through both the Gemini Developer API and the Gemini Enterprise API ( Vertex AI)\"\n\n— Wayde Gilliam (@waydegilliam) December 11, 2024\n\n\nAmongst the biggest complaints from Gemini devs, has been the inconsistency of the SDKs for both the Gemini API and Vertex API. Well it looks like Google has finally heard our cries and as part of the Gemini 2.0 release, they’ve released a new unified SDK that both Gemini API and Vertex API users can use to build applications. This makes is super easy to bounce back-and-forth between the two API options without having to understand the nuances of two somewhat different SDKs and what might break when you switch between them."
  },
  {
    "objectID": "posts/2024-12-24-gemini-part-2-sdk.html#initializing-the-sdk-client",
    "href": "posts/2024-12-24-gemini-part-2-sdk.html#initializing-the-sdk-client",
    "title": "Gemini (Part II) - The Unified SDK",
    "section": "Initializing the SDK client",
    "text": "Initializing the SDK client\n\nA note on the Gemini API and Vertex AI (from the official docs)\n\nIn the whitepapers, most of the example code uses the Enterprise Vertex AI platform. In contrast, this notebook, along with the others in this series, will use the Gemini Developer API and AI Studio.\nBoth APIs provide access to the Gemini family of models, and the code to interact with the models is very similar. Vertex provides a world-class platform for enterprises, governments and advanced users that need powerful features like data governance, ML ops and deep Google Cloud integration.\nAI Studio is free to use and only requires a compatible Google account to log in and get started. It is deeply integrated with the Gemini API, which comes with a generous free tier that you can use to run the code in these exercises.\nIf you are already set up with Google Cloud, you can check out the Enterprise Gemini API through Vertex AI, and run the samples directly from the supplied whitepapers.\n\nI’ll be showing how to use both platforms below, but the first step is the same either way … a single pip install :)\npip install google-genai\n\n\n\n\n\n\nNote\n\n\n\nHere are some reference links to the new SDK docs and a Getting Started notebook courtesy of Google with a lot more information and examples …\n\n2.0 SDK Docs\n2.0 SDK Getting Started Notebook\n\n\n\n\n\nIf you are using the Gemini Developer API …\n# with Google AI API key\nGOOGLE_AI_API_KEY = os.getenv(\"GOOGLE_AI_API_KEY\")\n\nclient = genai.Client(api_key=GOOGLE_AI_API_KEY)\n\nresponse = client.models.generate_content(model=\"gemini-2.0-flash-exp\", contents=\"How far is the moon from the earth?\")\n\ndisplay(Markdown(response.text))\n# The distance between the Earth and the Moon isn't constant, as the Moon travels in an elliptical orbit around our planet. Therefore, we talk about the distance in terms of averages:\n\n# Average distance: Approximately 238,855 miles (384,400 kilometers).\n# However, keep in mind:\n\n# Perigee (Closest Approach): The Moon can get as close as about 225,623 miles (363,104 kilometers).\n# Apogee (Farthest Distance): The Moon can be as far as about 252,088 miles (405,696 kilometers).\n# So, while the average distance is a good figure to keep in mind, it's important to remember that the actual distance varies throughout the month.\n\nprint(response.usage_metadata)\n# cached_content_token_count=None candidates_token_count=181 prompt_token_count=10 total_token_count=191\n\n\n\n\n\n\nNote\n\n\n\nThe resonse is an instance of GenerateContentResponse. In addition to the generated content, it includes a bunch of other information including the token usage (via response.usage_metadata), a safety rating and explanations, and a variety of other information that will prove more useful depending on how we use Gemini (e.g., whether we use it with multimodal inputs, or with tool calling, code execution, etc…). You’ll see what I mean down below.\nThere is an async version of generate_content available via the aio property on your client instance. You just need to add await in front of the call.\nresponse = await client.aio.models.generate_content(\n    model=\"gemini-2.0-flash-exp\", contents=\"How far is the moon from the earth?\", config=gen_config\n)\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe text response is almost always going to be in Markdown, so why not make it look pretty in notebooks by doing a from IPython.display import Markdown, display and then display(Markdown(response.text))?\n\n\n\n\nIf you are using Vertex …\n# this is the path to your json credentials file (at least it is to mine, :))\nGOOGLE_VERTEX_AI_CREDS = os.getenv(\"GOOGLE_VERTEX_AI_CREDS\")\n\nSCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\ncreds = service_account.Credentials.from_service_account_file(GOOGLE_VERTEX_AI_CREDS, scopes=SCOPES)\n\nclient = genai.Client(vertexai=True, project=\"generative-playground\", location=\"us-central1\", credentials=creds)\n\nresponse = client.models.generate_content(model=\"gemini-2.0-flash-exp\", contents=\"How far is the moon from the earth?\")\n\ndisplay(Markdown(response.text))\n# The distance between the Earth and the Moon is not constant, as the Moon's orbit is elliptical (an oval shape), not a perfect circle. Here's a breakdown:\n\n# Average Distance: The average distance is about 384,400 kilometers (238,900 miles). This is the number most commonly used.\n# Perigee: This is the point in the Moon's orbit where it is closest to Earth. At perigee, the distance can be as close as 363,104 kilometers (225,623 miles).\n# Apogee: This is the point in the Moon's orbit where it is furthest from Earth. At apogee, the distance can reach as far as 405,696 kilometers (252,088 miles).\n# So, the answer is variable, but the average distance is approximately 384,400 kilometers (238,900 miles).\n\nprint(response.usage_metadata)\n# cached_content_token_count=None candidates_token_count=221 prompt_token_count=9 total_token_count=230\nWe should expect to get something close to the same response from both the Gemini API and Vertex API. The only difference is with how you instantiate your Gemini client.\n\n\n\n\n\n\nImportant\n\n\n\nAs of the time of this writing, Gemini 2.0 models are ONLY available at the “us-central1” location.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe previous SDK had a list_models method that would return a list of models (at least the Gemini API did). This is not supported in the new SDK at the time of this writing.\nfor model in client.models.list():\n    print(model)  # ain't returning nothing useful"
  },
  {
    "objectID": "posts/2024-12-24-gemini-part-2-sdk.html#controlling-generation",
    "href": "posts/2024-12-24-gemini-part-2-sdk.html#controlling-generation",
    "title": "Gemini (Part II) - The Unified SDK",
    "section": "Controlling Generation",
    "text": "Controlling Generation\nYou can influence the generation of content by providing a GenerateContentConfig object. With this object, you can provide a system instruction, safety settings, and generation parameters (e.g., temperature, top_p, etc.).\nSee also:\n\nContent Generation Parameters\nSafety Settings\n\n\n\n\n\n\n\nTip\n\n\n\nI tend to turn all of these “safety” settings off and it would be great if there is an easier way to do this in one-line vs what you see below\n\n\nsafety_settings = [\n    types.SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"BLOCK_NONE\"),\n    types.SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"BLOCK_NONE\"),\n    types.SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"BLOCK_NONE\"),\n    types.SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"BLOCK_NONE\"),\n]\n\ngen_config = types.GenerateContentConfig(\n    system_instruction=\"You are an expert in all things astronomy and you ONLY provide very concise answers\",\n    safety_settings=safety_settings,\n    temperature=0,\n    top_p=0.95,\n    top_k=20,\n    candidate_count=1,\n    seed=5,\n    max_output_tokens=100,\n    stop_sequences=[\"STOP!\"],\n    presence_penalty=0.0,\n    frequency_penalty=0.0,\n)\n\nresponse = client.models.generate_content(model=\"gemini-2.0-flash-exp\", contents=\"How far is the moon from the earth?\", config=gen_config)\nWith that in place we can see that our response is now much more concise and to the point.\nresponse = client.models.generate_content(model=\"gemini-2.0-flash-exp\", contents=\"How far is the moon from the earth?\", config=gen_config)\nprint(response.text)\n# About 384,400 kilometers.\n\nprint(response.usage_metadata)\n# cached_content_token_count=None candidates_token_count=12 prompt_token_count=24 total_token_count=36"
  },
  {
    "objectID": "posts/2024-12-24-gemini-part-2-sdk.html#tokenization-goodies",
    "href": "posts/2024-12-24-gemini-part-2-sdk.html#tokenization-goodies",
    "title": "Gemini (Part II) - The Unified SDK",
    "section": "Tokenization Goodies",
    "text": "Tokenization Goodies\nYou can use the count_tokens method to calculate the number of input tokens before sending a request to the Gemini API. Super helpful to understanding the cost implications of your requests. Async operations are available for both count_tokens and compute_tokens as well as most other methods on your client instance via the aio property.\n# count_tokens\nresponse = client.models.count_tokens(model=\"gemini-2.0-flash-exp\", contents=\"How far is the moon from the earth?\")\nprint(response)\n# total_tokens=9 cached_content_token_count=None\n\nprint(\"\\n======\\n\")\n\n# count_tokens async\nresponse = await client.aio.models.count_tokens(model=\"gemini-2.0-flash-exp\", contents=\"How far is the moon from the earth?\")\nprint(response)\n# total_tokens=9 cached_content_token_count=None\n\n\n\n\n\n\nNote\n\n\n\nAll of the async bits are available via the aio property on your client instance.\n\n\nYou can use compute_tokens to get the tokenized inputs (only available in Vertex AI). This can be really helpful in terms of understanding how the Gemini models tokenize your input and troubleshooing issues that might arise from generation.\n# compute_tokens\nresponse = client.models.compute_tokens(model=\"gemini-2.0-flash-exp\", contents=\"How far is the moon from the earth?\")\nprint(response)\n# tokens_info=[TokensInfo(role='user', token_ids=['2299', '2166', '603', '573', '11883', '774', '573', '6683', '235336'], tokens=[b'How', b' far', b' is', b' the', b' moon', b' from', b' the', b' earth', b'?'])]\n\n\nprint(\"\\n======\\n\")\n\n# compute_tokens async\nresponse = await client.aio.models.compute_tokens(model=\"gemini-2.0-flash-exp\", contents=\"How far is the moon from the earth?\")\nprint(response)\n# tokens_info=[TokensInfo(role='user', token_ids=['2299', '2166', '603', '573', '11883', '774', '573', '6683', '235336'], tokens=[b'How', b' far', b' is', b' the', b' moon', b' from', b' the', b' earth', b'?'])]\n\n\n\n\n\n\nTip\n\n\n\nTokenization issues are often some of the biggest gotchas in issues folks have with working with LLMs. There’s an entire course about the importance of looking at your data … and that means looking at your tokens."
  },
  {
    "objectID": "posts/2024-12-24-gemini-part-2-sdk.html#streaming-content",
    "href": "posts/2024-12-24-gemini-part-2-sdk.html#streaming-content",
    "title": "Gemini (Part II) - The Unified SDK",
    "section": "Streaming Content",
    "text": "Streaming Content\nWe’ve already seen how we can use the generate_content method to generate content but what if we want to stream the content as it is being generated?\nThe generate_content_stream method is a great way to stream content as it is generated. This is useful for things like chat interfaces or long-form content where you want to see the progress as it is being generated.\nAn async version is also available via the aio property.\nfor chunk in client.models.generate_content_stream(model=\"gemini-2.0-flash-exp\", contents=\"How far is the moon from the earth?\"):\n    print(chunk.text, end=\"|\", flush=True)\n\n# The| distance between the Earth and the Moon is not constant, as the Moon's| orbit is elliptical. Here's a breakdown:\n\n# * **Average Distance:**| The average distance between the Earth and the Moon is about **384,400 kilometers (238,900 miles)**. This| is the most commonly cited figure.\n\n# * **Perigee:** This is the point in the Moon's orbit when it is closest to Earth. At| perigee, the distance can be as little as about **363,104 kilometers (225,623 miles)**.\n\n# * **Apogee:** This is the point in the Moon's orbit| when it is farthest from Earth. At apogee, the distance can be as much as about **405,696 kilometers (252,088 miles)**.\n\n# **Key Takeaway:** While we often| talk about the average distance, remember that the Moon's distance varies throughout its orbit, fluctuating between perigee and apogee.\n\n# So, while the average is 384,400 kilometers, it's more accurate to say the distance ranges from about 363,000| km to 406,000 km.\n|\nI’ve included the pipe character so you can see the chunks as they are being generated."
  },
  {
    "objectID": "posts/2024-12-24-gemini-part-2-sdk.html#multi-turn-chat",
    "href": "posts/2024-12-24-gemini-part-2-sdk.html#multi-turn-chat",
    "title": "Gemini (Part II) - The Unified SDK",
    "section": "Multi-Turn Chat",
    "text": "Multi-Turn Chat\nThe chats module provides a way to interact with the Gemini API in a multi-turn chat interface.\nThe create method is used to create a new chat session. The send_message method is used to send a message to the chat session.\nThe history parameter is used to provide the chat history.\nsystem_instruction = dedent(\"\"\"\n  You are an expert software developer and a helpful coding assistant.\n  You are able to generate high-quality code in any programming language.\n\"\"\").strip()\n\nconvo = []\nchat = client.chats.create(\n    model=\"gemini-2.0-flash-exp\", config=types.GenerateContentConfig(system_instruction=system_instruction, temperature=0.5), history=convo\n)\nYou can now use send_message to add a message to the chat.\nresponse = chat.send_message(\"Write a simple python program that accepts a person's name and returns a greeting.\")\n\nMarkdown(response.text)\n# def greet_person(name):\n#   \"\"\"\n#   Greets a person by name.\n\n#   Args:\n#     name: The name of the person to greet (string).\n\n#   Returns:\n#     A greeting string.\n#   \"\"\"\n#   return f\"Hello, {name}!\"\n\n# if __name__ == \"__main__\":\n#   person_name = input(\"Please enter your name: \")\n#   greeting = greet_person(person_name)\n#   print(greeting)\n\n# ... and an extended explanation of the code I'm omitting here ...\nCurrently we can look at, and even modify, the chat history by accessing the _curated_history property on the chat instance. That _ prefix tells me this part of the SDK is still under construction and likely to change.\nchat._curated_history\n# [Content(parts=[Part(video_metadata=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"Write a simple python program that accepts a person's name and returns a greeting.\")], role='user'),\n#  Content(parts=[Part(video_metadata=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='```python\\ndef greet_person(name):\\n  \"\"\"\\n  Greets a person by name.\\n\\n  Args:\\n    name: The name of the person to greet (string).\\n\\n  Returns:\\n    A greeting string.\\n  \"\"\"\\n  return f\"Hello, {name}!\"\\n\\nif __name__ == \"__main__\":\\n  person_name = input(\"Please enter your name: \")\\n  greeting = greet_person(person_name)\\n  print(greeting)\\n```\\n\\n**Explanation:**\\n\\n1.  **`def greet_person(name):`**:\\n    *   This line defines a function named `greet_person` that takes one argument, `name`.\\n    *   The `name` argument will hold the person\\'s name as a string.\\n\\n2.  **`\"\"\"...\"\"\"`**:\\n    *   This is a docstring, which is a multiline string used to document what the function does. It\\'s good practice to include docstrings to make your code more understandable.\\n\\n3.  **`return f\"Hello, {name}!\"`**:\\n    *   This line uses an f-string (formatted string literal) to create the greeting.\\n    *   `f\"...\"`  allows you to embed variables directly into the string by placing them inside curly braces `{}`.\\n    *   The function returns the complete greeting string (e.g., \"Hello, Alice!\").\\n\\n4.  **`if __name__ == \"__main__\":`**:\\n    *   This is a standard Python construct that ensures the code inside the `if` block only runs when the script is executed directly (not when it\\'s imported as a module into another script).\\n\\n5.  **`person_name = input(\"Please enter your name: \")`**:\\n    *   The `input()` function displays the message \"Please enter your name: \" to the user and waits for them to type something.\\n    *   Whatever the user types is stored as a string in the `person_name` variable.\\n\\n6.  **`greeting = greet_person(person_name)`**:\\n    *   This line calls the `greet_person` function, passing the `person_name` as an argument.\\n    *   The function returns the greeting string, which is then stored in the `greeting` variable.\\n\\n7.  **`print(greeting)`**:\\n    *   Finally, this line prints the greeting to the console.\\n\\n**How to run this code:**\\n\\n1.  Save the code in a file named, for example, `greeting.py`.\\n2.  Open a terminal or command prompt.\\n3.  Navigate to the directory where you saved the file.\\n4.  Run the command `python greeting.py`.\\n5.  The program will prompt you to enter your name. Type your name and press Enter.\\n6.  The program will then print the greeting to the console.\\n\\n**Example Interaction:**\\n\\n```\\nPlease enter your name: Bob\\nHello, Bob!\\n```\\n')], role='model')\nresponse = chat.send_message(\"Okay, write a unit test of the generated function.\")\n\nMarkdown(response.text)\n# import unittest\n# from greeting import greet_person  # Assuming the previous code is in greeting.py\n\n# class TestGreeting(unittest.TestCase):\n\n#     def test_greet_with_valid_name(self):\n#         self.assertEqual(greet_person(\"Alice\"), \"Hello, Alice!\")\n#         self.assertEqual(greet_person(\"Bob\"), \"Hello, Bob!\")\n#         self.assertEqual(greet_person(\"Charlie\"), \"Hello, Charlie!\")\n\n#     def test_greet_with_empty_name(self):\n#         self.assertEqual(greet_person(\"\"), \"Hello, !\")\n\n#     def test_greet_with_name_containing_spaces(self):\n#         self.assertEqual(greet_person(\"John Doe\"), \"Hello, John Doe!\")\n\n#     def test_greet_with_name_containing_numbers(self):\n#         self.assertEqual(greet_person(\"User123\"), \"Hello, User123!\")\n\n#     def test_greet_with_special_characters(self):\n#         self.assertEqual(greet_person(\"!@#$%^\"), \"Hello, !@#$%^!\")\n\n# if __name__ == '__main__':\n#     unittest.main()\n\n# ... and an extended explanation of the code I'm omitting here ...\nPretty cool!\n\n\n\n\n\n\nImportant\n\n\n\nNote the “role” names are a bit different than what you might expect. With Gemini 2.0, you’ll see “user” when the message comes from the user and “model” when the message comes from the the LLM."
  },
  {
    "objectID": "posts/2024-12-24-gemini-part-2-sdk.html#conclusion",
    "href": "posts/2024-12-24-gemini-part-2-sdk.html#conclusion",
    "title": "Gemini (Part II) - The Unified SDK",
    "section": "Conclusion",
    "text": "Conclusion\nThe SDK is still being fleshed out but I’m excited about the prospect of being able build with Gemini regardless of which API I’m using. Hopefully, some of the tips and tricks above will help you get started regardless of which API flavor you prefer.\nIn the next post, we’ll look at how to take advantage of Gemini’s multimodal capabilities including how to incorporate few-shot examples for both text only and multimodal tasks."
  },
  {
    "objectID": "posts/2021-05-23-ajtfb-chapter-4.html",
    "href": "posts/2021-05-23-ajtfb-chapter-4.html",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 4: Stochastic Gradient Descent",
    "section": "",
    "text": "Other posts in this series:\nA Journey Through Fastbook (AJTFB) - Chapter 1\nA Journey Through Fastbook (AJTFB) - Chapter 2\nA Journey Through Fastbook (AJTFB) - Chapter 3\nA Journey Through Fastbook (AJTFB) - Chapter 5\nA Journey Through Fastbook (AJTFB) - Chapter 6a\nA Journey Through Fastbook (AJTFB) - Chapter 6b\nA Journey Through Fastbook (AJTFB) - Chapter 7\nA Journey Through Fastbook (AJTFB) - Chapter 8\nA Journey Through Fastbook (AJTFB) - Chapter 9"
  },
  {
    "objectID": "posts/2021-05-23-ajtfb-chapter-4.html#how-to-visualize-a-grayscale-image-in-pandas",
    "href": "posts/2021-05-23-ajtfb-chapter-4.html#how-to-visualize-a-grayscale-image-in-pandas",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 4: Stochastic Gradient Descent",
    "section": "How to visualize a grayscale image in pandas …",
    "text": "How to visualize a grayscale image in pandas …\n\nmnist_path = untar_data(URLs.MNIST_SAMPLE)\nmnist_path.ls()\n\n\n\n\n(#3) [Path('/root/.fastai/data/mnist_sample/labels.csv'),Path('/root/.fastai/data/mnist_sample/valid'),Path('/root/.fastai/data/mnist_sample/train')]\n\n\n\nsample_3 = Image.open((mnist_path/'train/3').ls().sorted()[1])\nsample_3\n\n\n\n\n\n\n\n\n\nsample_3_t = tensor(sample_3)\ndf = pd.DataFrame(sample_3_t[4:15, 4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n29\n150\n195\n254\n255\n254\n176\n193\n150\n96\n0\n0\n0\n\n\n2\n0\n0\n0\n48\n166\n224\n253\n253\n234\n196\n253\n253\n253\n253\n233\n0\n0\n0\n\n\n3\n0\n93\n244\n249\n253\n187\n46\n10\n8\n4\n10\n194\n253\n253\n233\n0\n0\n0\n\n\n4\n0\n107\n253\n253\n230\n48\n0\n0\n0\n0\n0\n192\n253\n253\n156\n0\n0\n0\n\n\n5\n0\n3\n20\n20\n15\n0\n0\n0\n0\n0\n43\n224\n253\n245\n74\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n249\n253\n245\n126\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n14\n101\n223\n253\n248\n124\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n0\n11\n166\n239\n253\n253\n253\n187\n30\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n16\n248\n250\n253\n253\n253\n253\n232\n213\n111\n2\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n43\n98\n98\n208\n253\n253\n253\n253\n187\n22\n0"
  },
  {
    "objectID": "posts/2021-05-23-ajtfb-chapter-4.html#what-is-a-baseline-model-and-why-do-you-want-one",
    "href": "posts/2021-05-23-ajtfb-chapter-4.html#what-is-a-baseline-model-and-why-do-you-want-one",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 4: Stochastic Gradient Descent",
    "section": "What is a baseline model and why do you want one?",
    "text": "What is a baseline model and why do you want one?\n\nA simple model that you are confident should perform reasonably well. It should be simple to implement and easy to test\n\nWhy do you want to start with a baseline model? &gt; … without starting with a sensible baseline, it is difficult to know whether your super-fancy models are any good\nHow do you build/find one of these models?\nYou can search online for folks that have trained models to solve a problem similar to your’s and/or you can start with one of the high-level examples in the fastai docs against your data. There are a bunch covering core vision, text, tabuluar and colab filtering tasks right here."
  },
  {
    "objectID": "posts/2021-05-23-ajtfb-chapter-4.html#tensors",
    "href": "posts/2021-05-23-ajtfb-chapter-4.html#tensors",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 4: Stochastic Gradient Descent",
    "section": "Tensors",
    "text": "Tensors\nWhat is a “Tensor”?\nLike a numpy array, but with GPU support. The data it contains must be of the same type and must conform in rectangular shape.\n\n\n\n\n\n\nImportant\n\n\n\n“try to avoid as much as possible writing loops, and replace them by commands that work directly on arrays or tensors”\n\n\nLet’s take a look ..\n\nthrees = (mnist_path/'train/3').ls().sorted()\nlen(threes), threes[0]\n\n(6131, Path('/root/.fastai/data/mnist_sample/train/3/10.png'))\n\n\n\nall_threes = [ tensor(Image.open(fp)) for fp in threes ]\nlen(all_threes), all_threes[0].shape\n\n(6131, torch.Size([28, 28]))\n\n\n\nstacked_threes = torch.stack(all_threes).float()/255\nstacked_threes.shape\n\ntorch.Size([6131, 28, 28])\n\n\nImportant information about tensors include its shape, rank, and type:\n\n# shape = the length of each axis\nprint('shape: ', stacked_threes.shape)\n\n# rank = the total number of axes\nprint('rank: ', stacked_threes.ndim)\n\n# type = the datatype of its contents\nprint('type: ', stacked_threes.dtype)\n\nshape:  torch.Size([6131, 28, 28])\nrank:  3\ntype:  torch.float32\n\n\nImportant things you can do to a tensor, view, @, where\n\n# view = change the shape of a tensor without changing its contents\nstacked_threes_rank_2 = stacked_threes.view(-1, 28*28)\nprint('orig. shape: ', stacked_threes.shape)\nprint('make into a rank 2 tensor', stacked_threes_rank_2.shape)\n\n# @ = operator for matrix multiplication\nprint('result of matrix multiplication: ', (stacked_threes @ torch.randn((1,28,28))).shape)\n\n# where = torch.where(a,b,c) =&gt; [b[i] if a[i] else c[i] for i in range(len(a))] ... see p.167\ntrgs = tensor([1,0,1])\npreds = tensor([0.9, 0.4, 0.2])\n\ndef mnist_loss(preds, targs):\n  return torch.where(targs == 1, 1 - preds, preds).mean()\n\nprint('output of where: ', mnist_loss(preds, trgs))\n\norig. shape:  torch.Size([6131, 28, 28])\nmake into a rank 2 tensor torch.Size([6131, 784])\nresult of matrix multiplication:  torch.Size([6131, 28, 28])\noutput of where:  tensor(0.4333)\n\n\nFor an interactive lesson on matrix multiplication, this is the best!\nCheck out pp.145-148 to learn about “broadcasting”, a critical piece to understanding how you can and should manipulate tensors or numpy arrays!"
  },
  {
    "objectID": "posts/2021-05-23-ajtfb-chapter-4.html#stochastic-gradient-descent---how-to-train-a-model",
    "href": "posts/2021-05-23-ajtfb-chapter-4.html#stochastic-gradient-descent---how-to-train-a-model",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 4: Stochastic Gradient Descent",
    "section": "Stochastic Gradient Descent - How to train a model",
    "text": "Stochastic Gradient Descent - How to train a model\nHere are the steps:\n\nINITIALIZE the weights = initializing parameters to random values\nFor each image, PREDICT whether it is a 3 or 7\nBased on the predictions, calculate how good the model is by calculating its LOSS (small is good)\nCalculate the GRADIENT, “which measures for each weight how changing the weight would change the loss”\nSTEP, change all the weights based on the gradient\nStarting at step 2, REPEAT\nSTOP when you don’t want to train any longer or the model is good enough\n\nBelow, we’ll delve deeper into these steps. We’ll do this by getting a big more into the sample code beginning on p.150 …\n\nStep 1: Initializing weights\nOne way is presented on p.164:\ndef init_params(size, std=1.0) return (torch.randn(size)*std).requires_grad_()\n\nweights = init_params((28*28,1)) #=&gt; raturns a rank 2, 784x1 tensor, with random values\n\n\nStep 3: Calculating the loss\n\n\n\n\n\n\nImportant\n\n\n\n“For continuous data, it’s common to use mean squared error”. In order to understand how to write this, read it right-to-left (e.g., error -&gt; square -&gt; mean)\n\n\ndef mse(preds, targs): return ((preds-targs)**2).mean()\n\n# in PyTorch\nloss = F.mse_loss(preds, targs)\n\n\n\n\n\n\nImportant\n\n\n\nAccuracy is a bad loss function\n\n\nWhy is accuracy a poor loss function?\n\n“The gradient of a funciton is its slope, or its steepness … how much the value of the function goes up or down, divided by how much we changed the input (y_new - y_old) / (x_new - x_old) …. The problem with [accuracy] is that a small change in weights from x_old to x_new isn’t likely to cause any prediction to change, so (y_new - y_old) will almost always be 0 … **the gradient is 0 almost everywhere. A very small change in the value of a weight will often not change the accuracy at all\n\nA gradient = 0 will mean that the weights aren’t updated.\n\n\n\n\n\n\nImportant\n\n\n\n“We need a loss function that, when our weights result in slightly better predictions, gives us a slightly better loss”\n\n\nMetrics v. Loss\n\n\n\n\n\n\nImportant\n\n\n\n“… the metric is to drive human understanding and the loss is to drive automated learning.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“… focus on these metrics, rather than the loss, when judging the performance of a model.”\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“… the loss must be a function that has a meaningful derivative … must be reasonably smooth [so] that [it] would respond to small changes in confidence level.\n\n\nThe loss function is one that can be optimized using its gradient!\n\n\nStep 4: Calculating the gradients\n\n\n\n\n\n\nImportant\n\n\n\n“the gradients tell us how much we have to change each weight to make our model better … allows us to more quickly calculate whether our loss will go up or down we we make those adjustments”\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“The gradients tell us only the slope of our function; they don’t tell us exactly how far to adjust the parameters. But they do give us some idea of how far” (large slope = bigger adjustments needed whereas a small slope suggests we are close to the optimal value)\n\n\n“The derivative of a function tells you how much a change in its parameters will change its result”\nRemember: We are calculating a gradient for EVERY weight so we know how to adjust it to make our model better (i.e., lower the LOSS)\nrequires_grad tells PyTorch “that we want to calculate gradients with respect to that variable at that value”\n\ndef plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)):\n    x = torch.linspace(min,max)\n    fig,ax = plt.subplots(figsize=figsize)\n    ax.plot(x,f(x))\n    if tx is not None: ax.set_xlabel(tx)\n    if ty is not None: ax.set_ylabel(ty)\n    if title is not None: ax.set_title(title)\n\nHere we pretend that the below is our loss function. Running a number through it, our weight will produce a result, an activation … in this case, our loss (which again is a value telling us how good or bad our model is; smaller = good)\n\nxt = tensor(-1.5).requires_grad_(); xt\n\ntensor(-1.5000, requires_grad=True)\n\n\n\ndef f(x): return x**2\nloss = f(xt)\n\nplot_function(f, 'x', 'x**2')\nplt.scatter(xt.detach().numpy(), loss.detach().numpy(), color='red')\nprint('Loss: ', loss.item())\n\nLoss:  2.25\n\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Not providing a value for linspace's steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at  /pytorch/aten/src/ATen/native/RangeFactories.cpp:23.)\n  \n\n\n\n\n\n\n\n\n\nSo if our parameter is -1.5 we get a loss = 2.25. Since the direction of our slope is downward (negative), by changing its value to be a bit more positive, we get closer to achieving our goal of minimizing our loss\n\nxt = tensor(-1.).requires_grad_(); xt\n\nloss = f(xt)\n\nplot_function(f, 'x', 'x**2')\nplt.scatter(xt.detach().numpy(), loss.detach().numpy(), color='red')\nprint('Loss: ', loss.item())\n\nLoss:  1.0\n\n\n\n\n\n\n\n\n\nAnd yes, our loss has improved! If the direction of our slope were upwards (positive), we would conversely want x to be smaller.\nBUT now … imagine having to figure all this out for a million parameters. Obviously, we wouldn’t want to try doing this manually as we did before, and thanks to PyTorch, we don’t have too :)\nRemember that by utilizing the requires_grad_() function, we have told PyTorch to keep track of how to compute the gradients based on the other calucations we perform, like running it through our loss function above. Let’s see what that looks like.\n\nxt = tensor(-1.).requires_grad_(); \nprint(xt)\n\nloss = f(xt)\nprint(loss)\n\ntensor(-1., requires_grad=True)\ntensor(1., grad_fn=&lt;PowBackward0&gt;)\n\n\nThat &lt;PowBackward0&gt; is the gradient function it will use to calculate the gradients when needed. And when we need it, we call the backward method to do so.\n\nloss.backward()\nprint(xt.grad)\n\ntensor(-2.)\n\n\nAnd the calcuated gradient is exactly what we expected given that to calculate the derivate of x**2 is 2x … 2*-1 = -2.\nAgain, the gradient tells us the slope of our function. Here have a a negative/downward slope and so at the very least, we know what moving in that direction will get us closer to the minimum.\nThe question is now, How far do we move in that direction?\n\n\nStep 5: Change all the weights based on the gradient using a “Learning Rate”\nThe learning rate (or LR) is a number (usually a small number like 1e-3 or 0.1) that we multiply the gradient by to get a better parameter value. For a given parameter/weight w, the calculation looks like this:\nw -= w.grad * lr\nNotice we take the negative of the grad * lr operation because we want to move in the opposite direction.\n\n\n\n\n\n\nImportant\n\n\n\nWe do this in a with torch.no_grad() so that we don’t calculate the gradient for the gradient calculating operation\n\n\n\nlr = 0.01\n\nwith torch.no_grad():\n xt -= xt.grad * lr\n\n print('New value for xt: ', xt)\n print('New loss: ', f(xt))\n\nNew value for xt:  tensor(-0.9800, requires_grad=True)\nNew loss:  tensor(0.9604)\n\n\nYou can see the loss get smaller which is exactly what we want! “The magnitude of the gradient (i.e., the steepness of the slope) [tells] us how big a step to take.”\nThe above operation is also called the optimization step\nSee pp.156-157 for examples of what using a too small or too large LR might look like when training. This could help you troubleshoot things if yours looks wonky."
  },
  {
    "objectID": "posts/2021-05-23-ajtfb-chapter-4.html#datasets-dataloaders",
    "href": "posts/2021-05-23-ajtfb-chapter-4.html#datasets-dataloaders",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 4: Stochastic Gradient Descent",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders\nA Dataset contains tuples of independent and dependent variables\n\nds = L(enumerate(string.ascii_lowercase))\nds\n\n(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]\n\n\nA DataLoader receives a dataset and gives us back as many mini-batches are necessary based on the batch size we specify\n\ndl = DataLoader(ds, bs=6, shuffle=True)\nlist(dl)\n\n[(tensor([17,  5,  9, 22, 18, 21]), ('r', 'f', 'j', 'w', 's', 'v')),\n (tensor([15,  3, 23,  1,  0, 19]), ('p', 'd', 'x', 'b', 'a', 't')),\n (tensor([ 4, 10, 16, 25,  8,  2]), ('e', 'k', 'q', 'z', 'i', 'c')),\n (tensor([24, 14,  7, 20, 13, 12]), ('y', 'o', 'h', 'u', 'n', 'm')),\n (tensor([11,  6]), ('l', 'g'))]"
  },
  {
    "objectID": "posts/2021-05-23-ajtfb-chapter-4.html#measuring-distances",
    "href": "posts/2021-05-23-ajtfb-chapter-4.html#measuring-distances",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 4: Stochastic Gradient Descent",
    "section": "Measuring distances",
    "text": "Measuring distances\nSee pp.141-142. There are two main ways to measure distances.\nL1 norm (or mean absolute difference): Take the mean of the absolute value of differences\nl1_loss = (tensor_a - tensor_b).abs().mean()\nL2 norm (or root mean squared error, RMSE): Take the square root of the mean of the square differences. The squaring of differences makes everything positive and the square root undoes the squaring.\n\n\n\n\n\n\nImportant\n\n\n\n“… the latter will penalize bigger mistakes more heavily than the former (and be more lenient with small mistakes)”\n\n\nl2_loss = ((tensor_a - tensor_b) ** 2).sqrt()"
  },
  {
    "objectID": "posts/2021-05-23-ajtfb-chapter-4.html#important-pytorch-modules",
    "href": "posts/2021-05-23-ajtfb-chapter-4.html#important-pytorch-modules",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 4: Stochastic Gradient Descent",
    "section": "Important PyTorch Modules",
    "text": "Important PyTorch Modules\nA module is a class that inherits from PyTorch’s nn.Module class. “Every PyTorch module knows that parameters it has that can be trained.”\nHere are some key ones …\n\nnn.Linear\nInitializes its parameters and performs a linear operation. It contains both the weights and biases in a single class\n\nlin1 = nn.Linear(28*28, 1)\n\n# the trainable parameters\nweights, bias = lin1.parameters()\nprint(weights.shape, bias.shape)\n\ntorch.Size([1, 784]) torch.Size([1])\n\n\n\n\nnn.ReLU\nAllows us to add a non-linearity between linear classifiers. Simply put, it ensures that all activations passed to it are a positive number with every negative number replaced with a 0.\nNotice below that it has no trainable parameters!\n\nplot_function(F.relu)\n\n\n\n\n\n\n\n\n\nnon_lin1 = nn.ReLU()\nprint(list(non_lin1.parameters()))\n\nprint('using nn.ReLU: ', non_lin1(tensor(-1)), non_lin1(tensor(4)))\nprint('using max()', tensor(-1).max(tensor(0.0)), tensor(4).max(tensor(0.0)))\n\n[]\nusing nn.ReLU:  tensor(0) tensor(4)\nusing max() tensor(0.) tensor(4.)\n\n\nWhy do you want to have non-linearities?\n\nBecause “there’s no point in just putting one linear layout directly after another one, because when we multiply things together and then add them up multiple times, that could be replaced by multiplying different things together and adding them up just once …. BUT if we put a non-linear between them … this is no longer true. Now each linear layer is somewhat decoupled from the other ones and can do its own useful work.”\n\nThese kind of functions are also called “activation functions”, because the only operate and produce activations … there are no trainable parameters.\n\n\nnn.Sequential\nA module that can be passed modules, which when called, calls each of those layers in turn.\n\nlin2 = nn.Linear(1, 10)\nseq_model = nn.Sequential(lin1, non_lin1, lin2)\n\nseq_params = list(seq_model.parameters())\nprint(len(seq_params))\n\nfor p in seq_params: print(p.shape)\n\n4\ntorch.Size([1, 784])\ntorch.Size([1])\ntorch.Size([10, 1])\ntorch.Size([10])\n\n\nWhy 4? Simple, remember that each nn.Linear above has two trainable parameters (the weights and bias), 2+2 = 4."
  },
  {
    "objectID": "posts/2021-05-23-ajtfb-chapter-4.html#summary",
    "href": "posts/2021-05-23-ajtfb-chapter-4.html#summary",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 4: Stochastic Gradient Descent",
    "section": "Summary",
    "text": "Summary\nThis chapter walks you through creating a baseline model to a full blown training loop in PyTorch. Read it, and read it again and again! (I do and have).\nImportant Vocb/Concepts\nActivations: Numbers that are calculated by both linear and non-linear layers\nParameters: Randomly initialized parameters that can be trained.\nNeural Network: A chain of linear and non-linear functions your data runs through to produce a result.\nGradient: “The derivative of the loss with respect to some parameter of the model”\nBackpropagation: The computing of the gradients “of the loss with respect to all model parameters”\nGradient Descent: “Taking a step in the direction opposite to the gradients to make the model parameters a little bit better”"
  },
  {
    "objectID": "posts/2021-05-23-ajtfb-chapter-4.html#resources",
    "href": "posts/2021-05-23-ajtfb-chapter-4.html#resources",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 4: Stochastic Gradient Descent",
    "section": "Resources",
    "text": "Resources\n\nhttps://book.fast.ai - The book’s website; it’s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc…"
  },
  {
    "objectID": "posts/2020-11-16-ajtfb-chapter-2.html",
    "href": "posts/2020-11-16-ajtfb-chapter-2.html",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning",
    "section": "",
    "text": "import PIL\n\nfrom fastai.data.all import *\nfrom fastai.vision.all import *\nOther posts in this series:\nA Journey Through Fastbook (AJTFB) - Chapter 1\nA Journey Through Fastbook (AJTFB) - Chapter 3\nA Journey Through Fastbook (AJTFB) - Chapter 4\nA Journey Through Fastbook (AJTFB) - Chapter 5\nA Journey Through Fastbook (AJTFB) - Chapter 6a\nA Journey Through Fastbook (AJTFB) - Chapter 6b\nA Journey Through Fastbook (AJTFB) - Chapter 7\nA Journey Through Fastbook (AJTFB) - Chapter 8\nA Journey Through Fastbook (AJTFB) - Chapter 9"
  },
  {
    "objectID": "posts/2020-11-16-ajtfb-chapter-2.html#starting-your-project",
    "href": "posts/2020-11-16-ajtfb-chapter-2.html#starting-your-project",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning",
    "section": "Starting Your Project",
    "text": "Starting Your Project\n\nThings to think about when deciding on project feasibility\n“When selecting a project, the most important consideration is data availability.” If you don’t have enough quality data … good luck :)\n\n\n\n\n\n\nTip\n\n\n\nConsider that data augmentation can alleviate both the need for more manual labelling and also protect you from problems with out-of-domain data (e.g. when unexpected image types arise in the data when the model is being used in production) by synthetically creating more data likely to be seen that may not be in your dataset as is.\n\n\n\n\n\n\n\n\nNote\n\n\n\n“… iterate from end to end in your project; don’t spend months fine-tuning your model, or polishing the perfect GUI, or labeling the perfect dataset”\n\n\nThis is good advice for any software project …fail early and fail often. If you don’t, you’re likely to only uncover critical problems much later than you would have before, and even worse, you’re likely to not produce anything at all! In the world of deep learning there are a number of tools, that while helpful, can really get you so bogged down that you never deploy something usable (e.g., experiment tracking tools, hyperparameter optimization libraries, etc…).\nAlso, remember that getting something in production is a different task from winning a kaggle competition, where the later may require use of some of those aforementioned tools and the ensembling of dozens of models. For production, something better than human is often good enough to get out there and through refactoring, improve."
  },
  {
    "objectID": "posts/2020-11-16-ajtfb-chapter-2.html#the-drivetrain-approach",
    "href": "posts/2020-11-16-ajtfb-chapter-2.html#the-drivetrain-approach",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning",
    "section": "The Drivetrain Approach",
    "text": "The Drivetrain Approach\n\n\nFour Steps\n\nStep 1: Define your objective(s)\nIt’s amazing how in my 20+ years as a developer, how rare it is that a customer is able to clearly define what they want! In my experience, more than not, it is the developers that end up defining the goals. Not having a clear objective is likely to waste time, energy, and money to produce something that won’t even see the light of day. You can’t gauge the completion or quality of any software project without clear objective(s).\nEx.1: Show most relevant search results.\nEx.2: Drive additional sales by recommending to customers items to purchase they otherwise wouldn’t\n\n\nStep 2: What actions can you take to achieve those objective(s)?\nWhat things can make your goals a reality. Pretty simple.\nEx.1: Ranking the search results will help show the most relevants ones first.\nEx.2: Ranking the recommendations will help.\n\n\nStep 3: What data is needed to take those actions?\nIf you don’t have the data, you’ll need to get it … because the data pulls the levers which get you closer to your objective(s).\nEx.1: Seeing what how pages linked to other pages.\nEx.2: Collecting data on what customers purchased, what was recommended, and what they did with that info.\n\n\nStep 4: Build models\nOnly once you have the data and know what actions you want to be able to take based on the information within it, do you being modeling … first, defining what models you can even build with that data and second, what data you need to collect for models you can’t.\nEx.1: A model that takes the page relation data and predicts a ranking given a query.\nEx.2: Two models that predict the purchasing proabilities conditional on seeing or not seeing a recommendation."
  },
  {
    "objectID": "posts/2020-11-16-ajtfb-chapter-2.html#how-to-avoid-disaster",
    "href": "posts/2020-11-16-ajtfb-chapter-2.html#how-to-avoid-disaster",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning",
    "section": "How to Avoid Disaster",
    "text": "How to Avoid Disaster\n\n\n\n\n\n\nImportant\n\n\n\nYour model is only as good as the data it was trained on\n\n\nTwo problems to watch out for:\n\nout-of-domain data: “data that our model sees in production that is very different to what it saw during training.\ndomain shift: “whereby the type of data that our model sees changes over time.”\n\nMitigation steps:\n\n“Where possible, the first step is to use an entirely manual process with your model running in parallel and not being used to directly drive any actions.”\n“The second step is to try and limit the scope of the model.”\n“The third step is to gradually increase the scope of your rollout.”\n\n\n\n\n\n\nTip\n\n\n\n“Try to think about all the ways in which your system could go wrong, and then think about what measure or report or picture could reflect that problem, and ensure that your regular reporting includes that information.”\n\n\n\n\n\n\n\n\nNote\n\n\n\nDefining good validation and tests sets are part of the solution. See my “How to create good validation and test sets” for more details."
  },
  {
    "objectID": "posts/2020-11-16-ajtfb-chapter-2.html#getting-help",
    "href": "posts/2020-11-16-ajtfb-chapter-2.html#getting-help",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning",
    "section": "Getting help",
    "text": "Getting help\nA few of ways …\n\n# method signature only\ndownload_images?\n\n\n# full source of method\ndownload_images??\n\n\n# get link to fastai docs\ndoc(download_images)\n\nYou can also use pdb.set_trace (in code) or %debug(in a new cell following the one with the error) to step through your code. I use the former all the time … its a great way to debug and also learn what the code is doing and why. For example, I use it to look at the shape of things as the travel through and out of different layers in my NNs.\n\nimport pdb\ndef div_by_zero():\n  pdb.set_trace()\n  x = 1/0\n  print('here')\n\n# uncomment this to see what I'm talking about ...\n# div_by_zero()"
  },
  {
    "objectID": "posts/2020-11-16-ajtfb-chapter-2.html#getting-and-cleaning-your-images",
    "href": "posts/2020-11-16-ajtfb-chapter-2.html#getting-and-cleaning-your-images",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning",
    "section": "Getting and cleaning your images",
    "text": "Getting and cleaning your images\n\nUse download_images listed as URLs in a text file urls to download the actual images locally.\n\nGet the file path to the images via get_image_files in an L object.\n\nGet rid of the corrupt images using verify_images and Path.unlink.\n\npath = Path('bears/grizzly')\ndownload_images(path, urls=image_urls.txt)\n\nfile_paths = get_image_files(path)\nfailed = verify_images(file_paths)\nfailed.map(Path.unlink)\nNotice how L’s map method is used to apply the Path.unlink function to each item in-place."
  },
  {
    "objectID": "posts/2020-11-16-ajtfb-chapter-2.html#datablock-api-basics",
    "href": "posts/2020-11-16-ajtfb-chapter-2.html#datablock-api-basics",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning",
    "section": "DataBlock API Basics",
    "text": "DataBlock API Basics\nThe DataBlock API represents fastai’s high-level approach for building DataLoaders from your raw data sources. It is a reusable blueprint for how data is used both during model training and at inference time, and along with the fastai callback system, it represents one of the core pieces of the fastai framework.\n“… a DataBlock object … is like a template for creating a DataLoaders object”\n“A DataLoader is a class that provides batches of a few items at a time to the GPU”\n\nDefining your “blueprint” using the DataBlock API\nThere are four things you need to specify to make your data usable for training (e.g., to build at minimum a training and validation DataLoader).\n\nWhat kind of data you are working with\nHow to get the data\nHow to label the data\nHow to create a validation set\n\nHere’s an example of how this is done with the DataBlock API:\nd_block = DataBlock(\n  blocks=(ImageBlock, CategoryBlock),              #=&gt; our independent and dependent variable datatypes\n  get_items=get_image_files,                       #=&gt; how to get our data\n  splitter=RandomSplitter(valid_pct=0.2, seed=42), #=&gt; how to create the validation set\n  get_y=parent_label,                              #=&gt; how to label our data\n  item_tfms=Resize(128))                           #=&gt; code that runs against each item as it is fetched\n\n\n\n\n\n\nTip\n\n\n\nUse the seed argument to ensure you get the same training/validation set each time you run that code; else you won’t be able to know if, as you change hyperparameter values, your model performance changed because of those values and/or because of difference in your training/validation sets!\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo ensure reproducibility in your fastai training, follow the tips/tricks laid out in the Reproducibility: Where is the randomness coming in? forum post.\n\n\n\n\nUsing your “blueprint” to build your DataLoaders\nOnce you’ve defined your blueprint for how to get your modelable data (i.e., your DataLoaders), you need to pass it the “actual source” of your data, which can be a path or a DataFrame or whatever.\ndls = d_block.dataloaders(path)\n\n\n\n\n\n\nNote\n\n\n\nUse dls.show_batch(...) or dls.valid.show_batch(...) to visualize your training/validation data."
  },
  {
    "objectID": "posts/2020-11-16-ajtfb-chapter-2.html#transforms",
    "href": "posts/2020-11-16-ajtfb-chapter-2.html#transforms",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning",
    "section": "Transforms",
    "text": "Transforms\nThe DataBlock API relies heavily on the use of fastai transforms. They are used in the blocks you see above as well as inline, as you’ll see below.\n\nWhat is a “Transform”?\nA Transform contains code that is applied automatically during training.\n\n\nWhat kinds of transforms are there?\nThere are two kinds of transforms:\nItem Transforms: Applied to each individual item in your dataset, they are applied to an item from your dataset when it is fetched.\n\n\n\n\n\n\nNote\n\n\n\nUse the item_tfms argument to define your batch transforms. It is more technically correct to think of them as your after batch transforms since that is whey they are applied\n\n\nBatch Transforms: Applied to a batch of items using the GPU, they are applied to a collection of items on the GPU after they have been collated into the same shape.\n\n\n\n\n\n\nNote\n\n\n\nUse the batch_tfms argument to define your batch transforms. It is more technically correct to think of them as your after batch transforms since that is whey they are applied\n\n\nAn example:\nd_block = d_block.new(item_tfms=RandomResizedCrop(128, min_scale=0.3), batch_tfms=aug_transforms(mult=2))\n\n\n\n\n\n\nNote\n\n\n\naug_transforms are “a standard set of augmentations that we have found work pretty well”\n\n\n\n\nWhen should I use an item transform?\nTODO\n\n\nWhen should I use a batch transform?\n\nData augmentation\nData augmentation transorms (e.g., rotation, flipping, perspective warping, brightness changes, contrast changes, etc…) are defined as batch transforms and run on the GPU.\n\n\n\nTips & Tricks\n\n1. Changing your transforms without having to redefine your DataBlock from scratch\nYou can change the transforms in your DataBlock by reusing an existing DataBlock via d_block.new.\nd_block = d_block.new(item_tfms=Resize(128, ResizeMethod.squish))\ndls = d_block.dataloaders(path)\n...\nd_block = d_block.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeroes'))\ndls = d_block.dataloaders(path)\n..."
  },
  {
    "objectID": "posts/2020-11-16-ajtfb-chapter-2.html#how-to-train-an-image-classification-model-with-the-high-level-api",
    "href": "posts/2020-11-16-ajtfb-chapter-2.html#how-to-train-an-image-classification-model-with-the-high-level-api",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning",
    "section": "How to train an image classification model with the high-level API",
    "text": "How to train an image classification model with the high-level API\n\nStep 1: Get your data\nWe can grab all kinda of useful datasets via the fast.ai Datasets for various tasks and data types (e.g., images, text, etc…). In this example we’ll work with the Imagnette dataset, a “subset of 10 easily classified classes from Imagenet.”\n\n\n\n\n\n\nTip\n\n\n\nWorking with a representative subset of your full dataset is recommended for experimentation and as a means to verify your data prep and model training.\n\n\nWe’ll use untar_data to both download and decompress the dataset. It will return a Pathlib object pointing to where the data has been downloaded\n\nraw_data_path = untar_data(URLs.IMAGENETTE)\nprint(raw_data_path)\nraw_data_path.ls()\n\nWe can use get_image_files() to grab all the image filepaths in the training set. This method takes a path as an argument and recursively grabs all the images in that path by default.\nWe’ll need to know how to infer the class for each image and this can be done by looking at one or more of the actual image filepaths.\n\nfiles = get_image_files(raw_data_path/'train')\n\n\nfiles[0]\n\nHere we can see that the convention followed for this dataset is having the class Id we want to predict in its parent folder, while each image’s unique name is the class_id followed by a unique identifier: {class_id}/{class_id}_{unique_id}.JPEG.\nWe can validate this by ensure we see 10 parent folders …\n\n(raw_data_path/'train').ls()\n\nWe can even look at one or more of the images using the PIL package\n\nimg = PIL.Image.open(files[0])\nimg\n\n\n\nStep 2: Build your DataBlock\nThe objective here is to ultimately be able to build DataLoaders you can feed into your model. There are a variety of ways to do this but the recommended go to is to use the mid-level DataBlock API if you can. A DataBlock represents a blueprint for building DataLoaders from your raw data, whereas the DataLoaders are what allow us to feed our examples into our model a mini-batch (a few) at a time.\n\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),              # tell the block what are INPUTS/TARGETS are (images and a category/class here)\n    get_items=get_image_files,                       # tell the block HOW TO GET THE DATA (here its files but could also be rows in a .csv, etc...)\n    splitter=RandomSplitter(valid_pct=0.2, seed=42), # tell the block HOW TO BUILD THE VALIDATION SET (here we just randomly select 20% data)\n    get_y=parent_label,                              # tell the block WHERE TO GET OUR LABELS (here from the parent folder)\n    item_tfms=Resize(128)                            # tell the block things WE WANT DONE EACH TIME WE GRAB AN ITEM from the dataset\n)\n\nIf you’re unsure what any of these classes or methods do, don’t forget about the ?? syntax you can use in notebooks. For example …\n\nImageBlock??\n\n\n\n\n\n\n\nNote\n\n\n\nWe specify Resize(128) as an item transform because ultimately we’ll feed the data into our model a mini-batch at a time, and in order to take advantage of the GPU and tensor operations that items we feed in need to be the same size.\n\n\n“Item transforms are pieces of code that run on each individual item, whether it be an image, category, or so forth.”\n\ndls = dblock.dataloaders(raw_data_path)\n\nWe implement our blueprint by passing the path to the image files into the DataBlock.dataloaders() method. Whatever we pass in as an argument here, gets passed to the function we specified in get_items above (which isget_image_files in this case). While iterating over each image, get_y will be used to grab the label of the image and our two blocks, ImageBlock and CategoryBlock will provide both the pre and post-processing necessary to work with our images and classes. Finally, our splitter will randomly take 20% of the dataset and set it aside for our validation set.\n\n\n\n\n\n\nTip\n\n\n\nYou want to ensure you get the same validation set each time so you can meaningfully assess the performance of your model(s) as you tweak things. You do this by assigning a seed. If you don’t do this, you won’t know if your model’s performance is due to it seeing different images in the validation set or because of change you’ve made in your hyperparameters, model architecture, etc…\n\n\nEach time we grab an image, regardless of the size, from the dataset, we resize it as a 128x128 tensor.\n\ndls.show_batch(max_n=4, nrows=1)\n\nWe can use the DataBlock.new() method to modify only parts of our DataBlock defined above, and so create a new instance. Here for example, we can change how the Resize transform resizes images so that it randomly crops the image, keeping at least 30% of the image each time.\nBy default, this method crops the images to fit a square (but as you can see here we can also have fastai pad the images with zeroes, squish, or stretch them.).\n\ndblock2 = dblock.new(item_tfms=RandomResizedCrop(128, min_scale=0.2))\ndls = dblock2.dataloaders(raw_data_path)\n\n\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\n\n\nTip\n\n\n\nWhen dealing with data augmentation, its often helpful to see how a single example is augmented.\n\n\n\ndls.train.show_batch(max_n=4, nrows=1, unique=True)\n\nUsing RandomResizedCrop allows ” our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.”\n\n\n\n\n\n\nTip\n\n\n\n“…training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image.”\n\n\nWe can, and should, also use data augmentation to create “random variations” that are representative of what our model will see in the wild. “Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes and contrast changes.”\nFor “natural photos”, fastai provides the aug_tranforms() method that have proven to work well in general.\nIn addition, such “data augmentations” are tyically desirable to have run on the GPU since they’ll run much faster. To make this happen, these “transforms” are specified as batch_tfms since them happen on a “mini-batch” at a time.\n\ndblock2 = dblock.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = dblock2.dataloaders(raw_data_path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\nUsing the unique=True argument with our DataLoaders.show_batch() method allows us to see how these augmentations are applied to a single image.\n\n\n\n\n\n\nTip\n\n\n\nVerify that the augmentations you use are representative of what your model will see in the wild\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse augmentations to artificially provide more images than you do in your raw dataset\n\n\n\n\nStep 3: Train your model\n\ndblock = dblock.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms()\n)\n\ndls = dblock.dataloaders(raw_data_path)\n\n\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\nTip\n\n\n\n“It’s helpful to see where exactly our errors are occurring, to see whether they’re due to a dataset problem (e.g., images that aren’t bears at all, or are labeled incorrectly, etc.), or a model problem (perhaps it isn’t handling images taken with unusual lighting, or from a different angle, etc.). To do this, we can sort our images by their loss.”\n\n\nFor each image shown, ClassificationInterpretation.plot_top_losses() shows four things: the predicted class, the actual class, the loss, and the probability\n\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\n\n\n\nTip\n\n\n\n” a model can actually help you find data issues more quickly and easily. So, we normally prefer to train a quick and simple model first, and then use it to help us with data cleaning.”\n\n\nfastai includes the ImageClassifierCleaner that can be used to “clean your dataset” (remove or re-label images). See chapter 2 for more information on how to use this utility.\n\n\n\nStep 4: Inference\n“Remember that a model consists of two parts: the architecture and the trained parameters. The easiest way to save the model is to save both of these, because that way when you load a model you can be sure that you have the matching architecture and parameters.”\nHow do we do this? We use Learner.export().\nBy using this method we don’t have to redefine how the data needs to be transformed as the inference learner will know what transforms to apply based on what was applied to your validation DataLoader. For example, it will know not to apply any data augmentation which is only really useful during training.\n\nlearn.export()\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n\nlearn_inf = load_learner(path/'export.pkl')\nlearn_inf.dls.vocab\n\nCalling Learner.predict() will return the predicated label, the label index in the vocab, and the probabilities assigned to each of our 10 labels.\n\nlearn_inf.predict(files[0])"
  },
  {
    "objectID": "posts/2020-11-16-ajtfb-chapter-2.html#inference",
    "href": "posts/2020-11-16-ajtfb-chapter-2.html#inference",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning",
    "section": "Inference",
    "text": "Inference\nInference is about how you used your trained model to get predictions on new data. It is often structured to perform in real-time on a single (or at least a small set of data) item or in the background where large quantities of data can be processed together in batches. For the former, we can use fastai’s Learner.predict() method while for the later we can use either fastai’s Learner.get_preds() or write our own inference loop using PyTorch.\n\nexport() and predict()\n“a model consists of two parts: the architecture and the trained parameters.” You can use it just like any other function\n# saves the architecture, the trained parameters, and the definintion of how to create your DataLoaders\nlearn.export() \n\n\n\n\n\n\nNote\n\n\n\nfastai … uses your validation set DataLoader for inference by default, so your data augmentation will not be applied.\n\n\ninf_learn = load_learner(path/'export.pkl')\ninf_learn.predict('images/grizzly.jpg')\ninf_learn.dls.vocab # =&gt; To view possible classification categories/labels\nFor options on how to deploy your app, see the Deployment section in the course website. I personally like to use FastAPI and there is a good starter template here for that."
  },
  {
    "objectID": "posts/2020-11-16-ajtfb-chapter-2.html#resources",
    "href": "posts/2020-11-16-ajtfb-chapter-2.html#resources",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 2: Doing Deep Learning",
    "section": "Resources",
    "text": "Resources\n\nhttps://book.fast.ai - The book’s website; it’s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc…\nhttps://docs.fast.ai/ - The library’s documentation; includes tutorials and other tips for development.\nhttps://forums.fast.ai/ - If you’re not part of the community yet, you should be. Before posting a question, search to see if it has been answered already (95% of the time, it has)."
  },
  {
    "objectID": "posts/2020-11-22-ajtfb-chapter-3.html",
    "href": "posts/2020-11-22-ajtfb-chapter-3.html",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 3: Data Ethics",
    "section": "",
    "text": "Other posts in this series:\nA Journey Through Fastbook (AJTFB) - Chapter 1\nA Journey Through Fastbook (AJTFB) - Chapter 2\nA Journey Through Fastbook (AJTFB) - Chapter 4\nA Journey Through Fastbook (AJTFB) - Chapter 5\nA Journey Through Fastbook (AJTFB) - Chapter 6a\nA Journey Through Fastbook (AJTFB) - Chapter 6b\nA Journey Through Fastbook (AJTFB) - Chapter 7\nA Journey Through Fastbook (AJTFB) - Chapter 8\nA Journey Through Fastbook (AJTFB) - Chapter 9"
  },
  {
    "objectID": "posts/2020-11-22-ajtfb-chapter-3.html#data-ethics",
    "href": "posts/2020-11-22-ajtfb-chapter-3.html#data-ethics",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 3: Data Ethics",
    "section": "Data Ethics",
    "text": "Data Ethics\n\n… sometimes machine learning models can go wrong. They can have bugs. They can be presented with data that they haven’t seen before and behave in ways we don’t expect. Or … they can be used for something that we would much prefer they were never, ever used for.\n\n\n… no one really agrees on what right and wrong are, whether they exist, how to spot them, which people are good and which bad, or pretty much anything else.\n\nIf anything, this is a call to humility, self-examination, and thoughtful dialog. Though we are increasingly living in a polarized world where one is judged by what particular slogans they choose, what party they belong too, who they follow on Facebook, and so on, we have the choice to not be such human beings. But in my experience that is easier said than done. It’s too easy to shout at, rather than talk with, the “other” side because we can in blissful ignorance continue believing we got it right without ever being challenged. It’s hard to really reason out our world views and argue with those who don’t agree. If we did, we have find we have more in common that imagined and make further progress as a society into figuring these things out … things like right and wrong, good and evil, justice and injustice, and how we can get along with each other despite our differences.\nThe point of this chapter is simple: The goal of ML isn’t to find the model with the lowest loss … it is to build a model that drives the right kind of actions"
  },
  {
    "objectID": "posts/2020-11-22-ajtfb-chapter-3.html#recourse-and-accountability",
    "href": "posts/2020-11-22-ajtfb-chapter-3.html#recourse-and-accountability",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 3: Data Ethics",
    "section": "Recourse and accountability",
    "text": "Recourse and accountability\n\nIn a complex system, it is easy for no one person to feel responsible for outcomes.\n\nAs deep learning practioniers, we have better insight than most into what kind of actions will be made as a result of our model’s results. Therefore, if we care about people in general, we’ll care about those outcomes as much as our model’s validation loss."
  },
  {
    "objectID": "posts/2020-11-22-ajtfb-chapter-3.html#feedback-loops",
    "href": "posts/2020-11-22-ajtfb-chapter-3.html#feedback-loops",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 3: Data Ethics",
    "section": "Feedback Loops",
    "text": "Feedback Loops\n\nFeedback loops can occur when your model is controlling the next round of data you get.\n\n\n… an algorithm can interact with its environment to create a feedback loop, making predictions that reinforce actions taken in the real world, which lead to predictions even more pronounced in the same direction.\n\n\nPart of the problem here is the centrality of metrics in driving a financially important system.\n\nSee the “Meetup” example on p.105\n\nOnce people join a single conspiracy-minded [Facebook] group, they are algorithmically routed to a plethora of others. Join an anti-vaccine group, and your suggestions will include anit-GMO, chemtrail watch, falt Earther (yes, really), and “curing cancer naturallay” groups. Rather than pulling a user out of the rabbit hole, the recommendation engine pushes them farther in.\n\nFYI, I think most social media has a net-negative effect on us as humain beings. In particular, I try to avoid Facebook, Instagram, TikTok, and Snapchat while doing my best to limit my only social media account, a Twitter account, to things relevant to data science and public health (and that ain’t easy)."
  },
  {
    "objectID": "posts/2020-11-22-ajtfb-chapter-3.html#bias",
    "href": "posts/2020-11-22-ajtfb-chapter-3.html#bias",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 3: Data Ethics",
    "section": "Bias",
    "text": "Bias\nThere are different kinds of “data ethics” bias, here are 4 types:\n\nHistorical Bias\n\n… comes from the fact that people are biased, processes are biased, and society is biased. [It] is a fundamental, structural issue with the first step of data generation process and can exist even given perfect sampling and feature selection.\n\n\nAny dataset involving human can have this kind of bias: medical data, sales data, housing data, political data, and so on.\n\n\n\n\n\n\n\nImportant\n\n\n\nMaybe the best way to understand historical biase in your dataset is by spending time looking at both the outcomes and how they might be used???\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure your data is representative of what your model will see and to evaluate any automatic “labeling” features in your system. (see gorillas example on pp.107-108).\n\n\n\nSo what this showed is that the developers failed to utilize datasets containing enough darker faces, or test their product with darker faces.\n\nA good reminder that your model will only be as good as the data you trained it on! Sound familiar?\n\n… the vast majority of AI researches and developers are young white men. Most projects that we have seen do most user testing using friends and families of the immediate product development group. Given this, the kinds of problems we just discussed should not be suprising.\n\nI think at the very least, we need to be forthright about our dataset as much as on model performance. That way, expectations can be managed and a confidence level assigned to the results. A threshold perhaps that could trigger human intervention.\n\n\nMeasurement bias\n\n… occurs when our models make mistakes because we are measuring the wrong thing, or measuring it the wrong way, or incorporating that measurement into the model inappropriately.\n\nNot sure why, but this is perhaps the most insidious bias because I think its the hardest to figure out.\n\n\nAggregation bias\n\n… occurs when models do not aggregate data in a way that incorporates all of the appropriate factors, or when a model does not include the necessary interaction terms, nonlineraities, or so forth.\n\nThese are features that are not included though they would actually improve model performance if they were.\n\n\nRepresentation bias\n\nWhen there is a clear, easy-to-see underlying relationship, a simple model will often assume that this relationship holds all the time.\n\nEssentially models can see this real imbalance and make it bigger than it is."
  },
  {
    "objectID": "posts/2020-11-22-ajtfb-chapter-3.html#disinformation",
    "href": "posts/2020-11-22-ajtfb-chapter-3.html#disinformation",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 3: Data Ethics",
    "section": "Disinformation",
    "text": "Disinformation\n\nIt is not necessarily about getting someone to belive something false, but rather often used to sow disharmony and uncertainty, and to get people to give up on seeking the truth. Receiving conflicting accounts can lead people to assume that they can never know whom or what to trust.\n\nDisinformation will unfotunately be one of the greatest legacies of President Trump. A step backwards for American society. A culture that will back if you if you tell them what they want to hear, even if you’re a compulsive liar and base your statements on “gut feel” rather than facts and logic.\n\nWhile most of us like to think of ourselves as independent-minded, in reality we evolved to be influenced by others in our in-group, and in opposition to those in our out-group. Online discussions can influence our viewpoints, or alter the range of what we consider acceptable viewpoints. Humans are social animals, and as social animals, we are extremely influenced by the people around us. Increasingly, radicalization occurs in online environments; so influence is coming from people in the virtual space of online forums and social networks.\n\nThe biggest take here is that I am not as independently minded as I think I am. Knowing thyself is perhaps the best preventative of being swallowed up by disinformation. Limiting social media is another.\n\nDisinformation through autogenerated text is a particularly significant issue\n\nAs an NLP guy, this one scares me since part of my work is to summarize text. Knowing this, the first step I’ve taken is to let all business owners know the risk of text generation algorithms generating text that is either false and/or not necessarily reflective of the inputs, as in the case of abstract summarization. The second step I took was to introduce human beings into the process and a workflow that has them look at at least the most potentially wrong summarizations before reports go out."
  },
  {
    "objectID": "posts/2020-11-22-ajtfb-chapter-3.html#what-to-do",
    "href": "posts/2020-11-22-ajtfb-chapter-3.html#what-to-do",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 3: Data Ethics",
    "section": "What to do???",
    "text": "What to do???\n\nYou must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.\n\nData use and storage are things you need to think about.\nI think these are good questions to ask/answer in any project to ensure good outcomes:\n\n\nWhose interests, desires, skills, experiences, and values have we simply assumed rather than actually consulted?\nWho are all the stakeholders who will be directly affected by our product? How have their interests been protected? How do we know what their interests really are - have we asked?\nWhowhich groups and individuals will be indirectly affected in signficant ways?\nWho might use this product that we didn’t expect to use it, or for purposes we didn’t initially intend?\n\n\nSee pp.119-120 for a bunch of good questions to put into your practice!\n\nWhen everybody on a team has similar backgrounds, they are likely to have similar blind spots around ethical tasks.\n\n\n… first come up with a process, definition, set of questions etc., which is designed to resolve a problem. Then try to come up with an example in which the apparent solution results in a proposal that no one would consider acceptable. This can then lead to further refinement of the solution.\n\nThinking about all these things may lead one to analysis paralysis or even worse, complete apathy. We need to start with something and be okay with criticism and refactoring. Additionally, we need to be thoughtful in even spot on criticism of others’ systems. I don’t think most folks try to make something racist or mysoginistic or whatever, so instead of calling them a “Hitler” on Twitter when we taste something that looks to us like fasicism, maybe a phone call and one-on-one chat is the better and more productive move."
  },
  {
    "objectID": "posts/2020-11-22-ajtfb-chapter-3.html#resources",
    "href": "posts/2020-11-22-ajtfb-chapter-3.html#resources",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 3: Data Ethics",
    "section": "Resources",
    "text": "Resources\n\nhttps://book.fast.ai - The book’s website; it’s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc…\nhttps://forums.fast.ai/c/data-ethics/47 - Forum subcategory for all things “data ethics”."
  },
  {
    "objectID": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html",
    "href": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Multilabel Classification",
    "section": "",
    "text": "Other posts in this series:\nA Journey Through Fastbook (AJTFB) - Chapter 1\nA Journey Through Fastbook (AJTFB) - Chapter 2\nA Journey Through Fastbook (AJTFB) - Chapter 3\nA Journey Through Fastbook (AJTFB) - Chapter 4\nA Journey Through Fastbook (AJTFB) - Chapter 5\nA Journey Through Fastbook (AJTFB) - Chapter 6b\nA Journey Through Fastbook (AJTFB) - Chapter 7\nA Journey Through Fastbook (AJTFB) - Chapter 8\nA Journey Through Fastbook (AJTFB) - Chapter 9"
  },
  {
    "objectID": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html#multiclass-vs-multi-label-classification-again",
    "href": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html#multiclass-vs-multi-label-classification-again",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Multilabel Classification",
    "section": "Multiclass vs Multi-label classification (again)…",
    "text": "Multiclass vs Multi-label classification (again)…\nLast post we saw that multiclass classification is all about predicting a SINGLE CLASS an object belongs to from a list of two or more classes. It’s the go to task if we’re confident that every image our model sees is going to be one of these classes. Cross-entropy loss is our go to loss function as it wants to confidently pick one thing.\nMulti-label classification involves predicting MULTIPLE CLASSES to which an object belongs; it can belong to one, some, all, or even none of those classes. For example, you may be looking at satellite photos from which you need to predict the different kinds of terrain (your classes) each contains.\n\n\n\n\n\n\nImportant\n\n\n\nUse the multi-label classification approach in your “multiclassification problems” where you want your model to be able to result in None (which is probably a more common real world use case)"
  },
  {
    "objectID": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html#defining-your-datablock",
    "href": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html#defining-your-datablock",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Multilabel Classification",
    "section": "Defining your DataBlock",
    "text": "Defining your DataBlock\nAgain, the DataBlock is a blueprint for everything required to turn your raw data (images and labels) into something that can be fed through a neural network (DataLoaders with a numerical representation of both your images and labels). Below is the one presented in this chapter.\n\nfrom fastai.vision.all import *\npath = untar_data(URLs.PASCAL_2007)\n\n\n\n\n\n\n    \n      \n      100.00% [1637801984/1637796771 01:41&lt;00:00]\n    \n    \n\n\nInstead of working with the filesystem structure to get our images and define our labels, in this example we use a .csv file that we can explore and manipulate further via a pandas DataFrame.\n\n\n\n\n\n\nImportant\n\n\n\nDataFrames are one of those things you’re going to want to get comfortable using. Personally, I love using them if for nothing else, for how ubiquitous they are. You can create them from .csv files, excel files, dictionaries, from a sql database, and so forth. This makes them a fabulous datasource around which to build your DataBlocks!\n\n\nHere are some of my favorite pandas resources: 1. https://chrisalbon.com/ 2. https://pandas.pydata.org/docs/ (yah, the docs are really good!)\n\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nfname\nlabels\nis_valid\n\n\n\n\n0\n000005.jpg\nchair\nTrue\n\n\n1\n000007.jpg\ncar\nTrue\n\n\n2\n000009.jpg\nhorse person\nTrue\n\n\n3\n000012.jpg\ncar\nFalse\n\n\n4\n000016.jpg\nbicycle\nTrue\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\n\ndef splitter(df):\n  train = df.index[~df['is_valid']].tolist()\n  valid = df.index[df['is_valid']].tolist()\n  return train, valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), \n                   get_x=get_x, \n                   get_y=get_y,\n                   splitter=splitter, # or could just have used ColSplitter()   \n                   item_tfms=RandomResizedCrop(128, min_scale=0.35))\n\nLet’s break down our blueprint!\n\nDefine the data types for our inputs and targets via the blocks argument.\n\nThis is defined as a tuple, where we tell our DataBlock that the imputs are images and our targets are multiple potential categories. Above we can see that these labels are space delimited in the “labels” column. The later essentially returns a one-hot encoded list of possible labels, with a 0 indicating that the label wasn’t found for the item and a 1 indicating otherwise (see the DataBlock.summary results below).\n\nDefine how we’re going to get our images via get_x.\n\nAs we’ll be passing in a Dataframe as the raw data source, we don’t need to define a get_items to pull the raw data. We do however, need to instruct the DataBlock as to how to find the images, which we do via the get_x method. That method will get one row of DataFrame (r) at a time.\n\nDefine how, from the raw data, we’re going to create our labels via get_y.\n\nAs already mentioned, the classes are in the “labels” column and delimited by a space, and so, we return a list of labels splitting on ’ ’. Easy peasy.\n\nDefine how we’re going to create our validation dataset via splitter\n\nHere we define a custom splitter mostly to just show you how to do it. It has to return at least a tuple of train, validation data. We could have just used ColSplitter (see it in the docs here)\n\nDefine things we want to do for each item via item_tfms\n\nitem_tfms are transforms, or things we want to do, to each input individually! Above we only have one which says, “Randomly crop the image to be 128x128 that captures at least 35% of the image each time you grab an image”. See here for more info on RandomResizedCrop\n\nDefine things we want to do for each mini-batch of items via batch_tfms\n\nNone here, but remember that these are transforms you want applied to a mini-batch of images on the GPU at the same time.\n\n\n\n\n\n\nImportant\n\n\n\nDo not use lambda functions for defining your DataBlock methods! They can’t be serialized and so you’re lucky to get some errors when you try to save/export your DataLoaders and/or Learner\n\n\n\n\n\n\n\n\nImportant\n\n\n\nVerify your DataBlock works as expected, or else troubleshoot it, by running DataBlock.summary(data)\n\n\n\ndblock.summary(df)\n\nSetting-up type transforms pipelines\nCollecting items from            fname          labels  is_valid\n0     000005.jpg           chair      True\n1     000007.jpg             car      True\n2     000009.jpg    horse person      True\n3     000012.jpg             car     False\n4     000016.jpg         bicycle      True\n...          ...             ...       ...\n5006  009954.jpg    horse person      True\n5007  009955.jpg            boat      True\n5008  009958.jpg  person bicycle      True\n5009  009959.jpg             car     False\n5010  009961.jpg             dog     False\n\n[5011 rows x 3 columns]\nFound 5011 items\n2 datasets of sizes 2501,2510\nSetting up Pipeline: get_x -&gt; PILBase.create\nSetting up Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\n\nBuilding one sample\n  Pipeline: get_x -&gt; PILBase.create\n    starting from\n      fname       000012.jpg\nlabels             car\nis_valid         False\nName: 3, dtype: object\n    applying get_x gives\n      /root/.fastai/data/pascal_2007/train/000012.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=500x333\n  Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\n    starting from\n      fname       000012.jpg\nlabels             car\nis_valid         False\nName: 3, dtype: object\n    applying get_y gives\n      [car]\n    applying MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorMultiCategory([6])\n    applying OneHotEncode -- {'c': None} gives\n      TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.])\n\nFinal sample: (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.]))\n\n\nCollecting items from            fname          labels  is_valid\n0     000005.jpg           chair      True\n1     000007.jpg             car      True\n2     000009.jpg    horse person      True\n3     000012.jpg             car     False\n4     000016.jpg         bicycle      True\n...          ...             ...       ...\n5006  009954.jpg    horse person      True\n5007  009955.jpg            boat      True\n5008  009958.jpg  person bicycle      True\n5009  009959.jpg             car     False\n5010  009961.jpg             dog     False\n\n[5011 rows x 3 columns]\nFound 5011 items\n2 datasets of sizes 2501,2510\nSetting up Pipeline: get_x -&gt; PILBase.create\nSetting up Pipeline: get_y -&gt; MultiCategorize -- {'vocab': None, 'sort': True, 'add_na': False} -&gt; OneHotEncode -- {'c': None}\nSetting up after_item: Pipeline: RandomResizedCrop -- {'size': (128, 128), 'min_scale': 0.35, 'ratio': (0.75, 1.3333333333333333), 'resamples': (2, 0), 'val_xtra': 0.14, 'max_scale': 1.0, 'p': 1.0} -&gt; ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: RandomResizedCrop -- {'size': (128, 128), 'min_scale': 0.35, 'ratio': (0.75, 1.3333333333333333), 'resamples': (2, 0), 'val_xtra': 0.14, 'max_scale': 1.0, 'p': 1.0} -&gt; ToTensor\n    starting from\n      (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.]))\n    applying RandomResizedCrop -- {'size': (128, 128), 'min_scale': 0.35, 'ratio': (0.75, 1.3333333333333333), 'resamples': (2, 0), 'val_xtra': 0.14, 'max_scale': 1.0, 'p': 1.0} gives\n      (PILImage mode=RGB size=128x128, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.]))\n    applying ToTensor gives\n      (TensorImage of size 3x128x128, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.]))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n    starting from\n      (TensorImage of size 4x3x128x128, TensorMultiCategory of size 4x20)\n    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n      (TensorImage of size 4x3x128x128, TensorMultiCategory of size 4x20)\n\n\nNow we can create our DataLoaders and take a look at our x’s and y’s, our images and their labels (multiple labeled images have their labels separated by semi-colon)\n\ndls = dblock.dataloaders(df)\ndls.show_batch()\n\n\n\n\n\n\n\n\nTo get a feel for what our item_tfms (and batch_tfms if we had them) are doing, we can show_batch using a single image as we do below.\n\ndls.show_batch(unique=True)\n\n\n\n\n\n\n\n\nThe combination of what we’re doing in the item_tfms and batch_tfms is known as presizing.\n“Presizing is a particular way to do image augmentation that is designed to minimize data destruction while maintaining good performance.” After resizing all the images to a larger dimension that we will train on, we perform all our core augmentations on the GPU. This results in both faster and less destructive transformations of the data.\n\n\n\n\n\n\nImportant\n\n\n\nSee pp190-191 for how these augmentations are applied to the training and validation set!"
  },
  {
    "objectID": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html#train-a-model",
    "href": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html#train-a-model",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Multilabel Classification",
    "section": "Train a model",
    "text": "Train a model\n\n\n\n\n\n\nImportant\n\n\n\n“Once you think your data looks right, we generally recommend the next step should be using it to train a simple model” See bottom of p193 for why.\n\n\n\nDefine your loss function\nTo train a model we need a good loss function that will allow us to optimize the parameters of our model. For multi-label classification tasks where we want to predict a single class/label, to go to is binary cross-entropy loss\n\n\n\n\n\n\nImportant\n\n\n\nCross-entropy loss is the U.S. Marine of loss functions … “the few, the proud, the one hot encoded”\n\n\nWhy can’t we just use cross-entropy loss?\nBecause “the softmax function really wants to pick one class” whereas here want it to pick multiple or even none.\n“softmax … requires that all predictions sum to 1, and tends to push one activation to be much larger than the others (because of the use of exp) … we may want the sum to be less than 1, if we don’t think any of the categories appear in an image.”\n“nll_loss … returns the value of just one activation: the single activation corresponding with the single label for an item [which] doesn’t make sense when we have multiple labels”\n\nlearn = cnn_learner(dls, resnet18)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRun a batch through model to see/verify your batches and final activations look right.\n\n\n\nxb, yb = to_cpu(dls.train.one_batch())\nres = learn.model(xb)\nxb.shape, yb[0], res.shape, res[0]\n\n(torch.Size([64, 3, 128, 128]),\n TensorMultiCategory([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n         0., 0.]),\n torch.Size([64, 20]),\n TensorBase([ 1.4774, -2.1223,  4.1004,  1.5169, -0.5184,  0.1914, -1.0933, -1.4870,\n         -1.4855, -0.9137, -2.1899,  2.5738,  2.6155, -1.8979,  0.9340,  1.2691,\n         -0.2225,  0.4355, -4.2410, -2.5808], grad_fn=&lt;AliasBackward0&gt;))\n\n\nSo now we need a loss function that will scale those activations to be between 1 and 0 and then compare each activation with the value (0 or 1) in each target column.\n\ndef bce(inputs, targets):\n  inputs = inputs.sigmoid()\n  return -torch.where(targets==1, inputs, 1-inputs).log().mean()\n\nprint(bce(res, yb))\n\nTensorMultiCategory(1.0537, grad_fn=&lt;AliasBackward0&gt;)\n\n\nSo breaking the above down, line by line, for a single input/targets …\n\ninps = res.sigmoid()\nprint(f'1. {inps[0]}')\nprint(f'2. {yb[0]}')\nprint(f'3. {torch.where(yb==1, inps, 1-inps)[0]}')\nprint(f'4. {torch.where(yb==1, inps, 1-inps)[0].log()}')\nprint(f'5. {torch.where(yb==1, inps, 1-inps)[0].log().mean()}')\nprint(f'6. {-torch.where(yb==1, inps, 1-inps)[0].log().mean()}')\n\n1. TensorBase([0.8142, 0.1070, 0.9837, 0.8201, 0.3732, 0.5477, 0.2510, 0.1844, 0.1846,\n        0.2862, 0.1007, 0.9292, 0.9319, 0.1303, 0.7179, 0.7806, 0.4446, 0.6072,\n        0.0142, 0.0704], grad_fn=&lt;AliasBackward0&gt;)\n2. TensorMultiCategory([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n        0., 0.])\n3. TensorMultiCategory([0.1858, 0.8930, 0.0163, 0.1799, 0.3732, 0.4523, 0.7490, 0.8156, 0.8154,\n        0.7138, 0.8993, 0.0708, 0.0681, 0.8697, 0.7179, 0.7806, 0.5554, 0.3928,\n        0.9858, 0.9296], grad_fn=&lt;AliasBackward0&gt;)\n4. TensorMultiCategory([-1.6830, -0.1131, -4.1168, -1.7153, -0.9856, -0.7934, -0.2890, -0.2038,\n        -0.2041, -0.3372, -0.1061, -2.6473, -2.6861, -0.1397, -0.3314, -0.2477,\n        -0.5881, -0.9344, -0.0143, -0.0730], grad_fn=&lt;AliasBackward0&gt;)\n5. -0.9104629755020142\n6. 0.9104629755020142\n\n\n… what is binary cross-entropy loss doing?\nScale all activations to be between 0 and 1 using the sigmoid function (1). The resulting activations tell us, for each potential label, how confident the model is that the value is a “1”.\nBuild a tensor with a value for each target (2); if the target = 1 then use the corresponding scaled value above … if the target = 0, then use 1 minus this value (3). Notice how confident correct predictions will be very large, while confident incorrect predictions will be very small. We can think of this value as telling us how right the model was in predicting each label.\nTake the log (4) which will will turn correct and more confident predictions (those closer to 1) to a value closer to zero, and wrong and more confident prediction to a value closer to 0. This exactly what we want since the better the model, the smaller the lost, and the log(1) = 0 where as the log(0) approaches negative infinity! See the chart below.\nLastly, because the loss has to be a single value, we mean the losses for each label (5), and then turn it into a positive (6).\n\nplot_function(torch.log, 'x (prob correct class)', '-log(x)', title='Negative Log-Likelihood', min=0, max=1)\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Not providing a value for linspace's steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at  ../aten/src/ATen/native/RangeFactories.cpp:23.)\n  This is separate from the ipykernel package so we can avoid doing imports until\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nReview BCE and Cross Entropy Loss until you can explain it to your significant other :). They are by and far the most common loss functions you’ll come across and many of the problems you encounter in training your models will be because you’re using the wrong one.\n\n\nFortunately, PyTorch has a function and module we can use:\n\n# functional form\nloss = F.binary_cross_entropy_with_logits(res, yb)\nprint(loss)\n\n# modular form (most commonly used)\nloss_func = nn.BCEWithLogitsLoss()\nloss = loss_func(res, yb)\nprint(loss)\n\n# and for shits and giggles\nprint(bce(res, yb))\n\nTensorMultiCategory(1.0537, grad_fn=&lt;AliasBackward0&gt;)\nTensorMultiCategory(1.0537, grad_fn=&lt;AliasBackward0&gt;)\nTensorMultiCategory(1.0537, grad_fn=&lt;AliasBackward0&gt;)\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“Normally, for one-hot-encoded targets, you’ll want F.binary_cross_entropy_with_logits (or nn.BCEWithLogitsLoss), which do both sigmoid and binary cross entropy in a single function.\n\n\nIf the final activations already have the sigmoid applied to it, then you’d use F.binary_cross_entropy (or nn.BCELoss).\n\n\nDefine Your Metrics and Thresholds\nOne of the trickier bits with multilabel tasks, is selecting at what threshold (probability) do we want to consider a label 1 or 0. Using accuracy as our metric, we can play around with different values …\n\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.939569\n0.704923\n0.230299\n00:41\n\n\n1\n0.820619\n0.558957\n0.290279\n00:38\n\n\n2\n0.602579\n0.195408\n0.835618\n00:38\n\n\n3\n0.358107\n0.122511\n0.945438\n00:38\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy_multi\ntime\n\n\n\n\n0\n0.131727\n0.116799\n0.945359\n00:41\n\n\n1\n0.116440\n0.109859\n0.949861\n00:40\n\n\n2\n0.097460\n0.103571\n0.954841\n00:40\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe didn’t have to specify BCE as our loss function above because fastai was smart enough to figure it out from the dataset. This isn’t always the case, in particular when you start building your own or use 3rd party models for training. So Trainer beware!\n\n\n\nlearn.metrics = partial(accuracy_multi, thresh=0.2)\nprint(learn.validate())\n\nlearn.metrics = partial(accuracy_multi, thresh=0.9)\nprint(learn.validate())\n\nlearn.metrics = partial(accuracy_multi, thresh=0.75)\nprint(learn.validate())\n\n\n\n\n\n\n\n\n[0.10357054322957993, 0.9548406004905701]\n\n\n\n\n\n\n\n\n\n[0.10357054322957993, 0.9559162855148315]\n\n\n\n\n\n\n\n\n\n[0.10357054322957993, 0.9615935683250427]\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“This is much faster if we grab the predictions just once …. Note that by default get_preds applies the output activat function (sigmoid, in this case) for us, so we’ll need to tell accuracy_multi to not apply it”\n\n\n\npreds, targs = learn.get_preds()\n\n\n\n\n\n\n\n\n\nprint(accuracy_multi(preds, targs, thresh=0.9, sigmoid=False))\n\nTensorBase(0.9559)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFind the best threshold by testing over a range of potential thresholds!\n\n\n\nthresholds = torch.linspace(0.05, 0.99, 29)\naccs = [accuracy_multi(preds, targs, thresh=th, sigmoid=False) for th in thresholds]\nplt.plot(thresholds, accs)\n\n\n\n\n\n\n\n\n\nprint(accuracy_multi(preds, targs, thresh=0.55, sigmoid=False))\n\nTensorBase(0.9640)\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“… using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set” is perfectly fine here. “As you see in the plot, changing the threshold in this case results in a smooth curve, so we’re clearly not picking an inappropriate outlier”.\n\n\nSee p.231 for more discussion on this."
  },
  {
    "objectID": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html#summary",
    "href": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html#summary",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Multilabel Classification",
    "section": "Summary",
    "text": "Summary\nYou now know how to train both multi-label and muticlass vistion problems, when to use one or another, and what loss function to choose for each. You should consider treating multiclass problems where the predicted class should be “None” as a multi-label problem, especially if this is going to be used in the real-world and not just against some prefabbed dataset.\nAlso, we’re using accuracy as our metric to optimize the threshold in the example above, but you can use any metric (or combination of metrics you want). For example, a common issue with multi-label tasks is unbalanced datasets where one or more targets are ill represented in number. In that case, it may be more productive to use something like F1 or Recall."
  },
  {
    "objectID": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html#resources",
    "href": "posts/2021-06-10-ajtfb-chapter-6-multilabel.html#resources",
    "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Multilabel Classification",
    "section": "Resources",
    "text": "Resources\n\nhttps://book.fast.ai - The book’s website; it’s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc…\nTo learn more about pandas check the pandas documentation and chrisalbon.com."
  },
  {
    "objectID": "posts/2020-04-04-understanding-cross-entropy-loss.html",
    "href": "posts/2020-04-04-understanding-cross-entropy-loss.html",
    "title": "Loss Functions: Cross Entropy Loss and You!",
    "section": "",
    "text": "# only run this cell if you are in collab\n!pip install fastai\nimport torch\nfrom torch.nn import functional as F\nfrom fastai2.vision.all import *\nWe’ve been doing multi-classification since week one, and last week, we learned about how a NN “learns” by evaluating its predictions as measured by something called a “loss function.”\nSo for multi-classification tasks, what is our loss function?\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.loss_func\n\n\n\n\nDownloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n\n\n\n\n\n\n\n\nFlattenedLoss of CrossEntropyLoss()\nTo understand CrossEntropyLoss, we need to first understand something called Negative Log-Likelihood"
  },
  {
    "objectID": "posts/2020-04-04-understanding-cross-entropy-loss.html#negative-log-likelihood-nll-loss",
    "href": "posts/2020-04-04-understanding-cross-entropy-loss.html#negative-log-likelihood-nll-loss",
    "title": "Loss Functions: Cross Entropy Loss and You!",
    "section": "Negative Log-Likelihood (NLL) Loss",
    "text": "Negative Log-Likelihood (NLL) Loss\nLet’s imagine a model who’s objective is to predict the label of an example given five possible classes to choose from. Our predictions might look like this …\n\npreds = torch.randn(3, 5); preds\n\ntensor([[-0.3139,  0.6737, -0.0143,  1.9929, -0.6949],\n        [ 0.5285,  0.1311,  0.2628,  0.6450,  1.7745],\n        [-1.7458,  2.0199, -0.1365,  1.4622, -0.0940]])\n\n\nBecause this is a supervised task, we know the actual labels of our three training examples above (e.g., the label of the first example is the first class, the label of the 2nd example the 4th class, and so forth)\n\ntargets = torch.tensor([0, 3, 4])\n\nStep 1: Convert the predictions for each example into probabilities using softmax. This describes how confident your model is in predicting what it belongs to respectively for each class\n\nprobs = F.softmax(preds, dim=1); probs\n\ntensor([[0.0635, 0.1704, 0.0856, 0.6372, 0.0433],\n        [0.1421, 0.0955, 0.1089, 0.1596, 0.4939],\n        [0.0126, 0.5458, 0.0632, 0.3125, 0.0659]])\n\n\nIf we sum the probabilities across each example, you’ll see they add up to 1\n\nprobs.sum(dim=1)\n\ntensor([1.0000, 1.0000, 1.0000])\n\n\nStep 2: Calculate the “negative log likelihood” for each example where y = the probability of the correct class\nloss = -log(y)\nWe can do this in one-line using something called tensor/array indexing\n\nexample_idxs = range(len(preds)); example_idxs\n\nrange(0, 3)\n\n\n\ncorrect_class_probs = probs[example_idxs, targets]; correct_class_probs\n\ntensor([0.0635, 0.1596, 0.0659])\n\n\n\nnll = -torch.log(correct_class_probs); nll\n\ntensor([2.7574, 1.8349, 2.7194])\n\n\nStep 3: The loss is the mean of the individual NLLs\n\nnll.mean()\n\ntensor(2.4372)\n\n\n… or using PyTorch\n\nF.nll_loss(torch.log(probs), targets)\n\ntensor(2.4372)"
  },
  {
    "objectID": "posts/2020-04-04-understanding-cross-entropy-loss.html#cross-entropy-loss",
    "href": "posts/2020-04-04-understanding-cross-entropy-loss.html#cross-entropy-loss",
    "title": "Loss Functions: Cross Entropy Loss and You!",
    "section": "Cross Entropy Loss",
    "text": "Cross Entropy Loss\n… or we can do this all at once using PyTorch’s CrossEntropyLoss\n\nF.cross_entropy(preds, targets)\n\ntensor(2.4372)\n\n\nAs you can see, cross entropy loss simply combines the log_softmax operation with the negative log-likelihood loss"
  },
  {
    "objectID": "posts/2020-04-04-understanding-cross-entropy-loss.html#why-not-use-accuracy",
    "href": "posts/2020-04-04-understanding-cross-entropy-loss.html#why-not-use-accuracy",
    "title": "Loss Functions: Cross Entropy Loss and You!",
    "section": "Why not use accuracy?",
    "text": "Why not use accuracy?\n\n# this function is actually copied verbatim from the utils package in fastbook (see footnote 1)\ndef plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)):\n    x = torch.linspace(min,max)\n    fig,ax = plt.subplots(figsize=figsize)\n    ax.plot(x,f(x))\n    if tx is not None: ax.set_xlabel(tx)\n    if ty is not None: ax.set_ylabel(ty)\n    if title is not None: ax.set_title(title)\n\n\ndef f(x): return -torch.log(x)\n\nplot_function(f, 'x (prob correct class)', '-log(x)', title='Negative Log-Likelihood', min=0, max=1)\n\n\n\n\n\n\n\n\nNLL loss will be higher the smaller the probability of the correct class\nWhat does this all mean? The lower the confidence it has in predicting the correct class, the higher the loss. It will:\n\nPenalize correct predictions that it isn’t confident about more so than correct predictions it is very confident about.\nAnd vice-versa, it will penalize incorrect predictions it is very confident about more so than incorrect predictions it isn’t very confident about\n\nWhy is this better than accuracy?\nBecause accuracy simply tells you whether you got it right or wrong (a 1 or a 0), whereast NLL incorporates the confidence as well. That information provides you’re model with a much better insight w/r/t to how well it is really doing in a single number (INF to 0), resulting in gradients that the model can actually use!\nRember that a loss function returns a number. That’s it!\nOr the more technical explanation from fastbook:\n\n“The gradient of a function is its slope, or its steepness, which can be defined as rise over run – that is, how much the value of function goes up or down, divided by how much you changed the input. We can write this in maths: (y_new-y_old) / (x_new-x_old). Specifically, it is defined when x_new is very similar to x_old, meaning that their difference is very small. But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. So the problem is that a small change in weights from x_old to x_new isn’t likely to cause any prediction to change, so (y_new - y_old) will be zero. In other words, the gradient is zero almost everywhere.\n\n\nAs a result, a very small change in the value of a weight will often not actually change the accuracy at all. This means it is not useful to use accuracy as a loss function. When we use accuracy as a loss function, most of the time our gradients will actually be zero, and the model will not be able to learn from that number. That is not much use at all!” {% fn 1 %}"
  },
  {
    "objectID": "posts/2020-04-04-understanding-cross-entropy-loss.html#summary",
    "href": "posts/2020-04-04-understanding-cross-entropy-loss.html#summary",
    "title": "Loss Functions: Cross Entropy Loss and You!",
    "section": "Summary",
    "text": "Summary\nSo to summarize, accuracy is a great metric for human intutition but not so much for your your model. If you’re doing multi-classification, your model will do much better with something that will provide it gradients it can actually use in improving your parameters, and that something is cross-entropy loss."
  },
  {
    "objectID": "posts/2020-04-04-understanding-cross-entropy-loss.html#references",
    "href": "posts/2020-04-04-understanding-cross-entropy-loss.html#references",
    "title": "Loss Functions: Cross Entropy Loss and You!",
    "section": "References",
    "text": "References\n\nhttps://pytorch.org/docs/stable/nn.html#crossentropyloss\nhttp://wiki.fast.ai/index.php/Log_Loss\nhttps://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\nhttps://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy\nhttps://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hey there!\nI’m a full stack web application and machine learning developer with well over 20 years in the business. In addition to being the author of blurr, a python library that enables fast.ai developers to train Hugging Face transformers for core NLP tasks, I’m also a proud fast.ai community leader, where I’m active in both contributing to their deep learning library and helping folks out on the forums and discord. Outside of work, I enjoy hanging out with my two labradors, reading, working out, and above all else, learning. Feel free to give me a follow on twitter or github if any of the above interests you."
  },
  {
    "objectID": "pages/guides/vue3-guide.html",
    "href": "pages/guides/vue3-guide.html",
    "title": "Vue 3 Guide",
    "section": "",
    "text": "How to I define a computed property using the Composition API?\n\n\n\n\n\n// just a getter\nconst userEmail = computed(() =&gt; {\n  return $store.getters.userEmail;\n});\n\n// a getter and setter\nconst selectedUserClientId = computed({\n  get: () =&gt; {\n    return $store.getters.selectedUserClientId;\n  },\n  set: (newVal) =&gt; {\n    $store.dispatch(\"setSelectedUserClientId\", { client_id: newVal });\n  },\n});\nSee:\nHow to type a computed property in the new composition API?\nHow to call setter for object in computed properties\n\n\n\n\n\n\n\n\n\n\n\n\nHow to I define an asynchronous watcher or watchEffect?\n\n\n\n\n\nimport { ref, watch, watchEffect } from 'vue'\nimport { useRoute } from 'vue-router'\n\nexport default {\n  setup() {\n    const route = useRoute()\n    ...\n    // watch\n    watch(route, async (nv, ov) =&gt; {\n      const { data } = await api.get(`get-something/blah`)\n      console.log(data)\n    })\n\n    // watchEffect\n    const stateChanged = watchEffect(async () =&gt; {\n      const { data } = await api.get(`get-something/blah`)\n      console.log(data)\n     })\n     ...\n\n\n\n\n\n\n\n\n\nWhen should I use watch or watchEffect?\n\n\n\n\n\nUse watch when …\n\nYou want to do something with one or more specific reactive objects changes\nYou need both the old value and the new value\nYou need it to be called lazily\n\nUse watchEffect when …\n\nYou want to do something whenever the state changes for any of your reactive objects\nYou don’t care about the old value(s)\nYou need it to run immediately when reactive dependencies change\n\nSee:\nVue 3 Composition API - watch and watchEffect\nHow to use Vue Watch and Vue watchEffect\n\n\n\n\n\n\n\n\n\n\n\n\nHow can I pass data between routes using params?\n\n\n\n\n\nSimply add the data to your params attribute:\n&lt;router-link :to=\"{ name: 'my-named-route', params: { id: 1, another_param: 'something else' } }\"&gt;\nGo to my page\n&lt;/router-link&gt;\nSee:\nVue, is there a way to pass data between routes without URL params?\n\n\n\n\n\n\n\n\n\n\n\n\nHow can I watch for changes in the store’s values?\n\n\n\n\n\nSimply add the data to your params attribute:\nsetup(props) {\n   const store = useStore();\n\n   watch(()=&gt;store.getters.myvalue, function() {\n      console.log('value changes detected');\n   });\n\n   return {\n      myvalue: computed(() =&gt; store.getters.myvalue)\n   }\n},\nSee:\nVue3 composition api watch store value\n\n\n\n\n\n\n\n\n\nHow can I watch for changes in the store’s values across browser tabs and windows?\n\n\n\n\n\nconst app = new Vue({\n  el: \"#app\",\n  data: {\n    name: \"\",\n  },\n  methods: {\n    onStorageUpdate(event) {\n      if (event.key === \"name\") {\n        this.name = event.newValue;\n      }\n    },\n  },\n  mounted() {\n    if (localStorage.name) {\n      this.name = localStorage.name;\n    }\n    window.addEventListener(\"storage\", this.onStorageUpdate);\n  },\n  beforeDestroy() {\n    window.removeEventListener(\"storage\", this.onStorageUpdate);\n  },\n  watch: {\n    name(newName) {\n      localStorage.name = newName;\n    },\n  },\n});\nSee:\nReactive localStorage object across browser tabs using Vue.js\n\n\n\n\n\n\n\n\n\nHow can I store complex objects like arrays and dictionaries in localStorage?\n\n\n\n\n\nBy using JSON.stringfy(X) when storing the data and JSON.parse(X) when you fetch the data\nvar names = [];\nnames[0] = prompt(\"New member name?\");\nlocalStorage.setItem(\"names\", JSON.stringify(names));\nvar storedNames = JSON.parse(localStorage.getItem(\"names\"));\n\n// ... or ...\nlocalstorage.names = JSON.stringify(names);\nvar storedNames = JSON.parse(localStorage.names);\nSee:\nHow do I store an array in localStorage?"
  },
  {
    "objectID": "pages/guides/vue3-guide.html#core",
    "href": "pages/guides/vue3-guide.html#core",
    "title": "Vue 3 Guide",
    "section": "",
    "text": "How to I define a computed property using the Composition API?\n\n\n\n\n\n// just a getter\nconst userEmail = computed(() =&gt; {\n  return $store.getters.userEmail;\n});\n\n// a getter and setter\nconst selectedUserClientId = computed({\n  get: () =&gt; {\n    return $store.getters.selectedUserClientId;\n  },\n  set: (newVal) =&gt; {\n    $store.dispatch(\"setSelectedUserClientId\", { client_id: newVal });\n  },\n});\nSee:\nHow to type a computed property in the new composition API?\nHow to call setter for object in computed properties\n\n\n\n\n\n\n\n\n\n\n\n\nHow to I define an asynchronous watcher or watchEffect?\n\n\n\n\n\nimport { ref, watch, watchEffect } from 'vue'\nimport { useRoute } from 'vue-router'\n\nexport default {\n  setup() {\n    const route = useRoute()\n    ...\n    // watch\n    watch(route, async (nv, ov) =&gt; {\n      const { data } = await api.get(`get-something/blah`)\n      console.log(data)\n    })\n\n    // watchEffect\n    const stateChanged = watchEffect(async () =&gt; {\n      const { data } = await api.get(`get-something/blah`)\n      console.log(data)\n     })\n     ...\n\n\n\n\n\n\n\n\n\nWhen should I use watch or watchEffect?\n\n\n\n\n\nUse watch when …\n\nYou want to do something with one or more specific reactive objects changes\nYou need both the old value and the new value\nYou need it to be called lazily\n\nUse watchEffect when …\n\nYou want to do something whenever the state changes for any of your reactive objects\nYou don’t care about the old value(s)\nYou need it to run immediately when reactive dependencies change\n\nSee:\nVue 3 Composition API - watch and watchEffect\nHow to use Vue Watch and Vue watchEffect\n\n\n\n\n\n\n\n\n\n\n\n\nHow can I pass data between routes using params?\n\n\n\n\n\nSimply add the data to your params attribute:\n&lt;router-link :to=\"{ name: 'my-named-route', params: { id: 1, another_param: 'something else' } }\"&gt;\nGo to my page\n&lt;/router-link&gt;\nSee:\nVue, is there a way to pass data between routes without URL params?\n\n\n\n\n\n\n\n\n\n\n\n\nHow can I watch for changes in the store’s values?\n\n\n\n\n\nSimply add the data to your params attribute:\nsetup(props) {\n   const store = useStore();\n\n   watch(()=&gt;store.getters.myvalue, function() {\n      console.log('value changes detected');\n   });\n\n   return {\n      myvalue: computed(() =&gt; store.getters.myvalue)\n   }\n},\nSee:\nVue3 composition api watch store value\n\n\n\n\n\n\n\n\n\nHow can I watch for changes in the store’s values across browser tabs and windows?\n\n\n\n\n\nconst app = new Vue({\n  el: \"#app\",\n  data: {\n    name: \"\",\n  },\n  methods: {\n    onStorageUpdate(event) {\n      if (event.key === \"name\") {\n        this.name = event.newValue;\n      }\n    },\n  },\n  mounted() {\n    if (localStorage.name) {\n      this.name = localStorage.name;\n    }\n    window.addEventListener(\"storage\", this.onStorageUpdate);\n  },\n  beforeDestroy() {\n    window.removeEventListener(\"storage\", this.onStorageUpdate);\n  },\n  watch: {\n    name(newName) {\n      localStorage.name = newName;\n    },\n  },\n});\nSee:\nReactive localStorage object across browser tabs using Vue.js\n\n\n\n\n\n\n\n\n\nHow can I store complex objects like arrays and dictionaries in localStorage?\n\n\n\n\n\nBy using JSON.stringfy(X) when storing the data and JSON.parse(X) when you fetch the data\nvar names = [];\nnames[0] = prompt(\"New member name?\");\nlocalStorage.setItem(\"names\", JSON.stringify(names));\nvar storedNames = JSON.parse(localStorage.getItem(\"names\"));\n\n// ... or ...\nlocalstorage.names = JSON.stringify(names);\nvar storedNames = JSON.parse(localStorage.names);\nSee:\nHow do I store an array in localStorage?"
  },
  {
    "objectID": "pages/guides/vue3-guide.html#quasar",
    "href": "pages/guides/vue3-guide.html#quasar",
    "title": "Vue 3 Guide",
    "section": "Quasar",
    "text": "Quasar\n\nWorking with q-table\n\n\n\n\n\n\nHow can I create grid for CRUD operations using a modal form for inserts/upserts with filtering, sorting, etc…?\n\n\n\n\n\nSee:\nQTable with Action Buttons\nQuasar QTable: Editing with QPopupEdits and QButtons to add/delete/update rows\nQuasar - q-table with toggle filters and text search"
  },
  {
    "objectID": "pages/guides/postgresql-guide.html",
    "href": "pages/guides/postgresql-guide.html",
    "title": "PostgreSQL Guide",
    "section": "",
    "text": "How can I restrict the possible values for a column?\n\n\n\n\n\nYou have two options:\n\nA CHECK CONSTRAINT:\n\nALTER TABLE distributors\n   ADD CONSTRAINT check_types\n   CHECK (element_type = 'lesson' OR element_type = 'quiz');\n\nA ENUM:\n\nCREATE TYPE element_type AS ENUM ('lesson', 'quiz');\nSee:\nIn Postgres, how do you restrict possible values for a particular column?"
  },
  {
    "objectID": "pages/guides/postgresql-guide.html#database-design",
    "href": "pages/guides/postgresql-guide.html#database-design",
    "title": "PostgreSQL Guide",
    "section": "",
    "text": "How can I restrict the possible values for a column?\n\n\n\n\n\nYou have two options:\n\nA CHECK CONSTRAINT:\n\nALTER TABLE distributors\n   ADD CONSTRAINT check_types\n   CHECK (element_type = 'lesson' OR element_type = 'quiz');\n\nA ENUM:\n\nCREATE TYPE element_type AS ENUM ('lesson', 'quiz');\nSee:\nIn Postgres, how do you restrict possible values for a particular column?"
  }
]