{
  
    
        "post0": {
            "title": "What are the differences between training, validation, and test datasets",
            "content": "Here we look at the differences between training, validation, and test sets, and also both strategies and best practices for building each. . . What is a training set? . A training set consits of the data your model sees during training. These are the inputs and labels your model will use to determine the loss and update it&#39;s parameters in a way that will hopefully lead to a model that works well for its given task. . Why do we need a training set? . Because a model needs something to train on. It should be representative of the data the model will see in the future, and it should be updated if/when you discover that is not the case. . How to use a training set? . To train a model on examples resembling that which the model will seen in the future. More is generally better, but quality is king (e.g., bad data in, bad data out). . | To provide augmented examples for your model to see so as to increase the number of examples and better reflect what the model may see in the real world. . | . What is a validation set? . A validation set (also know as the &quot;development set&quot;) does not include any data from the training set. It&#39;s purpose to is gauge the generalization prowess of your model and also ensure you are neight overfitting or underfitting. . &quot;If [the model] makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by actually having seen that particular item.&quot; 1 . Why do we need a validation set? . &quot;[because] what we care about is how well our model works on previously unseen images ... the longer you train for, the better your accuracy will get on the training set ... as the model starts to memorize the training set rather than finding generalizable underlying patterns in the data = overfitting&quot; 2 . Overfitting happens when the model &quot;remembers specific features of the input data, rather than generalizing well to data not seen during training.&quot; 3 . Note: ALWAYS overfit before anything else. It is your training loss gets better while your validation loss gets worse ... in other words, if you&#8217;re validation loss is improving, even if not to the extent of your training loss, you are not overfitting . Note: ALWAYS include a validation set. . Note: ALWAYS use the validation set to measure your accuracy (or any metrics). . Note: ALWAYS set the seed parameter so that you &quot;get the same validation set every time&quot; so that &quot;if we change our model and retrain it, we know any differences are due to the changes to the model, not due to having a different random validation set.&quot; 4 . Tip: For a good discussion of how to achieve predictable randomness, see this discussion on the fast.ai forums. There are actually several seeds you need to set and in several places when using fast.ai to achieve reproducibility. . How to use a validation set? . It gives us a sense of how well our model is doing on examples it hasn&#39;t seen, which makes sense since the ultimate worth of a model is in how well it generalizes to things unseen in the future. . | The validation set also informs us how we may change the hyperparamters (e.g., model architecture, learning rates, data augmentation, etc...) to improve results. These parameters are NOT learned ... they are choices WE make that affect the learning of the model parameters. 5 . | . What is a test set? . A test set ensures that we aren&#39;t overfitting our hyperparameter choices; it is held back even from ourselves and used to evaulate the model at the very end. . &quot;[Since we] are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values ... subsequent version of the model are, indirectly, shaped by us having seen the validation data ... [and therefore], we are in danger of overfitting the validation data through human trial and error and exploration.&quot; 6 . Note:A key property of the validation and test sets is that they must be representative of the new data you will see in the future. . Why do we need a test set? . To ensure we aren&#39;t inadvertently causing the model to overfit via our hyperparameter tuning which happens as a result of us looking at the validation set. It is a completely hidden dataset; it isn&#39;t used for training or tuning, only for measuring performance. . How to use a test set? . If evaluating 3rd party solutions. You&#39;ll want to know how to create a good test set and how to create a good baseline model. Hold these out from the potential consultants and use them to fairly evaluate their work. . | To ensure you aren&#39;t overfitting your model as a result of validation set examination. As with the validation set, a good test set offers further assurance your model isn&#39;t learning particular ancillary features of particular things in your images. . | . How to create good validation and test sets . It isn&#39;t always as easy as randomly shuffling your data! . Again, what both of these sets should haven in common is that they &quot;must be representative of the new data you will see in the future.&quot; And what this looks like often dependes on your use case and task. . Tip: You really need to think about what you need to predict and what you&#8217;d look at to make that prediction. You also need to make sure your training data is qualitatively different enough from your real world data (e.g., what the validation and test sets represent) as to learn patterns and not specific examples. . First, consider cases where historical data is required to predict the future, for example of quant traders use &quot;backtesting to check whether their models are predictive of future periods, based on past data&quot; 7 . Note: &quot;For a time series ... (where you are using historical data to build a model for use in the future ... you will want to choose a continuous section with the latest dates as your validation set&quot; 8 . &quot;A second common case occurs when you can easily anticipate ways the data you will be making predictions for in production may be qualitatively different from the data you have to train your model with.&quot; 9 . As an example of this, the Kaggle distracted driver competition is used. In it, based on pictures of drivers you need to predict categories of distraction. Since the goal of such a model would be to make good predictions against drivers the model hasn&#39;t seen, it would make sense to create a validation and also a test set consiting of specific drivers the training set doesn&#39;t include (in fact, the competition&#39;s test set is exactly that!). &quot;If you used all the people in training your model, your model might be overfitting to the paricipants of those specific people and not just learning the states (texting, eating, etc.).&quot; 10 . Another example of this is the Kaggle fisheries competition where the objective is to predict the species of fish caught on fishing boats. As the goal of such a model is to predict the species on other/future boats, it makes sense then that &quot;the test set consisted of images from boats that didn&#39;t appear in the training data, so in this case you&#39;d want your validation set to also include boats that are not in the training set.&quot; 11 . Tip: Start with training a model and let the results guide your EDA! . For a stellar example of how this looks in practice, see this thread from Boris Dayma on an issue he noticed when looking at his results on the training and validation sets. Note how his EDA was directed via training a model ... and also make sure to read through all the comments, replies, etc... for other things to pay attention too when seeing unusual results during training (there is a lot of good stuff there). Ultimately, in his case, what he found out was that the dataset was imbalanced and the imbalanced data was getting lumped together in the same batches due to poor shuffling strategy. He documents his fix in a subsequent thread so check that out too. . Tip: Knowing how to read your training/validation results drives EDA and will lead to better train/validation/test splits. . . 1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In The Fastbook p.49‚Ü© . 2. Ibid., p.29‚Ü© . 3. Ibid.‚Ü© . 4. Ibid‚Ü© . 5. Ibid., p.49‚Ü© . 6. Ibid.‚Ü© . 7. Ibid., p.53‚Ü© . 8. Ibid., p.51. There are some really good illustraions on pp.51 and 52 with some follow-up intutition on page 53 wrt to time series splits‚Ü© . 9. Ibid., p.53‚Ü© . 10. Ibid.‚Ü© . 11. Ibid. pp.53-54‚Ü© .",
            "url": "https://ohmeow.com/what-is/training-validation-test-sets",
            "relUrl": "/what-is/training-validation-test-sets",
            "date": " ‚Ä¢ Nov 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "What is a metric",
            "content": ". A definition . Metrics are a human-understandable measures of model quality whereas the loss is the machine&#39;s. They are based on your validation set and are what you really care about, whereas the loss is &quot;a measure of performance&quot; that the training system can use to update weights automatically. . A good choice for loss is a function &quot;that is easy for stochastic gradient descent (SGD) to use, whereas a good choies for your metrics are functions that your business users will care about. Seldom are they the same because most metrics don&#39;t provide smooth gradients that SGD can use to update your model&#39;s weights. . . Note: Again, they are based on your validation/test sets (not your training set). Ultimately, we want to have a model that generalizes well to inputs it was not trained on, and this is what our validation/test sets represent. This is how we relay our model quality. . . Examples . There are a whole list of metrics built into the fastai library, see here. Below I begin a listing of the most common ones as they come up in the fastbook (and from personal experience). . error rate = &quot;the proportion of images that were incorrectly identified.&quot; 1 . accuracy = the proportation of images that were correctly identified (1 - error rate) . . Metrics to use based on task . Metric Multiclass classification Multilabel classification Regression . error rate | Yes | Yes* | No | . accuracy | Yes | Yes* | No | . * indicates that other metrics may be better for the given task. . . 1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In The Fastbook p.19‚Ü© .",
            "url": "https://ohmeow.com/what-is/a-metric",
            "relUrl": "/what-is/a-metric",
            "date": " ‚Ä¢ Nov 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "What are ResNets & Why use it for computer vision tasks",
            "content": "Arguably the best architecture for most computer vision tasks, here we take a look at ResNet and how it can be used in fastai for a variety of such tasks. . . What is a ResNet &amp; Why use it for computer vision tasks? . A ResNet is a model architecture that has proven to work well in CV tasks. Several variants exist with different numbers of layers with the larger architectures taking longer to train and more prone to overfitting especially with smaller datasets. . The number represents the number of layers in this particular ResNet variant ... &quot;(other options are 18, 50, 101, and 152) ... model architectures with more layers take longer to train and are more prone to overfitting ... on the other hand, when using more data, they can be qite a bit more accurate.&quot; 2 . What other things can use images recognizers for besides image tasks? . Sound, time series, malware classification ... &quot;a good rule of thumb for converting a dataset into an image representation: if the human eye can recognize categories from the images, then a deep learning model should be able to do so too.&quot; 1 . How does it fare against more recent architectures like vision transformers? . Pretty well apparently (at least at the time this post was written) ... I&#39;m pleased to announce that the &#39;ResNet strikes back&#39; paper is now on arxiv! Moving the baseline forward to 80.4% top-1 for a vanilla ResNet-50 arch w/ better training recipes. No extra data, no distillation. https://t.co/WP3UDXfV0r . &mdash; Ross Wightman (@wightmanr) October 4, 2021 . . ResNet best practices . . Tip: Start with a smaller ResNet (like 18 or 34) and move up as needed. . Note: If you have a lot of data, the bigger resnets will likely give you better results. . . An example using the high-level API . Step 1: Build our DataLoaders . from fastai.vision.all import * path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func(path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) . Why do we make images 224x224 pixels? . &quot;This is the standard size for historical reasons (old pretrained models require this size exactly) ... If you increase the size, you&#39;ll often get a model with better results since it will be able to focus on more details.&quot; 3 . Tip: Train on progressively larger image sizes using the weights trained on smaller sizes as a kind of pretrained model. . Step 2: Build our cnn_learner . learn = cnn_learner(dls, resnet18, metrics=error_rate) . As you can see above, the architecture being used is a resnet with 18 layers. . Step 3: Train . learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.161614 | 0.040670 | 0.013532 | 01:03 | . epoch train_loss valid_loss error_rate time . 0 | 0.062475 | 0.020072 | 0.006766 | 01:04 | . For more information on how transfer learning works, and the fine_tune method in particuarl, see this section in my &quot;What is machine learning&quot; post. . For more metrics like error_rate, see my &quot;What is a metric&quot; post. . . 2. &quot;Chaper 1: Your Deep Learning Journey&quot;. In The Fastbook pp.30-31.‚Ü© . 1. Ibid., p.39. Pages 36-39 provides several examples of how non-image data can be converted to an image for such a purpose.‚Ü© . 3. Ibid., p.28‚Ü© .",
            "url": "https://ohmeow.com/what-is/resnets",
            "relUrl": "/what-is/resnets",
            "date": " ‚Ä¢ Nov 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "What is the difference between categorial and continuous datatypes",
            "content": "When it comes to both your inputs and targets, knowing whether they are categorial or continuous guides how you represent them, the loss function you use, and the metrics you choose in measuring performance. . . What is a categorical datatype? . Categorical data &quot;contains values that are one of a discrete set of choice&quot; such as gender, occupation, day of week, etc... 1 . What if our target is categorical? . If your target/lables are categorical, then you have either a multi-classification classification problem (e.g., you are trying to predict a single class) or a multi-label classification problem (e.g., you are trying to predict whether your example belongs to zero or multiple classes). . Multi-classification tasks . For multi-classification tasks, a sensible loss function would be cross entropy loss (nn.CrossEntropyLoss) and useful metrics are likely to include error rate, accuracy, F1, recall, and/or precision depending on your business objectices and the make up of your dataset. For example, if you&#39;re dealing with a highly imbalanced dataset, choosing accuracy would lead to an inflated sense of model performance since it may be learning to just predict the most common class. . Note: What if you need to predict &quot;None&quot;? This is more real world and covered nicely in Zach Mueller&#8217;s Recognizing Unknown Images (or the Unknown Label problem). . Multi-label tasks . For multi-label tasks, a sensible loss function would be binary cross entropy loss (BCE) (nn.BCEWithLogitsLoss) and useful metrics are likely to include F1, recall, and/or precision depending on your business objectices and the make up of your dataset. Notice that I didn&#39;t include error rate, or its opposite accuracy, as their datasets are generally highly imbalanced. . What if our input is categorical? . Categorical inputs are generally represented by an embedding (e.g., a vector of numbers). Why? Mostly because it gives your model the ability to provide a more complex representation of your category than a single numer would. . For example, imagine that one of your inputs is day of week (e.g., Sunday, Monday, etc.) ... what does that mean? When combined with other inputs, its likely that the meaning of it is going to be much more nuanced than a single number can represent, and so we&#39;d like to use multiple learned numbers. This is what an embedding is. . . What is a continuous datatype? . Continuous data is numerical that represents a quantity such as age, salary, prices, etc... . What if our target is continuous? . If your target/labels are continuous, then you have a regression problem and the most likely loss function you would choose would be mean-square-error loss (MSE) (nn.MSELoss) and your metric MSE as well . &quot;... MSE is already a a useful metric for this task (although its&#39; probably more interpretable after we take the square root)&quot; ... the RMSE (% fn 3 %} . Note:For tasks that predict a continuous number, consider using y_range to constrain the network to predicting a value in the known range of valid values.2 . What if our input is continuous? . In many cases there isn&#39;t anything special you need to do, in others, it makes sense to scale these numbers so they are in the same range (usually 0 to 1) as the rest of your continuous inputs. This process is called normalization. 4. The reason you would want to do this is so continuous values with bigger range of values (say 1000) don&#39;t drown out those with a smaller range (say 5) during model training. . Normalization . . Note: &quot;When training a model, if helps if your input data is normalizaed - that is, has a mean of 0 and a standard deviation of 1. . See How To Calculate the Mean and Standard Deviation ‚Äî Normalizing Datasets in Pytorch . import torch print(&#39;Example 1&#39;) nums = torch.tensor([0, 50, 100], dtype=float) print(f&#39;Some raw values: {nums}&#39;) # 1. calculate their mean and standard deviation m = nums.mean() std = nums.std() print(f&#39;Their mean is {m} and their standard deviation is {std}&#39;) # 2. normalize their values normalized = (nums - m) / std print(f&#39;Here are their values after normalization: {normalized}&#39;) print(&#39;&#39;) print(&#39;Example 2&#39;) nums = torch.tensor([0, 5000, 10000], dtype=float) print(f&#39;Some raw values: {nums}&#39;) # 1. calculate their mean and standard deviation m = nums.mean() std = nums.std() print(f&#39;Their mean is {m} and their standard deviation is {std}&#39;) # 2. normalize their values normalized = (nums - m) / std print(f&#39;Here are their values after normalization: {normalized}&#39;) print(&#39;&#39;) . Example 1 Some raw values: tensor([ 0., 50., 100.], dtype=torch.float64) Their mean is 50.0 and their standard deviation is 50.0 Here are their values after normalization: tensor([-1., 0., 1.], dtype=torch.float64) Example 2 Some raw values: tensor([ 0., 5000., 10000.], dtype=torch.float64) Their mean is 5000.0 and their standard deviation is 5000.0 Here are their values after normalization: tensor([-1., 0., 1.], dtype=torch.float64) . fastai supplies a Normalize transform you can use to do this ... &quot;it acts on a whole mini-batch at once, so you can add it to the batch_tfms secion of your data block ... you need to pass to this transform the mean and standard deviation that you want to use. If you don&#39;t, &quot;fastai will automatically calculate them from a single batch of your data). p.241 . Note: &quot;This means that when you distribute a model, you need to also distribute the statistics used for normalization.&quot; (p.242) . Important: &quot;... if you&#8217;re using a model that someon else has trained, make sure you find out what normalization statistics they used an match them&quot; (p.242) . . 1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In The Fastbook p.46‚Ü© . 3. Ibid. p.236. A good examle of how RMSE provides a reasonable metric for regression tasks is included on this page in reference to KeyPoint detection (e.g., detecting a point/coordinate, an x and y)‚Ü© . 2. Ibid., p.47‚Ü© . 4. Ibid., pp.241-42, 320 includes an extended discussion of the why, how, and where &quot;normalization&quot; is needed. ‚Ü© .",
            "url": "https://ohmeow.com/what-is/categorial-continuous-datatypes",
            "relUrl": "/what-is/categorial-continuous-datatypes",
            "date": " ‚Ä¢ Nov 4, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "What is a metric",
            "content": ". A definition . Metrics are a human-understandable measures of model quality whereas the loss is the machine&#39;s. They are based on your validation set and are what you really care about, whereas the loss is &quot;a measure of performance&quot; that the training system can use to update weights automatically. . A good choice for loss is a function &quot;that is easy for stochastic gradient descent (SGD) to use, whereas a good choies for your metrics are functions that your business users will care about. Seldom are they the same because most metrics don&#39;t provide smooth gradients that SGD can use to update your model&#39;s weights. . . Note: Again, they are based on your validation/test sets (not your training set). Ultimately, we want to have a model that generalizes well to inputs it was not trained on, and this is what our validation/test sets represent. This is how we relay our model quality. . . Examples . There are a whole list of metrics built into the fastai library, see here. Below I begin a listing of the most common ones as they come up in the fastbook (and from personal experience). . error rate = &quot;the proportion of images that were incorrectly identified.&quot; 1 . accuracy = the proportation of images that were correctly identified (1 - error rate) . . Metrics to use based on task . Metric Multiclass classification Multilabel classification Regression . error rate | Yes | Yes* | No | . accuracy | Yes | Yes* | No | . * indicates that other metrics may be better for the given task. . . 1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In The Fastbook p.19‚Ü© .",
            "url": "https://ohmeow.com/what-is/a-metric",
            "relUrl": "/what-is/a-metric",
            "date": " ‚Ä¢ Nov 3, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "What is machine learning",
            "content": "Here we look at machine learning in general (of which deep learning is a subset) as well as the process of finetuning a pretrained ML model. When you think of deep learning ... think neural networks. . . A picture . An explanation . &quot;Suppowe we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignemnt so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would &#39;learn&#39; from its experince&quot; - Arthur Samuel 1 . Architecture vs. model . ... a model is a special kind of program:it&#39;s one that can do many different things, depending &gt; on the weights. 2 . The functional form of the model is called its architecture. . . Note: The architecture is &quot;the template of the model that we&#8217;re trying to fit; i.e., the actual mathematical function that we&#8217;re passing the input data and parameters to&quot; ... whereas the model is a particular set of parameters + the architecture. . Parameters . Weights are just variables, and a weight assignment is a particuarl choice of values for those variables. [Weights] are generally referred to as model parameters ... the term weights being reserved for a particular type of model parameter. 3 . The weights are called parameters. . . Note: These parameters are the things that are &quot;learnt&quot;; the values that can be updated, whereas activations in a neural network are simply numbers as the result of some calculation. . Inputs vs.labels . The inputs, also known as your independent variable(s) [your X] is what your model uses to make predictions. 4 . The labels, also known as your dependent variable(s) [your y] represent the correct target value for your task. 5 . Loss . The [model&#39;s] measure of performance is called the loss ... [the value of which depends on how well your model is able to predict] the correct labels. 6 . The loss is a measure of model performance that SGD can use to make your model better. A good loss function provides good gradients (slopes) that can be used to make even very minor changes to your weights so as to improve things. Visually, you want gentle rolling hills rather than abrupt steps or jagged peaks. . . Note: You can think of the loss as the model&#8217;s metric, that is, how it both understands how good it is and can help it improve. . . Transfer learning . Transfer learning is the process of taking a &quot;pretrained model&quot; that has been trained on a very large dataset with proven SOTA results, and &quot;fine tuning&quot; it for your specific task, which while likely similar to the task the pretrained model was trained for to one degree or another, is not the necesarily the same. . How does it work? . The head of your model (the newly added part specific to your dataset/task) should be trained first since it is the only one with completely random weights. | The degree to which your weights of the pretrained model will need to be updated is proportional to how similar your data is to the data it was trained on. The more dissimilar, the more the weights will need to be changed. | Your model will only be as good as the data it was trained on, so make sure what you have is representative of what it will see in the real world. It &quot;can learn to operate on only the patterns seen in the input data used to train it.&quot; | The process of training (or fitting) the model is the process of finding a set of parameter values (or weights) that specialize that general architecture into a model that works well for our particular kind of data [and task] . What is the high-level approach in fastai? . fastai provides a fine_tune method that uses proven tricks and hyperparameters for various DL tasks that the author&#39;s have found works well most of the time. 7 . . What do we have at the end of training (or finetuning)? . ... once the model is trained - that is, once we&#39;ve chosen our final weight assignments - then we can think of the weights as being part of the model since we&#39;re not varying them anymore. 8 . This means a trained model can be treated like a typical function. . . 1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In The Fastbook p.21‚Ü© . 2. Ibid.‚Ü© . 3. Ibid., pp.21-22‚Ü© . 4. Ibid., p.22‚Ü© . 5. Ibid.‚Ü© . 6. Ibid.‚Ü© . 7. Ibid., pp.32-33. Includes a full discussion on how the method works‚Ü© . 8. Ibid., p.22‚Ü© .",
            "url": "https://ohmeow.com/what-is/machine-learning",
            "relUrl": "/what-is/machine-learning",
            "date": " ‚Ä¢ Nov 2, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "How to learn (deep learning)",
            "content": "Cervantes once wrote that &quot;the journey is better than the inn&quot;, but I rather like to think that the journey is the inn. . It means that irrespective to its difficulties (and likely because of them), your adventures is what you look back on with fondness at its end rather than the end itself. It&#39;s why I enjoy reading &quot;The Lord of the Rings&quot; every five years or so, where as I age and experience the hand life has dealt me, I find myself appreciating different aspects of the story and gaining new insights into what I value and into what I want to be as a human being. I find my journey with deep learning to be roughly analgous to that. . I&#39;ve been a part of the fast.ai community for several years. I&#39;ve been through the course multiple times (since it was using theano back in the old days), I&#39;ve contributed to the library, and use it as the basis for one of my own. And as with each course, a re-reading of the book brings new insights, ideas, and revelations. . And so here I begin with my meandering thoughts and reflections from yet another reading of what I consider &quot;The Lord of the Rings&quot; of deep learning. As such, it makes sense to being with the most foundational topic in fastbook, &quot;How do you learn Deep Learning?&quot; . . You can do this! . Hi, everybody; I&#39;m Jeremy ... I do not have any formal technical education ... didn&#39;t have great grades. I was much more interested in doing real projects. 1 . This is meaningful to me as someone with a BA in History and a MA in Theology. It&#39;s a reminder that if you want something, it&#39;s within your grasp to make it happen if you are willing to put in the work. It&#39;s also a reminder that key to getting there is actually doing something! If find too many people thinking that if they just get into that school, or if they can just take that class, then they&#39;ll be a good software enginner or deep learning practitioner. The reality is that the only way you get there is by doing it ... just like pull-ups (which aren&#39;t much fun when you&#39;re starting out and/or you&#39;re 49 and overweight). . The problem with traditional education . ... how math is taught - we require students to spend years doing rote memorization and learning dry disconnected fundatmentals that we claim will pay off later, long after most of them quit the subject. 2 . This also is the problem with higher education in general, where young people spend at least four to five years learning things they already learned in High School or else things they don&#39;t really care about and will be forgotten right after finals, spending in excess of $100,000 for the privilege of it and likely going into debt in the tens of thousands of dollars, all with this idea that having done it they will be prepared for the real world. Unfortunately, that&#39;s not how it works. Whether you are in a university of even go to university, what matter is what you do ... not what classes you took or what your GPA is. . Deep Learning (and coding in general) is an art maybe more so than a science . The hardest part of deep learning is artisanal. 3 . I remember going to an iOS conference way back in the day and a conference speaker asking how many folks in the session I was sitting in had a background in music. 80-90% of the audience raised their hands. Sure, there is math and stats and a science to deep learning, but like any coding enterprise, it&#39;s an art ... with some artists being better than others along with room for improvement regardless of whether you&#39;re Van Gough or painting by the numbers. . Doing is how you learn, and what you&#39;ve done is what matters . ... focus on your hobbies and passions ... Common character traits in the people who do well at deep learning include playfulness and curiosity. 4 . at Tesla .. CEO Elon Musk says &#39;A PhD is definitely not required. All that matters is a deep understanding of AI &amp; ability to implement NNs in a way that is actually useful .... Don&#39;t care if you even graduated High School.&#39; 5 . ... the most important thing for learning deep learning is writing code and experimenting.&quot; 6 . . Folks to follow . It&#39;s always helpful to have some role models; folks who practice the lessons learned above and can help you along your journey. . For starters, consider this image of the top 12 users based on most likes in the fast.ai forums: . Aside from the founders of fast.ai and a bunch of them working for noteable ML companies like Hugging Face and Weights &amp; Biases, I can think of at least FOUR things these folks have in common: . They are fearless in asking what they may have even considered, dumb questions. . | They are active in researching the answers to their own questions (even the dumb ones) and those asked by others. . | They are active in teaching others through blogs, books, open source libraries, study groups, and podcasts. . | They build things! That is, they all have experience building models and making them usable via deployed applications and/or in kaggle compeititions. Anyone can bake a half-cooked model in a Jupyter notebook, but few can turn it into something others can use. . | These traits aren&#39;t just key to learning deep learning; they are key to learning anything! Practice them and you guarantee yourself success in learning anything you&#39;ve set your mind on. . . What if I can only follow three? . Aside from Jeremy (@jeremyphoward), who&#39;s a given, if I could only follow three people who have mastered to art of learning deep learning, they would be ... . Radek Osmulsk: (twitter: @radekosmulski) If you found this of value, you might be interested in a book on learning deep learning that I wrotecheck it out here &gt;&gt;&gt; https://t.co/ApKlm8BRmy . &mdash; Radek Osmulski (@radekosmulski) November 2, 2021 . Zach Mueller: (twitter: @TheZachMueller) To me, I think it boiled down to how I learned. I took those two courses essentially over the course of a year or so. Approaching each lesson slowly, and letting myself wander in the related concepts, learning as much as I could through online communities. . &mdash; Zach Mueller (@TheZachMueller) October 23, 2021 . Sanyam Bhutani: (twitter: @bhutanisanyam1) The @PyTorch book reading group @weights_biases comes to an endüôèWe had an incredible 10 weeks of learning!As a group wanted to extend our gratitude to the incredible authors: Eli, @lantiga &amp; @ThomasViehmann A few words from our community:https://t.co/3ODz6J1vad . &mdash; Sanyam Bhutani (@bhutanisanyam1) October 25, 2021 . Personally, I do follow each of these individuals on twitter and you should too! Though I&#39;ve never met any of them IRL, I consider the colleagues, friends, and amongst the most helpful for those looking to get started in machine learning. . . Tip: Twitter is imo the best place to network with fellow ML/DL practioners and stay up-to-date with the latest developments in ML in general . . What if I&#39;m too lazy to read any of that stuff above? . Well, you can watch this video (and then go back and read that stuff anyways) ... . . 1. &quot;Chaper 1: Your Deep Learning Journey&quot;. In The Fastbook p.8‚Ü© . 2. Ibid., p.9‚Ü© . 3. Ibid., p.10‚Ü© . 4. Ibid., p.11‚Ü© . 5. Ibid.‚Ü© . 6. Ibid.‚Ü© .",
            "url": "https://ohmeow.com/how-to/learn-deep-learning",
            "relUrl": "/how-to/learn-deep-learning",
            "date": " ‚Ä¢ Nov 1, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "A Journey Through Fastbook (AJTFB) - Chapter 6: Multilabel Classification",
            "content": "Other posts in this series: A Journey Through Fastbook (AJTFB) - Chapter 1 A Journey Through Fastbook (AJTFB) - Chapter 2 A Journey Through Fastbook (AJTFB) - Chapter 3 A Journey Through Fastbook (AJTFB) - Chapter 4 A Journey Through Fastbook (AJTFB) - Chapter 5 . Chapter 6 . . Multiclass vs Multi-label classification (again)... . Last post we saw that multiclass classification is all about predicting a SINGLE CLASS an object belongs to from a list of two or more classes. It&#39;s the go to task if we&#39;re confident that every image our model sees is going to be one of these classes. Cross-entropy loss is our go to loss function as it wants to confidently pick one thing. . Multi-label classification involves predicting MULTIPLE CLASSES to which an object belongs; it can belong to one, some, all, or even none of those classes. For example, you may be looking at satellite photos from which you need to predict the different kinds of terrain (your classes) each contains. . Important: Use the multi-label classification approach in your &quot;multiclassification problems&quot; where you want your model to be able to result in None (which is probably a more common real world use case) . . Defining your DataBlock . Again, the DataBlock is a blueprint for everything required to turn your raw data (images and labels) into something that can be fed through a neural network (DataLoaders with a numerical representation of both your images and labels). Below is the one presented in this chapter. . from fastai.vision.all import * path = untar_data(URLs.PASCAL_2007) . Instead of working with the filesystem structure to get our images and define our labels, in this example we use a .csv file that we can explore and manipulate further via a pandas DataFrame. . Important: DataFrames are one of those things you&#8217;re going to want to get comfortable using. Personally, I love using them if for nothing else, for how ubiquitous they are. You can create them from .csv files, excel files, dictionaries, from a sql database, and so forth. This makes them a fabulous datasource around which to build your DataBlocks! . Here are some of my favorite pandas resources: . https://chrisalbon.com/ | https://pandas.pydata.org/docs/ (yah, the docs are really good!) | df = pd.read_csv(path/&#39;train.csv&#39;) df.head() . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . def get_x(r): return path/&#39;train&#39;/r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;].split(&#39; &#39;) def splitter(df): train = df.index[~df[&#39;is_valid&#39;]].tolist() valid = df.index[df[&#39;is_valid&#39;]].tolist() return train, valid dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x=get_x, get_y=get_y, splitter=splitter, # or could just have used ColSplitter() item_tfms=RandomResizedCrop(128, min_scale=0.35)) . Let&#39;s break down our blueprint! . Define the data types for our inputs and targets via the blocks argument. . This is defined as a tuple, where we tell our DataBlock that the imputs are images and our targets are multiple potential categories. Above we can see that these labels are space delimited in the &quot;labels&quot; column. The later essentially returns a one-hot encoded list of possible labels, with a 0 indicating that the label wasn&#39;t found for the item and a 1 indicating otherwise (see the DataBlock.summary results below). . | Define how we&#39;re going to get our images via get_x. . As we&#39;ll be passing in a Dataframe as the raw data source, we don&#39;t need to define a get_items to pull the raw data. We do however, need to instruct the DataBlock as to how to find the images, which we do via the get_x method. That method will get one row of DataFrame (r) at a time. . | Define how, from the raw data, we&#39;re going to create our labels via get_y. . As already mentioned, the classes are in the &quot;labels&quot; column and delimited by a space, and so, we return a list of labels splitting on &#39; &#39;. Easy peasy. . | Define how we&#39;re going to create our validation dataset via splitter . Here we define a custom splitter mostly to just show you how to do it. It has to return at least a tuple of train, validation data. We could have just used ColSplitter (see it in the docs here) . | Define things we want to do for each item via item_tfms . item_tfms are transforms, or things we want to do, to each input individually! Above we only have one which says, &quot;Randomly crop the image to be 128x128 that captures at least 35% of the image each time you grab an image&quot;. See here for more info on RandomResizedCrop . | Define things we want to do for each mini-batch of items via batch_tfms . None here, but remember that these are transforms you want applied to a mini-batch of images on the GPU at the same time. . Important: Do not use lambda functions for defining your DataBlock methods! They can&#8217;t be serialized and so you&#8217;re lucky to get some errors when you try to save/export your DataLoaders and/or Learner . Important: Verify your DataBlock works as expected, or else troubleshoot it, by running DataBlock.summary(data) . | dblock.summary(df) . Setting-up type transforms pipelines Collecting items from fname labels is_valid 0 000005.jpg chair True 1 000007.jpg car True 2 000009.jpg horse person True 3 000012.jpg car False 4 000016.jpg bicycle True ... ... ... ... 5006 009954.jpg horse person True 5007 009955.jpg boat True 5008 009958.jpg person bicycle True 5009 009959.jpg car False 5010 009961.jpg dog False [5011 rows x 3 columns] Found 5011 items 2 datasets of sizes 2501,2510 Setting up Pipeline: get_x -&gt; PILBase.create Setting up Pipeline: get_y -&gt; MultiCategorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} -&gt; OneHotEncode -- {&#39;c&#39;: None} Building one sample Pipeline: get_x -&gt; PILBase.create starting from fname 000012.jpg labels car is_valid False Name: 3, dtype: object applying get_x gives /root/.fastai/data/pascal_2007/train/000012.jpg applying PILBase.create gives PILImage mode=RGB size=500x333 Pipeline: get_y -&gt; MultiCategorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} -&gt; OneHotEncode -- {&#39;c&#39;: None} starting from fname 000012.jpg labels car is_valid False Name: 3, dtype: object applying get_y gives [car] applying MultiCategorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorMultiCategory([6]) applying OneHotEncode -- {&#39;c&#39;: None} gives TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) Final sample: (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) Collecting items from fname labels is_valid 0 000005.jpg chair True 1 000007.jpg car True 2 000009.jpg horse person True 3 000012.jpg car False 4 000016.jpg bicycle True ... ... ... ... 5006 009954.jpg horse person True 5007 009955.jpg boat True 5008 009958.jpg person bicycle True 5009 009959.jpg car False 5010 009961.jpg dog False [5011 rows x 3 columns] Found 5011 items 2 datasets of sizes 2501,2510 Setting up Pipeline: get_x -&gt; PILBase.create Setting up Pipeline: get_y -&gt; MultiCategorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} -&gt; OneHotEncode -- {&#39;c&#39;: None} Setting up after_item: Pipeline: RandomResizedCrop -- {&#39;size&#39;: (128, 128), &#39;min_scale&#39;: 0.35, &#39;ratio&#39;: (0.75, 1.3333333333333333), &#39;resamples&#39;: (2, 0), &#39;val_xtra&#39;: 0.14, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} Building one batch Applying item_tfms to the first sample: Pipeline: RandomResizedCrop -- {&#39;size&#39;: (128, 128), &#39;min_scale&#39;: 0.35, &#39;ratio&#39;: (0.75, 1.3333333333333333), &#39;resamples&#39;: (2, 0), &#39;val_xtra&#39;: 0.14, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) applying RandomResizedCrop -- {&#39;size&#39;: (128, 128), &#39;min_scale&#39;: 0.35, &#39;ratio&#39;: (0.75, 1.3333333333333333), &#39;resamples&#39;: (2, 0), &#39;val_xtra&#39;: 0.14, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=128x128, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) applying ToTensor gives (TensorImage of size 3x128x128, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} starting from (TensorImage of size 4x3x128x128, TensorMultiCategory of size 4x20) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x128x128, TensorMultiCategory of size 4x20) . Now we can create our DataLoaders and take a look at our x&#39;s and y&#39;s, our images and their labels (multiple labeled images have their labels separated by semi-colon) . dls = dblock.dataloaders(df) dls.show_batch() . To get a feel for what our item_tfms (and batch_tfms if we had them) are doing, we can show_batch using a single image as we do below. . dls.show_batch(unique=True) . The combination of what we&#39;re doing in the item_tfms and batch_tfms is known as presizing. . &quot;Presizing is a particular way to do iamge augmentation taht is designed to minimize data destruction while maintaining good performance.&quot; After resizing all the images to a larger dimension that we will train on, we perform all our core augmentations on the GPU. This results in both faster and less destructive transformations of the data. . Important: See pp190-191 for how these augmentations are applied to the training and validation set! . . Train a model . . Important: &quot;Once you think your data looks right, we generally recommend the next step should be using it to train a simple model&quot; See bottom of p193 for why. . Define your loss function . To train a model we need a good loss function that will allow us to optimize the parameters of our model. For multi-label classification tasks where we want to predict a single class/label, to go to is binary cross-entropy loss . Important: Cross-entropy loss is the U.S. Marine of loss functions ... &quot;the few, the proud, the one hot encoded&quot; . Why can&#39;t we just use cross-entropy loss? . Because &quot;the softmax function really wants to pick one class&quot; whereas here want it to pick multiple or even none. . &quot;softmax ... requires that all predictions sum to 1, and tends to push one activation to be much larger than the others (because of the use of exp) ... we may want the sum to be less than 1, if we don&#39;t think any of the categories appear in an image.&quot; . &quot;nll_loss ... returns the value of just one activation: the single activation corresponding with the single label for an item [which] doesn&#39;t make sense when we have multiple labels&quot; . learn = cnn_learner(dls, resnet18) . . Important: Run a batch through model to see/verify your batches and final activations look right. . xb, yb = to_cpu(dls.train.one_batch()) res = learn.model(xb) xb.shape, yb[0], res.shape, res[0] . (torch.Size([64, 3, 128, 128]), TensorMultiCategory([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.]), torch.Size([64, 20]), tensor([ 1.0286, -0.2662, -0.9220, -1.8723, -0.0310, 1.5409, -1.4988, 1.9177, -3.0152, 0.8912, -1.5613, -2.4579, 0.1826, -0.3759, -0.8392, 0.6600, -1.8470, -0.7925, -0.5168, 4.9052], grad_fn=&lt;SelectBackward&gt;)) . So now we need a loss function that will scale those activations to be between 1 and 0 and then compare each activation with the value (0 or 1) in each target column. . def bce(inputs, targets): inputs = inputs.sigmoid() return -torch.where(targets==1, inputs, 1-inputs).log().mean() print(bce(res, yb)) . TensorMultiCategory(1.0280, grad_fn=&lt;AliasBackward&gt;) . So breaking the above down, line by line, for a single input/targets ... . inps = res.sigmoid() print(f&#39;1. {inps[0]}&#39;) print(f&#39;2. {yb[0]}&#39;) print(f&#39;3. {torch.where(yb==1, inps, 1-inps)[0]}&#39;) print(f&#39;4. {torch.where(yb==1, inps, 1-inps)[0].log()}&#39;) print(f&#39;5. {torch.where(yb==1, inps, 1-inps)[0].log().mean()}&#39;) print(f&#39;6. {-torch.where(yb==1, inps, 1-inps)[0].log().mean()}&#39;) . 1. tensor([0.7367, 0.4338, 0.2846, 0.1333, 0.4922, 0.8236, 0.1826, 0.8719, 0.0467, 0.7091, 0.1735, 0.0789, 0.5455, 0.4071, 0.3017, 0.6593, 0.1362, 0.3116, 0.3736, 0.9926], grad_fn=&lt;SelectBackward&gt;) 2. TensorMultiCategory([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.]) 3. TensorMultiCategory([0.2633, 0.5662, 0.7154, 0.8667, 0.4922, 0.1764, 0.8174, 0.1281, 0.0467, 0.2909, 0.8265, 0.9211, 0.4545, 0.5929, 0.3017, 0.6593, 0.8638, 0.6884, 0.6264, 0.0074], grad_fn=&lt;AliasBackward&gt;) 4. TensorMultiCategory([-1.3343, -0.5689, -0.3348, -0.1430, -0.7088, -1.7350, -0.2016, -2.0548, -3.0630, -1.2349, -0.1905, -0.0821, -0.7886, -0.5227, -1.1983, -0.4166, -0.1464, -0.3734, -0.4678, -4.9126], grad_fn=&lt;AliasBackward&gt;) 5. -1.0239182710647583 6. 1.0239182710647583 . ... what is binary cross-entropy loss doing? . Scale all activations to be between 0 and 1 using the sigmoid function (1). The resulting activations tell us, for each potential label, how confident the model is that the value is a &quot;1&quot;. . Build a tensor with a value for each target (2); if the target = 1 then use the corresponding scaled value above ... if the target = 0, then use 1 minus this value (3). Notice how confident correct predictions will be very large, while confident incorrect predictions will be very small. We can think of this value as telling us how right the model was in predicting each label. . Take the log (4) which will will turn correct and more confident predictions (those closer to 1) to a value closer to zero, and wrong and more confident prediction to a value closer to 0. This exactly what we want since the better the model, the smaller the lost, and the log(1) = 0 where as the log(0) approaches negative infinity! See the chart below. . Lastly, because the loss has to be a single value, we mean the losses for each label (5), and then turn it into a positive (6). . plot_function(torch.log, &#39;x (prob correct class)&#39;, &#39;-log(x)&#39;, title=&#39;Negative Log-Likelihood&#39;, min=0, max=1) . . Important: Review BCE and Cross Entropy Loss until you can explain it to your significant other :). They are by and far the most common loss functions you&#8217;ll come across and many of the problems you encounter in training your models will be because you&#8217;re using the wrong one. . Fortunately, PyTorch has a function and module we can use: . loss = F.binary_cross_entropy_with_logits(res, yb) print(loss) # modular form (most commonly used) loss_func = nn.BCEWithLogitsLoss() loss = loss_func(res, yb) print(loss) # and for shits and giggles print(bce(res, yb)) . TensorMultiCategory(1.0280, grad_fn=&lt;AliasBackward&gt;) TensorMultiCategory(1.0280, grad_fn=&lt;AliasBackward&gt;) TensorMultiCategory(1.0280, grad_fn=&lt;AliasBackward&gt;) . . Important: &quot;Normally, for one-hot-encoded targets, you&#8217;ll want F.binary_cross_entropy_with_logits (or nn.BCEWithLogitsLoss), which do both sigmoid and binary cross entropy in a single function. . If the final activations already have the sigmoid applied to it, then you&#39;d use F.binary_cross_entropy (or nn.BCELoss). . Define Your Metrics and Thresholds . One of the trickier bits with multilabel tasks, is selecting at what threshold (probability) do we want to consider a label 1 or 0. Using accuracy as our metric, we can play around with different values ... . learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.941592 | 0.700602 | 0.240737 | 00:28 | . 1 | 0.824755 | 0.558159 | 0.287510 | 00:28 | . 2 | 0.603689 | 0.208358 | 0.810956 | 00:28 | . 3 | 0.360686 | 0.126487 | 0.938645 | 00:28 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.133217 | 0.116908 | 0.947988 | 00:29 | . 1 | 0.116724 | 0.107212 | 0.948586 | 00:29 | . 2 | 0.096901 | 0.104591 | 0.953466 | 00:29 | . . Important: We didn&#8217;t have to specify BCE as our loss function above because fastai was smart enough to figure it out from the dataset. This isn&#8217;t always the case, in particular when you start building your own or use 3rd party models for training. So Trainer beware! . learn.metrics = partial(accuracy_multi, thresh=0.2) print(learn.validate()) learn.metrics = partial(accuracy_multi, thresh=0.9) print(learn.validate()) learn.metrics = partial(accuracy_multi, thresh=0.75) print(learn.validate()) . [0.10459084063768387, 0.9534662365913391] . [0.10459084063768387, 0.9563146829605103] . [0.10459084063768387, 0.9610358476638794] . . Important: &quot;This is much faster if we grab the predictions just once .... Note that by default get_preds applies the output activat function (sigmoid, in this case) for us, so we&#8217;ll need to tell accuracy_multi to not apply it&quot; . preds, targs = learn.get_preds() . print(accuracy_multi(preds, targs, thresh=0.9, sigmoid=False)) . TensorBase(0.9563) . . Important: Find the best threshold by testing over a range of potential thresholds! . thresholds = torch.linspace(0.05, 0.99, 29) accs = [accuracy_multi(preds, targs, thresh=th, sigmoid=False) for th in thresholds] plt.plot(thresholds, accs) . [&lt;matplotlib.lines.Line2D at 0x7fa0e17e8bd0&gt;] . print(accuracy_multi(preds, targs, thresh=0.55, sigmoid=False)) . TensorBase(0.9636) . . Important: &quot;... using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set&quot; is perfectly fine here. &quot;As you see in the plot, changing the threshold in this case results in a smooth curve, so we&#8217;re clearly not picking an inappropriate outlier&quot;. . See p.231 for more discussion on this. . . Summary . You now know how to train both multi-label and muticlass vistion problems, when to use one or another, and what loss function to choose for each. You should consider treating multiclass problems where the predicted class should be &quot;None&quot; as a multi-label problem, especially if this is going to be used in the real-world and not just against some prefabbed dataset. . Also, we&#39;re using accuracy as our metric to optimize the threshold in the example above, but you can use any metric (or combination of metrics you want). For example, a common issue with multi-label tasks is unbalanced datasets where one or more targets are ill represented in number. In that case, it may be more productive to use something like F1 or Recall. . . Resources . https://book.fast.ai - The book&#39;s website; it&#39;s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc... |",
            "url": "https://ohmeow.com/posts/2021/06/10/ajtfb-chapter-6-multilabel.html",
            "relUrl": "/posts/2021/06/10/ajtfb-chapter-6-multilabel.html",
            "date": " ‚Ä¢ Jun 10, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "A Journey Through Fastbook (AJTFB) - Chapter 5",
            "content": "Other posts in this series: A Journey Through Fastbook (AJTFB) - Chapter 1 A Journey Through Fastbook (AJTFB) - Chapter 2 A Journey Through Fastbook (AJTFB) - Chapter 3 A Journey Through Fastbook (AJTFB) - Chapter 4 . Chapter 5 . . Multiclass vs Multi-label classification ... . Yah, it can be confusing! . Anyhow, multiclass classification is all about predicting a SINGLE CLASS an object belongs to from a list of two or more classes. It can be a simple as predicting whether an image is a dog or a cat, or as complex as predicting the breed of dog from amongst dozens of potential breeds. . Multi-label classification (covered in the next chapter) involves predicting MULTIPLE CLASSES to which an object belongs; it can belong to one, some, all, or even none of those classes. For example, you may be looking at satellite photos from which you need to predict the different kinds of terrain (your classes) each contains. . . Defining your DataBlock . Again, the DataBlock is a blueprint for everything required to turn your raw data (images and labels) into something that can be fed through a neural network (DataLoaders with a numerical representation of both your images and labels). Below is the one presented in this chapter. . from fastai.vision.all import * path = untar_data(URLs.PETS) . dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), splitter=RandomSplitter(seed=42), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=.75)) . Let&#39;s break down our blueprint! . Define the data types for our inputs and targets via the blocks argument. . This is defined as a tuple, where we tell our DataBlock that the imputs are images and our targets are a single category (or class or label). . | Define how we&#39;re going to get our raw data via get_items. . We use get_image_files because we are getting image files from the filesystem. When we kick off the DataBlock to build our DataLoaders, we&#39;ll pass in the path to our images which will in turn be passed to get_image_files to pull the raw data. . | Define how, from the raw data, we&#39;re going to create our labels (e.g., the classes for each image) via get_y. . In this case, we don&#39;t need to define a get_x because get_items gets the x&#39;s already. However, since we are working with filenames from which we want to define our labels, we do need this fancy get_y function above. using_attr tells the RegexLabeller what attribute of our data to apply itself too, and since our data is filenames, we tell it to use the .name property of each filename object as the thing the RegexLabeller acts against. That will give us our target class. . | Define how we&#39;re going to create our validation dataset via splitter . | Define things we want to do for each item via item_tfms . item_tfms are transforms, or things we want to do, to each input individually! Above we only have one which says, &quot;Resize each image to 460 max width/height&quot; one by one when we grab it. For individual images to be collated into mini-batches, they have to be the same size ... thus we do this here and not below. . | Define things we want to do for each mini-batch of items via batch_tfms . batch_tfms are transforms, or things we want to do, to a mini-batch of inputs at once on the GPU. aug_transforms includes a bunch that have proven to be effective in computer vision tasks. With the parameters we&#39;re passing into it above (size=224, min_scale=.75), we&#39;re saying, &quot;Take the mini-batch of images here and randomly crop the 460x460 images to be 224x224 that captures at least 3/4 of the image&quot;. See here for more info on RandomResizedCrop. . Important: If you can describe your DataBlock like I have above, you understand it! . Important: Verify your DataBlock works as expected, or else troubleshoot it, by running DataBlock.summary(data) . | dblock.summary(path/&#39;images&#39;) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/oxford-iiit-pet/images/Bombay_111.jpg applying PILBase.create gives PILImage mode=RGB size=604x453 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/oxford-iiit-pet/images/Bombay_111.jpg applying partial gives Bombay applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(3) Final sample: (PILImage mode=RGB size=604x453, TensorCategory(3)) Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=604x453, TensorCategory(3)) applying Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=460x460, TensorCategory(3)) applying ToTensor gives (TensorImage of size 3x460x460, TensorCategory(3)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} starting from (TensorImage of size 4x3x460x460, TensorCategory([ 3, 17, 10, 4], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x460x460, TensorCategory([ 3, 17, 10, 4], device=&#39;cuda:0&#39;)) applying Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} gives (TensorImage of size 4x3x460x460, TensorCategory([ 3, 17, 10, 4], device=&#39;cuda:0&#39;)) applying RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} gives (TensorImage of size 4x3x224x224, TensorCategory([ 3, 17, 10, 4], device=&#39;cuda:0&#39;)) applying Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} gives (TensorImage of size 4x3x224x224, TensorCategory([ 3, 17, 10, 4], device=&#39;cuda:0&#39;)) . Now we can create our DataLoaders and take a look at our x&#39;s and y&#39;s, our pet images and their label/class . dls = dblock.dataloaders(path/&#39;images&#39;) dls.show_batch() . To get a feel for what our batch_tfms are doing, we can show_batch using a single image as we do below. . dls.show_batch(unique=True) . The combination of what we&#39;re doing in the item_tfms and batch_tfms is known as presizing. . &quot;Presizing is a particular way to do iamge augmentation taht is designed to minimize data destruction while maintaining good performance.&quot; After resizing all the images to a larger dimension that we will train on, we perform all our core augmentations on the GPU. This results in both faster and less destructive transformations of the data. . Important: See pp190-191 for how these augmentations are applied to the training and validation set! . . Train a model . . Important: &quot;Once you think your data looks right, we generally recommend the next step should be using it to train a simple model&quot; See bottom of p193 for why. . Define your loss function . To train a model we need a good loss function that will allow us to optimize the parameters of our model. For multiclassification tasks where we want to predict a single class/label, to go to is cross-entropy loss . To understand how this particular loss function operates and its interesting effects, see my prior article &quot;Loss Functions: Cross Entropy Loss and You!&quot; It&#39;s all about how it works, why use it over something like accuracy, and so forth. Pages 194-203 is the place to look in fastbook for more details on the ins and outs of this loss function. . Important: Cross-entropy loss is the Highlander of loss functions ... &quot;there can only be one&quot; . &quot;Intuitively, the softmax function really wants to pick one class ... so it&#39;s ideal for training a classifier when we know each piecture has a definite label. (Note taht it may be less ideal during inference, as you might want your model to sometimes tell you it doesn&#39;t recognize any of the classes taht is has seen during training, and not pick a class because it has a slightly bigger activation score. In this case, it might be better to train a model using multiple binary output columns, each using a sigmoid activation.)&quot; . Train a model . learn = cnn_learner(dls, resnet34, metrics=error_rate) . . Important: Use the Learning Rate Finder to determine a good LR to use during the optimization step! . &quot;One of the most importatn things we can do when training a model is to make sure that we have the right learning rate. If our learning rate is too low, it can take many, many epochs to train our model ... also that we may have problems with overfitting, ceacuse every time we do a complete pass through the data, we give our model a chance to memorize it&quot; . See p 205-206 for more information on how it works, and also Leslie Smith&#39;s paper on it here. A must read for fastai developers!!! . lr_min, lr_steep = learn.lr_find() . lr_min, lr_steep . (0.010000000149011612, 0.0063095735386013985) . . Important: Pick either &quot;one order of magnitude less that where the minimum loss was achieved&quot; (lr_min above, which is actually the true minimum, roughly 1e-1 / 10, 0.01 or else 1e-2) -or- &quot;the last point where the loss was clearly decreasing&quot; These two are likely close to one another, and if you&#39;re not sure which to use, try them both! . learn.fine_tune(2, base_lr=1e-2) . epoch train_loss valid_loss error_rate time . 0 | 0.988194 | 0.477657 | 0.131258 | 01:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.990748 | 0.932280 | 0.228687 | 01:09 | . 1 | 0.537608 | 0.289773 | 0.089986 | 01:07 | . Using fine_tune gives us a nice and quick baseline we can look back at going forward. Nevertheless, we can likely improve our model by taking more control over what parameters are trained (updated), when, and by how much using fit_one_cycle. . So let&#39;s start again, by defining our Learner and finding a good LR for training ONLY the last layer&#39;s parameters (the idea being that the pretrained model we&#39;re finetuning, our backbone, is already pretty good at understanding images ... while the last layer&#39;s parameters are random because they are specific to our task at hand). . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min, lr_steep = learn.lr_find() . learn.fit_one_cycle(2, lr_max=1e-2) . epoch train_loss valid_loss error_rate time . 0 | 1.094374 | 0.674852 | 0.180650 | 01:05 | . 1 | 0.645389 | 0.303023 | 0.096752 | 01:03 | . NOW ... we&#39;re going to &quot;unfreeze&quot; our model, meaning we&#39;re going to make all the parameters trainable. And then we&#39;re going to apply discriminative learning rates, or different base LRs to different parts of our models, with the assumption that earlier layers likely only need to change a little while later layers, especially our classification head, have to change more. This is covered thoroughly in another must read paper, Universal Language Model Fine-tuning for Text Classification . Look at the bottom of the cell below&#39;s output to see the number of traininable parameters for our currently frozen model. . learn.summary() . Sequential (Input shape: 64) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 64 x 112 x 112 Conv2d 9408 False BatchNorm2d 128 True ReLU MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ____________________________________________________________________________ 64 x 128 x 28 x 28 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 8192 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ 64 x 256 x 14 x 14 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 32768 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ 64 x 512 x 7 x 7 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 131072 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ [] AdaptiveAvgPool2d AdaptiveMaxPool2d Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 64 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 64 x 37 Linear 18944 True ____________________________________________________________________________ Total params: 21,830,976 Total trainable params: 563,328 Total non-trainable params: 21,267,648 Optimizer used: &lt;function Adam at 0x7fe0e7fb57a0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Let&#39;s unfreeze and look at the same ... . learn.unfreeze() learn.summary() . Sequential (Input shape: 64) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 64 x 112 x 112 Conv2d 9408 True BatchNorm2d 128 True ReLU MaxPool2d Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True Conv2d 36864 True BatchNorm2d 128 True ReLU Conv2d 36864 True BatchNorm2d 128 True ____________________________________________________________________________ 64 x 128 x 28 x 28 Conv2d 73728 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 8192 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True Conv2d 147456 True BatchNorm2d 256 True ReLU Conv2d 147456 True BatchNorm2d 256 True ____________________________________________________________________________ 64 x 256 x 14 x 14 Conv2d 294912 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 32768 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True Conv2d 589824 True BatchNorm2d 512 True ReLU Conv2d 589824 True BatchNorm2d 512 True ____________________________________________________________________________ 64 x 512 x 7 x 7 Conv2d 1179648 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 131072 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True Conv2d 2359296 True BatchNorm2d 1024 True ReLU Conv2d 2359296 True BatchNorm2d 1024 True ____________________________________________________________________________ [] AdaptiveAvgPool2d AdaptiveMaxPool2d Flatten BatchNorm1d 2048 True Dropout ____________________________________________________________________________ 64 x 512 Linear 524288 True ReLU BatchNorm1d 1024 True Dropout ____________________________________________________________________________ 64 x 37 Linear 18944 True ____________________________________________________________________________ Total params: 21,830,976 Total trainable params: 21,830,976 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fe0e7fb57a0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . ... and as you can see, we&#39;re training everything! . Because what parameters were training has changed, we also need to run the LR finder again to get some guidance on how to set our LRs. . lr_min, lr_steep = learn.lr_find() . So lets see how many parameter/layer groups we have . len(learn.opt.param_groups) . 3 . What we can now do is say, train the first layer group with an LR of 1e-6 ... the last with an LR of 1e-4, and &quot;the layers in between will have learning rates that are multiplicatively equidistnat throughout that range.&quot; Since we only have 3, the middle group will be trained with a starting LR of 1e-5 . Important: We usually choose the first number a bit back from where things start to go bad ... and the last number 1-2 magnitudes lower than the base LR of the frozen model . learn.fit_one_cycle(12, lr_max=slice(1e-6, 1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 0.394857 | 0.291812 | 0.091340 | 01:06 | . 1 | 0.359788 | 0.266810 | 0.084574 | 01:05 | . 2 | 0.315511 | 0.248843 | 0.078484 | 01:05 | . 3 | 0.308508 | 0.245649 | 0.081191 | 01:06 | . 4 | 0.272639 | 0.231762 | 0.077131 | 01:06 | . 5 | 0.231295 | 0.222235 | 0.075778 | 01:06 | . 6 | 0.216695 | 0.223222 | 0.077808 | 01:06 | . 7 | 0.207144 | 0.226977 | 0.075778 | 01:07 | . 8 | 0.191794 | 0.223768 | 0.075778 | 01:07 | . 9 | 0.193688 | 0.222243 | 0.076455 | 01:07 | . 10 | 0.185064 | 0.219272 | 0.076455 | 01:07 | . 11 | 0.177980 | 0.221938 | 0.073072 | 01:07 | . learn.recorder.plot_loss() . . Important: Overfitting does not mean your model isn&#8217;t improving! . &quot;You will often see that the accuracy continues improving, even as the validation loss gets worse. In the end, what matters is your accuracy [or your chosen metric], not the loss. The loss is just the function we&#39;ve given the computer to help us to optimize&quot; . How to choose the number of epochs? . Important: &quot;Your first approach to training should be to simply pick a number of epochs that will train in the amount of time that you are happy to wait for.&quot; . If the model is still getting better, then you haven&#39;t trained your model long enough. . If your metric(s) are getting worse, &quot;if you find that you have overfit, what you should do is retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found.&quot; . When to choose a deeper architecture? . &quot;A larger (more layers and parameters; sometimes described as the capacity of a model) version of ResNet will always be able to give us a better training loss, but it can suffer more from overfitting, because it has more parameters to overfit with. In general, a bigger model has the ability to better capture the real underlying relationships in your data, as well as to capture and memorize the specific details of your individual images.&quot; . So consider these if ... . You aren&#39;t getting the results you need. | Have time to experiment and a big enough GPU to experiment with | You may need to reduce the size of your batches with these bigger models, and you can also us mixed-precision training, in order to get things to run on your GPU. The later results in faster training and gives you the ability to have bigger batch sizes than you would be able to support otherwise. All you need to do is add to_fp16() to your `Learner. . learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16() learn.fine_tune(6, freeze_epochs=1) . Downloading: &#34;https://download.pytorch.org/models/resnet50-19c8e357.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth . . epoch train_loss valid_loss error_rate time . 0 | 0.977287 | 0.299639 | 0.100812 | 01:06 | . epoch train_loss valid_loss error_rate time . 0 | 0.371637 | 0.253767 | 0.080514 | 01:07 | . 1 | 0.375325 | 0.300937 | 0.079161 | 01:07 | . 2 | 0.281241 | 0.309774 | 0.089310 | 01:07 | . 3 | 0.149089 | 0.216163 | 0.059540 | 01:07 | . 4 | 0.093488 | 0.176675 | 0.054127 | 01:07 | . 5 | 0.063744 | 0.169187 | 0.050744 | 01:07 | . . Summary . Well, at this point, you know how to train a multiclassification computer vision task. So, go train one for yourself using what you&#39;ve learned! . Make sure you understand and why we use cross-entropy loss for multiclassification problems. I can&#39;t tell you how many times I&#39;ve responded to questions about why someone&#39;s model wasn&#39;t training only to find out the reason was because they had the wrong loss function. Know it, love it, use it :) . Also, so much of what fastai incorporates from the LR finder and the fit_one_cycle, comes from Leslie Smith&#39;s research. Checkout the &quot;Resources&quot; section below for some of his more influential papers, all of which I&#39;ve read and encourage the rest of you to read if you want some inside scoop about why fastai works the way it does. I guarantee, reading and studying those papers will make you a better deep learning practioner and a better fastai developer in particular! You may think you&#39;re not ready to start reading academic papers at chapter 5, but believe me, I&#39;ve been there, and you are :) . . Resources . https://book.fast.ai - The book&#39;s website; it&#39;s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc... . | Cyclical Learning Rates for Training Neural Networks . | Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates . | A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay . | Universal Language Model Fine-tuning for Text Classification . | fastai: A Layered API for Deep Learning . |",
            "url": "https://ohmeow.com/posts/2021/06/03/ajtfb-chapter-5.html",
            "relUrl": "/posts/2021/06/03/ajtfb-chapter-5.html",
            "date": " ‚Ä¢ Jun 3, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Contributing to fastai: Setup your local development environment & submit a PR",
            "content": "Steps . Configure your local development environment . Install the github CLI, gh. Instructions for all the OS flavor are here! . The CLI makes it trivial to work on open source projects. In particular, it really shines when making a PR as you&#39;ll see below. Here&#39;s the link to the cli&#39;sexcellent documentation which you&#39;ll likely be referring to again and again (so keep it handy). . | Clone the fastai repo locally . gh repo clone https://github.com/fastai/fastai.git cd ./fastai . | Build your conda environment (I&#39;m using mamba based on Jeremy Howard&#39;s recommendation) . Mamba is a makes issuing conda commands faster! Basically just replace conda with mamba whenever you are working with packages in your environment. . mamba env create -f environment.yml . | Activate the environment (you want to make sure you&#39;re in the fastai environment going forward) conda activate fastai . | Install Jupyter and extensions into your fastai environment (I do this all the time because it leads to less problems when trying to use a base install of jupyter notebook for everything) mamba install -c conda-forge notebook mamba install -c conda-forge jupyter_contrib_nbextensions . | Install nbdev and run the nbdev_install_git_hooks script per the fastai docs mamba install -c fastai nbdev nbdev_install_git_hooks . | Create a symlink from /nbs/fastai to fastai to make sure the notebooks can find the fastai library which is up one level from the notebooks cd ./nbs ln -s ../fastai fastai cd .. . | At this point your local development environment is good to go! Run jupyter notebook, open your browser, and head over to the /nbs folder to begin. . Submit a PR . With the github CLI, its amazingly easy! Once you&#39;ve made your changes and added your unit tests all you have to do is: . Make sure you&#39;re local repo is up-to-date BEFORE you start working. In fact, this is a good command to run periodically so you don&#39;t have to deal with any conflicts once you make your PR. . git pull . | Commit your changes to git . git commit -am &#39;My amazing addition to fastai here&#39; . | Submit a PR using gh . gh pr create --title &quot;My amaizing change&quot; --body &quot;Here&#39;s what you need to know about it!&quot; . There are actually a few ways to issue the pr, but the above is the easiest. Check out the gh pr create docs for more options. . Once you do this, you&#39;ll be asked some questions about where to push the branch and offer an option to fork the base repository under your own account. Easy peasy friends. . | Congrats! You&#39;re a contributor now. . Summary . But that&#39;s not all! There are all kinds of cool things you can do using the github CLI including monitoring the status of your PRs and also fixing them (which you&#39;ll likely have to do when the base repo owners ask you to make changes of one sort or another). It&#39;s all in the docs and it&#39;s all fairly straightforward! .",
            "url": "https://ohmeow.com/posts/2021/06/02/how-to-submit-a-pr-to-fastai.html",
            "relUrl": "/posts/2021/06/02/how-to-submit-a-pr-to-fastai.html",
            "date": " ‚Ä¢ Jun 2, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Multilingual Sequence Classifaction with the MBart Family",
            "content": "!pip install ohmeow-blurr -q !pip install datasets -q . from fastai.text.all import * from datasets import load_dataset from transformers import AutoModelForSequenceClassification from blurr.utils import BLURR from blurr.data.core import * from blurr.modeling.core import * . from blurr import __version__ as blurr_version from fastai import __version__ as fa_version from torch import __version__ as pt_version from transformers import __version__ as hft_version print(f&#39;Using blurr {blurr_version}&#39;) print(f&#39;Using pytorch {pt_version}&#39;) print(f&#39;Using fastai {fa_version}&#39;) print(f&#39;Using transformers {hft_version}&#39;) . Using blurr 0.0.25 Using pytorch 1.8.1+cu101 Using fastai 2.3.1 Using transformers 4.6.1 . Data . trn_ds, val_ds, tst_ds = load_dataset(&quot;amazon_reviews_multi&quot;, &quot;de&quot;, split=[&#39;train&#39;, &#39;validation&#39;, &#39;test&#39;]) . Reusing dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/de/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609) . trn_df = pd.DataFrame(trn_ds) val_df = pd.DataFrame(val_ds) tst_df = pd.DataFrame(tst_ds) . . Important: Use a subset of the data when building your model to speed up developement time! . After you got everything, throw the full dataset at it and go get some coffee :) . # this won&#39;t work because the rows are ordered by our targets! # trn_ds, val_ds = load_dataset(&quot;amazon_reviews_multi&quot;, &quot;de&quot;, split=[&#39;train[:10%]&#39;, &#39;validation[:10%]&#39;]) trn_df = trn_df.sample(frac=0.05) val_df = val_df.sample(frac=0.05) . trn_df[&#39;is_valid&#39;] = False; val_df[&#39;is_valid&#39;] = True df = pd.concat([trn_df, val_df]) print(len(trn_df), len(val_df), len(df)) . 10000 250 10250 . trn_df.head() . language product_category product_id review_body review_id review_title reviewer_id stars is_valid . 115812 de | apparel | product_de_0727194 | f√ºr so einen Preis muss ich dazusagen, es h√§lt was es verspricht, nat√ºrlich ist hier keine hohe Qualit√§t zu erwarten, aber f√ºr solchen Preis recht gut. ist. Die N√§hte gehen mit Zeit auf | de_0580512 | Preis Leistung Ok | reviewer_de_0542714 | 3 | False | . 58074 de | toy | product_de_0014211 | Die Qualit√§t ist nicht gut. Nur nach kurzem nutzen sind die plastikverkleidungen abgefallen. Der Ball l√§sst sich schlecht f√ºr Kinder entfernen. Die Scheiben sehen nach kurzer Nutzung aus als w√§ren sie Jahre im Gebrauch. W√ºrde ich nicht wieder kaufen. | de_0100470 | Schlechte Qualit√§t | reviewer_de_0719961 | 2 | False | . 122195 de | toy | product_de_0304268 | Erf√ºllt seinen Zweck und sie gut aus. | de_0811174 | Toller Lederbeutel | reviewer_de_0999635 | 4 | False | . 75151 de | other | product_de_0720910 | Ich mach es kurz; Kauft euch das Album nicht und behaltet Rise Against so on Erinnerung wie sie fr√ºher waren. Alles nach Appeal to reason ist generisch, einfallslos und irgendwie langweilig geworden. The Violence ist der einzige Track der mir vielleicht im Ged√§chtnis bleiben k√∂nnte. | de_0713777 | Langweilg | reviewer_de_0130191 | 2 | False | . 94760 de | digital_video_download | product_de_0847576 | Fand die Twists im Film ziemlich vorhersehbar. Dennoch fand ich den Film ansich ganz cool, da ich 1. Den Schauspieler sehr mag und 2. Diese Thematik immer wieder spannend finde. F√ºr abends zuhause mit dem Partner auf der Couch ein solider Film, den man durchaus mal schauen kann, aber auch kein Super hollywood Kino erwartet | de_0471539 | Solider Film f√ºr zuhause | reviewer_de_0223020 | 3 | False | . unique_tgt_vals = trn_df.stars.value_counts() print(unique_tgt_vals) labels = sorted(list(df.stars.unique())) print(labels) . 4 2054 3 2054 5 2025 2 1946 1 1921 Name: stars, dtype: int64 [1, 2, 3, 4, 5] . huggingface objects . model_name = &quot;facebook/mbart-large-50&quot; model_cls = AutoModelForSequenceClassification hf_tok_kwargs = {&#39;src_lang&#39;: &#39;de_DE&#39;, &#39;tgt_lang&#39;: &#39;de_DE&#39;} hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(model_name, model_cls=model_cls, tokenizer_kwargs=hf_tok_kwargs, config_kwargs={&#39;num_labels&#39;: len(labels)}) print(&#39;arch: &#39;, type(hf_arch)) print(&#39;config: &#39;, type(hf_config)) print(&#39;tokenizer: &#39;, type(hf_tokenizer)) print(&#39;model: &#39;, type(hf_model)) . arch: &lt;class &#39;str&#39;&gt; config: &lt;class &#39;transformers.models.mbart.configuration_mbart.MBartConfig&#39;&gt; tokenizer: &lt;class &#39;transformers.models.mbart.tokenization_mbart50_fast.MBart50TokenizerFast&#39;&gt; model: &lt;class &#39;transformers.models.mbart.modeling_mbart.MBartForSequenceClassification&#39;&gt; . . Important: Always good to look into the config as you&#8217;ll often find good defaults to use in your training and inference! . hf_config . MBartConfig { &#34;_name_or_path&#34;: &#34;facebook/mbart-large-50&#34;, &#34;_num_labels&#34;: 3, &#34;activation_dropout&#34;: 0.0, &#34;activation_function&#34;: &#34;gelu&#34;, &#34;add_bias_logits&#34;: false, &#34;add_final_layer_norm&#34;: true, &#34;architectures&#34;: [ &#34;MBartForConditionalGeneration&#34; ], &#34;attention_dropout&#34;: 0.0, &#34;bos_token_id&#34;: 0, &#34;classif_dropout&#34;: 0.0, &#34;classifier_dropout&#34;: 0.0, &#34;d_model&#34;: 1024, &#34;decoder_attention_heads&#34;: 16, &#34;decoder_ffn_dim&#34;: 4096, &#34;decoder_layerdrop&#34;: 0.0, &#34;decoder_layers&#34;: 12, &#34;decoder_start_token_id&#34;: 2, &#34;dropout&#34;: 0.1, &#34;early_stopping&#34;: true, &#34;encoder_attention_heads&#34;: 16, &#34;encoder_ffn_dim&#34;: 4096, &#34;encoder_layerdrop&#34;: 0.0, &#34;encoder_layers&#34;: 12, &#34;eos_token_id&#34;: 2, &#34;forced_eos_token_id&#34;: 2, &#34;gradient_checkpointing&#34;: false, &#34;id2label&#34;: { &#34;0&#34;: &#34;LABEL_0&#34;, &#34;1&#34;: &#34;LABEL_1&#34;, &#34;2&#34;: &#34;LABEL_2&#34;, &#34;3&#34;: &#34;LABEL_3&#34;, &#34;4&#34;: &#34;LABEL_4&#34; }, &#34;init_std&#34;: 0.02, &#34;is_encoder_decoder&#34;: true, &#34;label2id&#34;: { &#34;LABEL_0&#34;: 0, &#34;LABEL_1&#34;: 1, &#34;LABEL_2&#34;: 2, &#34;LABEL_3&#34;: 3, &#34;LABEL_4&#34;: 4 }, &#34;max_length&#34;: 200, &#34;max_position_embeddings&#34;: 1024, &#34;model_type&#34;: &#34;mbart&#34;, &#34;normalize_before&#34;: true, &#34;normalize_embedding&#34;: true, &#34;num_beams&#34;: 5, &#34;num_hidden_layers&#34;: 12, &#34;output_past&#34;: true, &#34;pad_token_id&#34;: 1, &#34;scale_embedding&#34;: true, &#34;static_position_embeddings&#34;: false, &#34;tokenizer_class&#34;: &#34;MBart50Tokenizer&#34;, &#34;transformers_version&#34;: &#34;4.6.1&#34;, &#34;use_cache&#34;: true, &#34;vocab_size&#34;: 250054 } . DataLoaders . blocks = (HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=256), CategoryBlock) dblock = DataBlock(blocks=blocks, get_x=ColReader(&#39;review_body&#39;), get_y=ColReader(&#39;stars&#39;), splitter=ColSplitter()) dls = dblock.dataloaders(df, bs=4) . . Important: It&#8217;s almost always useful to look at the shape of things in your batches (esp. when debugging) For example, when running on a colab GPU I kept getting CUDA OOM even with a batch size of just 4. So I looked at the input_ids and saw that they were over 1,000 tokens long in some batches. So I adjusted the max_length above to ensure they weren&#39;t longer that 128 characters and voila, you have the tutorial before you now. . Of course, you should run this with the biggest batch size and sequence size your GPU(s) will support. . xb, yb = dls.one_batch() xb[&#39;input_ids&#39;].shape . torch.Size([4, 256]) . dls.show_batch(dataloaders=dls, max_n=2, trunc_at=1500) . text category . 0 Ich habe mir f√ºr mein neues Apple iPad (2018er Modell) diese Folie zugelegt. Das ausschlaggebende Kriterium hierbei war nat√ºrlich die durch den Hersteller beworbene besondere Oberfl√§chenstruktur der Folie. Ich benutze das iPad sehr oft zusammen mit dem Apple Pen und fand die Idee, dass sich der Stift beim Schreiben wie ein Bleistift auf einem Blatt Papier anf√ºhlt, sehr verhei√üungsvoll. Dementsprechend war ich nat√ºrlich sehr gespannt darauf, ob die Folie denn auch h√§lt, was der Hersteller verspricht. Dank Amazon kam die Lieferung wie gewohnt z√ºgig an und nach dem ersten Auspacken zeigte sich, dass der Hersteller nicht zu viel versprochen hatte. Die Oberfl√§che der Folie f√ºhlte sich in der Tat an wie ein Blatt Papier und auch die ersten &quot;Trocken√ºbungen&quot; mit dem Apple Pen f√ºhlten sich beinahe an wie echt. Das anbringen der Folie war dann aber leider (wie bei allen Folien) eine Herausforderung. Obwohl ich mich an die Anleitung gehalten hatte und besonders auf eine staubfreie Umgebung achtete schaffte ich es nicht die erste Folie g√§nzlich ohne Luft- und Staubeinschl√ºsse auf meinem iPad anzubringen | 2 | . 1 Der erste Punkt das Design ist schlicht und schick. Die Uhr kann man zum Sport wie auch auf der Arbeit und in der Freizeit ohne Probleme anziehen. Das Laden der Fitnessuhr gestaltet sich ebenfalls intuitiv und schnell. Durch die USB- Schnittstelle kann die Uhr mit vorhanden Ger√§ten geladen werden. Das waren dann auch schon die positiven Punkte der Uhr. Die Kopplung mit dem Handy funktioniert nur bedingt. Eine direkte Bluetooth Verbindung zwischen Handy und Uhr ist unm√∂glich. Durch die App k√∂nnen die Ger√§te zwar gekoppelt werden, diese weist aber erhebliche Fehlerstellen auf. Oft verbindet sich das Handy erst wieder mit der Uhr nach einem Neustart der Ger√§te. Die Uhr besitzt keinen eigenen Knopf zum An- und Ausschalten. Das automatische Aufblenden der Anzeige funktioniert ebenfalls nur bedingt und gef√ºhlt nach dem Zufallsprinzip. Die Hilfestellungen durch Bedingungsanleitung und App bringen einen nicht wirklich weiter. Wer ausschlie√ülich eine Uhr zum tracken der Schritte und Herzfrequenz sucht ist mit der Uhr gut aufgehoben. Zum t√§glichen Gebrauch als Uhrenersatz nicht zu empfehlen. | 2 | . Training . Print out the model so we can build a custom set of parameter groups for an MBart + Sequence Classification task . . def mbart_splitter(m): model = m.hf_model if (hasattr(m, &#39;hf_model&#39;)) else m embeds_modules = [ model.model.encoder.embed_positions, model.model.encoder.embed_tokens, model.model.decoder.embed_positions, model.model.decoder.embed_tokens ] embeds = nn.Sequential(*embeds_modules) groups = L(embeds, model.model.encoder, model.model.decoder, model.classification_head) return groups.map(params).filter(lambda el: len(el) &gt; 0) . Configure our metrics and callbacks required by blurr . precision = Precision(average=&#39;macro&#39;) recall = Recall(average=&#39;macro&#39;) f1 = F1Score(average=&#39;macro&#39;) learn_metrics = [accuracy, precision, recall, f1] learn_cbs = [HF_BaseModelCallback] . Configure our Learner and train away ... . model = HF_BaseModelWrapper(hf_model) learn = Learner(dls, model, opt_func=Adam, loss_func=CrossEntropyLossFlat(), metrics=learn_metrics, cbs=learn_cbs, splitter=mbart_splitter) learn.freeze() print(len(learn.opt.param_groups)) . 4 . learn.lr_find(suggestions=True) . SuggestedLRs(lr_min=6.918309736647643e-07, lr_steep=0.001737800776027143) . learn.fit_one_cycle(1, lr_max=7e-5) . epoch train_loss valid_loss accuracy precision_score recall_score f1_score time . 0 | 1.151728 | 0.951163 | 0.592000 | 0.574056 | 0.578043 | 0.575234 | 05:23 | . learn.show_results(learner=learn, max_n=2, trunc_at=1500) . /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) . text category target . 0 Die Qualit√§t meiner neuen Hausschuhe ist, wenn ich das nach drei Tagen schon sagen kann, sehr gut. Durch den sehr weichen Stoff &quot;schmeicheln&quot; sie regelrecht den F√º√üen. Es l√§sst sich wunderbar darauf laufen und -besonders wichtig f√ºr mich- sie halten auch sch√∂n warm. Was mir nicht gef√§llt, und deshalb ziehe ich auch einen Stern ab, ist die hintere Umrandung des Obermaterials. Hausschuhe sind ja hinten eigentlich flach, sodass ich sch√∂n in den Schuh rein- und rausschl√ºpfen kann. Durch die 1 cm Kante, die wahrscheinlich deshalb eingearbeitet wurde, um mehr Halt im Schuh zu haben, macht mich beim Treppensteigen sehr unsicher. Obwohl der Schuh an sich wunderbar passt, gibt mir die hintere Kante das Gef√ºhl, jederzeit beim Laufen aus dem Schuh zu rutschen. Und beim Treppensteigen verst√§rkt sich dieses Gef√ºhl. Ein klarer Minuspunkt; obwohl das sehr subjektiv ist. Manche werden wahrscheinlich nicht verstehen, was ich damit meine. F√ºr knapp 20 Euro habe ich jedenfalls einen tollen Gegenwert erhalten. Meine letzten Hausschuhe, f√ºr die ich hier bei Amazon seinerzeit f√ºr knapp 40 Euro bezahlt habe, sind nach drei Jahren total aus | 4 | 3 | . 1 Da ich unsere Fr√ºhst√ºcksbr√∂tchen gerne selber backe, habe ich mir das Baguetteblech besorgt. Das Blech ist wie abgebildet und funktioniert super. Passt genau auf ein Backblech. Reinigung ist sehr leicht, ich habe √ºbrig gebliebene Mehl einfach mit Wasser abgewaschen. Einfetten war nicht von N√∂ten. Ich habe das Blech nur mit Mehl best√§ubt, bevor ich die Br√∂tchen drauf gelegt habe. Einen Stern gibt es Abzug, da bei der Lieferung bereits einige Kratzer in der Beschichtung waren. Ansonsten bin ich bisher sehr zufrieden! Nachtrag - √Ñnderung der Punktezahl: Mittlerweile bin ich nicht mehr so von dem Backblech angetan. Der Teig bleibt immer √∂fter kleben und zuletzt waren die Br√∂tchen unten sogar schwarz gef√§rbt. Das war nicht verbrannt sondern vom Backblech, ich wei√ü nicht ob sich die Beschichtung l√∂st, oder ob das die Farbe ist. Aber das ist mit Sicherheit nicht gesund. Deswegen nur noch 2 Sterne! | 2 | 3 | . We&#39;ll freeze all the layers with the exception of the decoder and classification_head layers (the last 2) . learn.freeze_to(-2) . learn.lr_find(suggestions=True) . SuggestedLRs(lr_min=1.4454397387453355e-06, lr_steep=9.12010818865383e-07) . learn.fit_one_cycle(3, lr_max=slice(2e-8, 2e-7)) . epoch train_loss valid_loss accuracy precision_score recall_score f1_score time . 0 | 1.020526 | 0.942416 | 0.612000 | 0.590443 | 0.597485 | 0.591900 | 10:13 | . 1 | 1.020601 | 0.935697 | 0.612000 | 0.593317 | 0.598285 | 0.594764 | 10:13 | . 2 | 1.063076 | 0.934123 | 0.616000 | 0.597113 | 0.602207 | 0.598684 | 10:13 | . learn.recorder.plot_loss() . learn.show_results(learner=learn, max_n=2, trunc_at=1500) . /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) . text category target . 0 Die Qualit√§t meiner neuen Hausschuhe ist, wenn ich das nach drei Tagen schon sagen kann, sehr gut. Durch den sehr weichen Stoff &quot;schmeicheln&quot; sie regelrecht den F√º√üen. Es l√§sst sich wunderbar darauf laufen und -besonders wichtig f√ºr mich- sie halten auch sch√∂n warm. Was mir nicht gef√§llt, und deshalb ziehe ich auch einen Stern ab, ist die hintere Umrandung des Obermaterials. Hausschuhe sind ja hinten eigentlich flach, sodass ich sch√∂n in den Schuh rein- und rausschl√ºpfen kann. Durch die 1 cm Kante, die wahrscheinlich deshalb eingearbeitet wurde, um mehr Halt im Schuh zu haben, macht mich beim Treppensteigen sehr unsicher. Obwohl der Schuh an sich wunderbar passt, gibt mir die hintere Kante das Gef√ºhl, jederzeit beim Laufen aus dem Schuh zu rutschen. Und beim Treppensteigen verst√§rkt sich dieses Gef√ºhl. Ein klarer Minuspunkt; obwohl das sehr subjektiv ist. Manche werden wahrscheinlich nicht verstehen, was ich damit meine. F√ºr knapp 20 Euro habe ich jedenfalls einen tollen Gegenwert erhalten. Meine letzten Hausschuhe, f√ºr die ich hier bei Amazon seinerzeit f√ºr knapp 40 Euro bezahlt habe, sind nach drei Jahren total aus | 4 | 3 | . 1 Die Lampe machte zun√§chst einen ordentlichen Eindruck. Jedoch waren einige LEDs bereits bei der Auslieferung defekt. Der daraufhin kontaktierte Lieferant ignorierte mein Anschreiben. Die dazugeh√∂rige Zeitschaltuhr hat alle paar Wochen den Dienst quittiert. Da ich nicht so h√§ufig in den Keller gehe, in dem die Lampe bei der √úberwinterung Licht spenden sollte, stellte ich dies meist - offenbar jeweils einige Tage nach dem Ausfall fest - da die Pflanzen kurz vor dem krepieren waren. Die Zeitschaltuhr lie√ü sich mehrmals wieder aktivieren, ist nun aber nach wenigen Monaten ganz defekt, die Lampe l√§sst sich nicht mehr einschalten. Die Pflanzen sind nun alle eingegangen. Ich rate dringend von dem Kauf des Artikels ab. | 1 | 1 | . Inference . txt = tst_df.review_body[0] print(txt) learn.blurr_predict(txt) . Leider, leider nach einmal waschen ausgeblichen . Es sieht super h√ºbsch aus , nur leider stinkt es ganz schrecklich und ein Waschgang in der Maschine ist notwendig ! Nach einem mal waschen sah es aus als w√§re es 10 Jahre alt und hatte 1000 e von Waschg√§ngen hinter sich :( echt schade ! . [((&#39;1&#39;,), (#1) [tensor(0)], (#1) [tensor([0.5725, 0.3474, 0.0716, 0.0067, 0.0018])])] . txts = list(tst_df.review_body.values[1:10]) print(txts) learn.blurr_predict(txts) . [&#39;zun√§chst macht der Anker Halter einen soliden Eindruck. Die Magnethalterung ist auch brauchbar. Was gar nicht geht ist die Tatsache, dass die Halterung f√ºr runde L√ºftungsd√ºsen, anders als vom Hersteller beschrieben, nicht geeignet ist! St√§ndig f√§llt das Smartphone runter. Durch das h√§ufige Wiederanbringen ist nun auch die Gummierung kaputt, was zur Folge hat, dass die L√ºftungsd√ºse sch√∂n zerkratzt wird! Also Schrott, der auch noch mein Auto besch√§digt! F√ºr mich ist das nicht brauchbar!&#39;, &#39;Siegel sowie Verpackung war besch√§digt und ware war gebraucht mit Verschlei√ü und Fingerabdr√ºcke. Zur√ºck geschickt und bessere qualitativere Artikel gekauft.&#39;, &#39;Habe dieses Produkt NIE erhalten und das Geld wurde nicht r√ºckerstattet!!!!!!!&#39;, &#39;Die Tr√§ger sind schnell abgerissen&#39;, &#39;Druckbild ist leider nicht akzeptabel. Die kompletten seiten werden grau eingef√§rbt. Verk√§ufer antwortet nicht auf Emails. Deshalb absolut nicht empfehlenswert.&#39;, &#39;ü§¨ü§¨ü§¨ Stoff l√∂st sich nach kurzer Zeit&#39;, &#39;Beim zweiten Gebrauch bereits undicht!!!&#39;, &#39;Die Lieferung war prompt. 2 Gl√§ser sind bereits undicht und Wasser befindet sich in den Zwischenr√§umen... was nun?&#39;, &#39;Bin √ºberhaupt nicht zufrieden. Das Handy ist mir einmal kurz, aus minimalen H√∂he ca 30cm, auf den Tisch gefallen und die Folie ist schon wieder kaputt. MfG Sonja Sax&#39;] . [((&#39;2&#39;,), (#1) [tensor(1)], (#1) [tensor([0.1873, 0.5228, 0.2539, 0.0327, 0.0032])]), ((&#39;1&#39;,), (#1) [tensor(0)], (#1) [tensor([0.6606, 0.2255, 0.0867, 0.0233, 0.0038])]), ((&#39;1&#39;,), (#1) [tensor(0)], (#1) [tensor([9.5275e-01, 4.1451e-02, 5.0995e-03, 3.2897e-04, 3.6943e-04])]), ((&#39;1&#39;,), (#1) [tensor(0)], (#1) [tensor([0.3692, 0.2738, 0.2347, 0.0907, 0.0316])]), ((&#39;1&#39;,), (#1) [tensor(0)], (#1) [tensor([8.1323e-01, 1.5297e-01, 3.0495e-02, 2.6044e-03, 6.9975e-04])]), ((&#39;4&#39;,), (#1) [tensor(3)], (#1) [tensor([0.0447, 0.1097, 0.2021, 0.3402, 0.3033])]), ((&#39;1&#39;,), (#1) [tensor(0)], (#1) [tensor([0.6773, 0.1921, 0.0808, 0.0320, 0.0179])]), ((&#39;3&#39;,), (#1) [tensor(2)], (#1) [tensor([0.1179, 0.2596, 0.4055, 0.1970, 0.0200])]), ((&#39;1&#39;,), (#1) [tensor(0)], (#1) [tensor([0.6736, 0.2547, 0.0619, 0.0078, 0.0020])])] . Well that&#39;s it! . I hope this article helps your fastai, huggingface, blurr out, and hey, if I&#39;m doing something wrong above please let me know! I&#39;m far from perfect :) . For more information on the MBart/MBar-50 architecture, see the huggingface docs here. .",
            "url": "https://ohmeow.com/posts/2021/05/25/mbart-sequence-classification-with-blurr.html",
            "relUrl": "/posts/2021/05/25/mbart-sequence-classification-with-blurr.html",
            "date": " ‚Ä¢ May 25, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "A Journey Through Fastbook (AJTFB) - Chapter 4",
            "content": "Other posts in this series: A Journey Through Fastbook (AJTFB) - Chapter 1 A Journey Through Fastbook (AJTFB) - Chapter 2 A Journey Through Fastbook (AJTFB) - Chapter 3 . Chapter 4 . . How to visualize a grayscale image in pandas ... . mnist_path = untar_data(URLs.MNIST_SAMPLE) mnist_path.ls() . (#3) [Path(&#39;/root/.fastai/data/mnist_sample/labels.csv&#39;),Path(&#39;/root/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/root/.fastai/data/mnist_sample/train&#39;)] . sample_3 = Image.open((mnist_path/&#39;train/3&#39;).ls().sorted()[1]) sample_3 . sample_3_t = tensor(sample_3) df = pd.DataFrame(sample_3_t[4:15, 4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . . What is a baseline model and why do you want one? . A simple model that you are confident should perform reasonably well. It should be simple to implement and easy to test . Why do you want to start with a baseline model? . ... without starting with a sensible baseline, it is difficult to know whether your super-fancy models are any good . How do you build/find one of these models? . You can search online for folks that have trained models to solve a problem similar to your&#39;s and/or you can start with one of the high-level examples in the fastai docs against your data. There are a bunch covering core vision, text, tabuluar and colab filtering tasks right here. . . Tensors . What is a &quot;Tensor&quot;? . Like a numpy array, but with GPU support. The data it contains must be of the same type and must conform in rectangular shape. . Important: &quot;try to avoid as much as possible writing loops, and replace them by commands that work directly on arrays or tensors&quot; . Let&#39;s take a look .. . threes = (mnist_path/&#39;train/3&#39;).ls().sorted() len(threes), threes[0] . (6131, Path(&#39;/root/.fastai/data/mnist_sample/train/3/10.png&#39;)) . all_threes = [ tensor(Image.open(fp)) for fp in threes ] len(all_threes), all_threes[0].shape . (6131, torch.Size([28, 28])) . stacked_threes = torch.stack(all_threes).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . Important information about tensors include its shape, rank, and type: . print(&#39;shape: &#39;, stacked_threes.shape) # rank = the total number of axes print(&#39;rank: &#39;, stacked_threes.ndim) # type = the datatype of its contents print(&#39;type: &#39;, stacked_threes.dtype) . shape: torch.Size([6131, 28, 28]) rank: 3 type: torch.float32 . Important things you can do to a tensor, view, @, where . stacked_threes_rank_2 = stacked_threes.view(-1, 28*28) print(&#39;orig. shape: &#39;, stacked_threes.shape) print(&#39;make into a rank 2 tensor&#39;, stacked_threes_rank_2.shape) # @ = operator for matrix multiplication print(&#39;result of matrix multiplication: &#39;, (stacked_threes @ torch.randn((1,28,28))).shape) # where = torch.where(a,b,c) =&gt; [b[i] if a[i] else c[i] for i in range(len(a))] ... see p.167 trgs = tensor([1,0,1]) preds = tensor([0.9, 0.4, 0.2]) def mnist_loss(preds, targs): return torch.where(targs == 1, 1 - preds, preds).mean() print(&#39;output of where: &#39;, mnist_loss(preds, trgs)) . orig. shape: torch.Size([6131, 28, 28]) make into a rank 2 tensor torch.Size([6131, 784]) result of matrix multiplication: torch.Size([6131, 28, 28]) output of where: tensor(0.4333) . For an interactive lesson on matrix multiplication, this is the best! . Check out pp.145-148 to learn about &quot;broadcasting&quot;, a critical piece to understanding how you can and should manipulate tensors or numpy arrays! . . Stochastic Gradient Descent - How to train a model . Here are the steps: . INITIALIZE the weights = initializing parameters to random values . | For each image, PREDICT whether it is a 3 or 7 . | Based on the predictions, calculate how good the model is by calculating its LOSS (small is good) . | Calculate the GRADIENT, &quot;which measures for each weight how changing the weight would change the loss&quot; . | STEP, change all the weights based on the gradient . | Starting at step 2, REPEAT . | STOP when you don&#39;t want to train any longer or the model is good enough . | Below, we&#39;ll delve deeper into these steps. We&#39;ll do this by getting a big more into the sample code beginning on p.150 ... . Step 1: Initializing weights . One way is presented on p.164: . def init_params(size, std=1.0) return (torch.randn(size)*std).requires_grad_() weights = init_params((28*28,1)) #=&gt; raturns a rank 2, 784x1 tensor, with random values . Step 3: Calculating the loss . . Important: &quot;For continuous data, it&#8217;s common to use mean squared error&quot;. In order to understand how to write this, read it right-to-left (e.g., error -&gt; square -&gt; mean) . def mse(preds, targs): return ((preds-targs)**2).mean() # in PyTorch loss = F.mse_loss(preds, targs) . . Important: Accuracy is a bad loss function . Why is accuracy a poor loss function? . &quot;The gradient of a funciton is its slope, or its steepness ... how much the value of the function goes up or down, divided by how much we changed the input (y_new - y_old) / (x_new - x_old) .... The problem with [accuracy] is that a small change in weights from x_old to x_new isn&#39;t likely to cause any prediction to change, so (y_new - y_old) will almost always be 0 ... **the gradient is 0 almost everywhere. A very small change in the value of a weight will often not change the accuracy at all . A gradient = 0 will mean that the weights aren&#39;t updated. . Important:&quot;We need a loss function that, when our weights result in slightly better predictions, gives us a slightly better loss&quot; . Metrics v. Loss . Important: &quot;... the metric is to drive human understanding and the loss is to drive automated learning. . Important: &quot;... focus on these metrics, rather than the loss, when judging the performance of a model.&quot; . Important: &quot;... the loss must be a function that has a meaningful derivative ... must be reasonably smooth [so] that [it] would respond to small changes in confidence level. . The loss function is one that can be optimized using its gradient! . Step 4: Calculating the gradients . . Important: &quot;the gradients tell us how much we have to change each weight to make our model better ... allows us to more quickly calculate whether our loss will go up or down we we make those adjustments&quot; . Important: &quot;The gradients tell us only the slope of our function; they don&#8217;t tell us exactly how far to adjust the parameters. But they do give us some idea of how far&quot; (large slope = bigger adjustments needed whereas a small slope suggests we are close to the optimal value) . &quot;The derivative of a function tells you how much a change in its parameters will change its result&quot; . Remember: We are calculating a gradient for EVERY weight so we know how to adjust it to make our model better (i.e., lower the LOSS) . requires_grad tells PyTorch &quot;that we want to calculate gradients with respect to that variable at that value&quot; . def plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)): x = torch.linspace(min,max) fig,ax = plt.subplots(figsize=figsize) ax.plot(x,f(x)) if tx is not None: ax.set_xlabel(tx) if ty is not None: ax.set_ylabel(ty) if title is not None: ax.set_title(title) . Here we pretend that the below is our loss function. Running a number through it, our weight will produce a result, an activation ... in this case, our loss (which again is a value telling us how good or bad our model is; smaller = good) . xt = tensor(-1.5).requires_grad_(); xt . tensor(-1.5000, requires_grad=True) . def f(x): return x**2 loss = f(xt) plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(xt.detach().numpy(), loss.detach().numpy(), color=&#39;red&#39;) print(&#39;Loss: &#39;, loss.item()) . Loss: 2.25 . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/RangeFactories.cpp:23.) . So if our parameter is -1.5 we get a loss = 2.25. Since the direction of our slope is downward (negative), by changing its value to be a bit more positive, we get closer to achieving our goal of minimizing our loss . xt = tensor(-1.).requires_grad_(); xt loss = f(xt) plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(xt.detach().numpy(), loss.detach().numpy(), color=&#39;red&#39;) print(&#39;Loss: &#39;, loss.item()) . Loss: 1.0 . And yes, our loss has improved! If the direction of our slope were upwards (positive), we would conversely want x to be smaller. . BUT now ... imagine having to figure all this out for a million parameters. Obviously, we wouldn&#39;t want to try doing this manually as we did before, and thanks to PyTorch, we don&#39;t have too :) . Remember that by utilizing the requires_grad_() function, we have told PyTorch to keep track of how to compute the gradients based on the other calucations we perform, like running it through our loss function above. Let&#39;s see what that looks like. . xt = tensor(-1.).requires_grad_(); print(xt) loss = f(xt) print(loss) . tensor(-1., requires_grad=True) tensor(1., grad_fn=&lt;PowBackward0&gt;) . That &lt;PowBackward0&gt; is the gradient function it will use to calculate the gradients when needed. And when we need it, we call the backward method to do so. . loss.backward() print(xt.grad) . tensor(-2.) . And the calcuated gradient is exactly what we expected given that to calculate the derivate of x**2 is 2x ... 2*-1 = -2. . Again, the gradient tells us the slope of our function. Here have a a negative/downward slope and so at the very least, we know what moving in that direction will get us closer to the minimum. . The question is now, How far do we move in that direction? . . Step 5: Change all the weights based on the gradient using a &quot;Learning Rate&quot; . The learning rate (or LR) is a number (usually a small number like 1e-3 or 0.1) that we multiply the gradient by to get a better parameter value. For a given parameter/weight w, the calculation looks like this: . w -= w.grad * lr . Notice we take the negative of the grad * lr operation because we want to move in the opposite direction. . Important: We do this in a with torch.no_grad() so that we don&#8217;t calculate the gradient for the gradient calculating operation . lr = 0.01 with torch.no_grad(): xt -= xt.grad * lr print(&#39;New value for xt: &#39;, xt) print(&#39;New loss: &#39;, f(xt)) . New value for xt: tensor(-0.9800, requires_grad=True) New loss: tensor(0.9604) . You can see the loss get smaller which is exactly what we want! &quot;The magnitude of the gradient (i.e., the steepness of the slope) [tells] us how big a step to take.&quot; . The above operation is also called the optimization step . See pp.156-157 for examples of what using a too small or too large LR might look like when training. This could help you troubleshoot things if yours looks wonky. . . Datasets &amp; DataLoaders . A Dataset contains tuples of independent and dependent variables . ds = L(enumerate(string.ascii_lowercase)) ds . (#26) [(0, &#39;a&#39;),(1, &#39;b&#39;),(2, &#39;c&#39;),(3, &#39;d&#39;),(4, &#39;e&#39;),(5, &#39;f&#39;),(6, &#39;g&#39;),(7, &#39;h&#39;),(8, &#39;i&#39;),(9, &#39;j&#39;)...] . A DataLoader receives a dataset and gives us back as many mini-batches are necessary based on the batch size we specify . dl = DataLoader(ds, bs=6, shuffle=True) list(dl) . [(tensor([17, 5, 9, 22, 18, 21]), (&#39;r&#39;, &#39;f&#39;, &#39;j&#39;, &#39;w&#39;, &#39;s&#39;, &#39;v&#39;)), (tensor([15, 3, 23, 1, 0, 19]), (&#39;p&#39;, &#39;d&#39;, &#39;x&#39;, &#39;b&#39;, &#39;a&#39;, &#39;t&#39;)), (tensor([ 4, 10, 16, 25, 8, 2]), (&#39;e&#39;, &#39;k&#39;, &#39;q&#39;, &#39;z&#39;, &#39;i&#39;, &#39;c&#39;)), (tensor([24, 14, 7, 20, 13, 12]), (&#39;y&#39;, &#39;o&#39;, &#39;h&#39;, &#39;u&#39;, &#39;n&#39;, &#39;m&#39;)), (tensor([11, 6]), (&#39;l&#39;, &#39;g&#39;))] . . Measuring distances . See pp.141-142. There are two main ways to measure distances. . L1 norm (or mean absolute difference): Take the mean of the absolute value of differences . l1_loss = (tensor_a - tensor_b).abs().mean() . L2 norm (or root mean squared error, RMSE): Take the square root of the mean of the square differences. The squaring of differences makes everything positive and the square root undoes the squaring. . Important: &quot;... the latter will penalize bigger mistakes more heavily than the former (and be more lenient with small mistakes)&quot; . l2_loss = ((tensor_a - tensor_b) ** 2).sqrt() . . Important PyTorch Modules . A module is a class that inherits from PyTorch&#39;s nn.Module class. &quot;Every PyTorch module knows that parameters it has that can be trained.&quot; . Here are some key ones ... . nn.Linear: Initializes its parameters and performs a linear operation. It contains both the weights and biases in a single class . lin1 = nn.Linear(28*28, 1) # the trainable parameters weights, bias = lin1.parameters() print(weights.shape, bias.shape) . torch.Size([1, 784]) torch.Size([1]) . nn.ReLU: Allows us to add a non-linearity between linear classifiers. Simply put, it ensures that all activations passed to it are a positive number with every negative number replaced with a 0. . Notice below that it has no trainable parameters! . plot_function(F.relu) . non_lin1 = nn.ReLU() print(list(non_lin1.parameters())) print(&#39;using nn.ReLU: &#39;, non_lin1(tensor(-1)), non_lin1(tensor(4))) print(&#39;using max()&#39;, tensor(-1).max(tensor(0.0)), tensor(4).max(tensor(0.0))) . [] using nn.ReLU: tensor(0) tensor(4) using max() tensor(0.) tensor(4.) . Why do you want to have non-linearities? . Because &quot;there&#39;s no point in just putting one linear layout directly after another one, because when we multiply things together and then add them up multiple times, that could be replaced by multiplying different things together and adding them up just once .... BUT if we put a non-linear between them ... this is no longer true. Now each linear layer is somewhat decoupled from the other ones and can do its own useful work.&quot; . These kind of functions are also called &quot;activation functions&quot;, because the only operate and produce activations ... there are no trainable parameters. . nn.Sequential: A module that can be passed modules, which when called, calls each of those layers in turn. . lin2 = nn.Linear(1, 10) seq_model = nn.Sequential(lin1, non_lin1, lin2) seq_params = list(seq_model.parameters()) print(len(seq_params)) for p in seq_params: print(p.shape) . 4 torch.Size([1, 784]) torch.Size([1]) torch.Size([10, 1]) torch.Size([10]) . Why 4? Simple, remember that each nn.Linear above has two trainable parameters (the weights and bias), 2+2 = 4. . . Summary . This chapter walks you through creating a baseline model to a full blown training loop in PyTorch. Read it, and read it again and again! (I do and have). . Important Vocb/Concepts . Activations: Numbers that are calculated by both linear and non-linear layers . Parameters: Randomly initialized parameters that can be trained. . Neural Network: A chain of linear and non-linear functions your data runs through to produce a result. . Gradient: &quot;The derivative of the loss with respect to some parameter of the model&quot; . Backpropagation: The computing of the gradients &quot;of the loss with respect to all model parameters&quot; . Gradient Descent: &quot;Taking a step in the direction opposite to the gradients to make the model parameters a little bit better&quot; . . Resources . https://book.fast.ai - The book&#39;s website; it&#39;s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc... |",
            "url": "https://ohmeow.com/posts/2021/05/23/ajtfb-chapter-4.html",
            "relUrl": "/posts/2021/05/23/ajtfb-chapter-4.html",
            "date": " ‚Ä¢ May 23, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "A Journey Through Fastbook (AJTFB) - Chapter 3",
            "content": "Other posts in this series: A Journey Through Fastbook (AJTFB) - Chapter 1 A Journey Through Fastbook (AJTFB) - Chapter 2 . Chapter 3 . The chapter most folks pass over, but shouldn&#39;t. . . Data Ethics . ... sometimes machine learning models can go wrong. They can have bugs. They can be presented with data that they haven&#39;t seen before and behave in ways we don&#39;t expect. Or ... they can be used for something that we would much prefer they were never, ever used for. . ... no one really agrees on what right and wrong are, whether they exist, how to spot them, which people are good and which bad, or pretty much anything else. . If anything, this is a call to humility, self-examination, and thoughtful dialog. Though we are increasingly living in a polarized world where one is judged by what particular slogans they choose, what party they belong too, who they follow on Facebook, and so on, we have the choice to not be such human beings. But in my experience that is easier said than done. It&#39;s too easy to shout at, rather than talk with, the &quot;other&quot; side because we can in blissful ignorance continue believing we got it right without ever being challenged. It&#39;s hard to really reason out our world views and argue with those who don&#39;t agree. If we did, we have find we have more in common that imagined and make further progress as a society into figuring these things out ... things like right and wrong, good and evil, justice and injustice, and how we can get along with each other despite our differences. . The point of this chapter is simple:The goal of ML isn&#39;t to find the model with the lowest loss ... it is to build a model that drives the right kind of actions . . Recourse and accountability . In a complex system, it is easy for no one person to feel responsible for outcomes. . As deep learning practioniers, we have better insight than most into what kind of actions will be made as a result of our model&#39;s results. Therefore, if we care about people in general, we&#39;ll care about those outcomes as much as our model&#39;s validation loss. . . Feedback Loops . Feedback loops can occur when your model is controlling the next round of data you get. . ... an algorithm can interact with its environment to create a feedback loop, making predictions that reinforce actions taken in the real world, which lead to predictions even more pronounced in the same direction. . Part of the problem here is the centrality of metrics in driving a financially important system. . See the &quot;Meetup&quot; example on p.105 . Once people join a single conspiracy-minded [Facebook] group, they are algorithmically routed to a plethora of others. Join an anti-vaccine group, and your suggestions will include anit-GMO, chemtrail watch, falt Earther (yes, really), and &quot;curing cancer naturallay&quot; groups. Rather than pulling a user out of the rabbit hole, the recommendation engine pushes them farther in. . FYI, I think most social media has a net-negative effect on us as humain beings. In particular, I try to avoid Facebook, Instagram, TikTok, and Snapchat while doing my best to limit my only social media account, a Twitter account, to things relevant to data science and public health (and that ain&#39;t easy). . . Bias . There are different kinds of &quot;data ethics&quot; bias, here are 4 types: . Historical Bias . ... comes from the fact that people are biased, processes are biased, and society is biased. [It] is a fundamental, structural issue with the first step of data generation process and can exist even given perfect sampling and feature selection. . Any dataset involving human can have this kind of bias:medical data, sales data, housing data, political data, and so on. . Important: Maybe the best way to understand historical biase in your dataset is by spending time looking at both the outcomes and how they might be used??? . Important: Make sure your data is representative of what your model will see and to evaluate any automatic &quot;labeling&quot; features in your system. (see gorillas example on pp.107-108). . So what this showed is that the developers failed to utilize datasets containing enough darker faces, or test their product with darker faces. . A good reminder that your model will only be as good as the data you trained it on! Sound familiar? . ... the vast majority of AI researches and developers are young white men. Most projects that we have seen do most user testing using friends and families of the immediate product development group. Given this, the kinds of problems we just discussed should not be suprising. . I think at the very least, we need to be forthright about our dataset as much as on model performance. That way, expectations can be managed and a confidence level assigned to the results. A threshold perhaps that could trigger human intervention. . Measurement bias . ... occurs when our models make mistakes because we are measuring the wrong thing, or measuring it the wrong way, or incorporating that measurement into the model inappropriately. . Not sure why, but this is perhaps the most insidious bias because I think its the hardest to figure out. . Aggregation bias . ... occurs when models do not aggregate data in a way that incorporates all of the appropriate factors, or when a model does not include the necessary interaction terms, nonlineraities, or so forth. . These are features that are not included though they would actually improve model performance if they were. . Representation bias . When there is a clear, easy-to-see underlying relationship, a simple model will often assume that this relationship holds all the time. . Essentially models can see this real imbalance and make it bigger than it is. . . Disinformation . It is not necessarily about getting someone to belive something false, but rather often used to sow disharmony and uncertainty, and to get people to give up on seeking the truth. Receiving conflicting accounts can lead people to assume that they can never know whom or what to trust. . Disinformation will unfotunately be one of the greatest legacies of President Trump. A step backwards for American society. A culture that will back if you if you tell them what they want to hear, even if you&#39;re a compulsive liar and base your statements on &quot;gut feel&quot; rather than facts and logic. . While most of us like to think of ourselves as independent-minded, in reality we evolved to be influenced by others in our in-group, and in opposition to those in our out-group. Online discussions can influence our viewpoints, or alter the range of what we consider acceptable viewpoints. Humans are social animals, and as social animals, we are extremely influenced by the people around us. Increasingly, radicalization occurs in online environments; so influence is coming from people in the virtual space of online forums and social networks. . The biggest take here is that I am not as independently minded as I think I am. Knowing thyself is perhaps the best preventative of being swallowed up by disinformation. Limiting social media is another. . Disinformation through autogenerated text is a particularly significant issue . As an NLP guy, this one scares me since part of my work is to summarize text. Knowing this, the first step I&#39;ve taken is to let all business owners know the risk of text generation algorithms generating text that is either false and/or not necessarily reflective of the inputs, as in the case of abstract summarization. The second step I took was to introduce human beings into the process and a workflow that has them look at at least the most potentially wrong summarizations before reports go out. . . What to do??? . You must assuem that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal. . Data use and storage are things you need to think about. . I think these are good questions to ask/answer in any project to ensure good outcomes:&gt; * Whose interests, desires, skills, experiences, and values have we simply assumed rather than&gt; actually consulted? . Who are all the stakeholders who will be directly affected by our product? How have their interests been protected? How do we know what their interests really are - have we asked? | Whowhich groups and individuals will be indirectly affected in signficant ways? | Who might use this product that we didn&#39;t expect to use it, or for purposes we didn&#39;t initially intend? | . See pp.119-120 for a bunch of good questions to put into your practice! . When everybody on a team has similar backgrounds, they are likely to have similar blind spots around ethical tasks. . ... first come up with a process, definition, set of questions etc., which is designed to resolve a problem. Then try to come up with an example in which the apparent solution results in a proposal that no one would consider acceptable. This can then lead to further refinement of the solution. . Thinking about all these things may lead one to analysis paralysis or even worse, complete apathy. We need to start with something and be okay with criticism and refactoring. Additionally, we need to be thoughtful in even spot on criticism of others&#39; systems. I don&#39;t think most folks try to make something racist or mysoginistic or whatever, so instead of calling them a &quot;Hitler&quot; on Twitter when we taste something that looks to us like fasicism, maybe a phone call and one-on-one chat is the better and more productive move. . . Resources . https://book.fast.ai - The book&#39;s website; it&#39;s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc... . | https://forums.fast.ai/c/data-ethics/47 - Forum subcategory for all things &quot;data ethics&quot;. . |",
            "url": "https://ohmeow.com/posts/2020/11/22/ajtfb-chapter-3.html",
            "relUrl": "/posts/2020/11/22/ajtfb-chapter-3.html",
            "date": " ‚Ä¢ Nov 22, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "A Journey Through Fastbook (AJTFB) - Chapter 2",
            "content": "Other posts in this series: A Journey Through Fastbook (AJTFB) - Chapter 1 . Chapter 2 . Starting Your Project . Things to think about when deciding on project feasibility . When selecting a project, the most important consideration is data availability . If you don&#39;t have enough quality data ... good luck :) . Important: Consider that data augmentation can alleviate both the need for more manual labelling and also protect you from problems with out-of-domain data (e.g. when unexpected image types arise in the data when the model is being used in production) by synthetically creating more data likely to be seen that may not be in your dataset as is. . ... iterate from end to end in your project; don&#39;t spend months fine-tuning your model, or polishing the perfect GUI, or labeling the perfect dataset . This is good advice for any software project ...fail early and fail often. If you don&#39;t, you&#39;re likely to only uncover critical problems much later than you would have before, and even worse, you&#39;re likely to not produce anything at all! In the world of deep learning there are a number of tools, that while helpful, can really get you so bogged down that you never deploy something usable (e.g., experiment tracking tools, hyperparameter optimization libraries, etc...). Also, remember that getting something in production is a different task from winning a kaggle competition, where the later may require use of some of those aforementioned tools and the ensembling of dozens of models. For production, something better than human is often good enough to get out there and through refactoring, improve. . . The Drivetrain Approach . Four Steps . Step 1: Define your objective(s) . It&#39;s amazing how in my 20+ years as a developer, how rare it is that a customer is able to clearly define what they want! In my experience, more than not, it is the developers that end up defining the goals. Not having a clear objective is likely to waste time, energy, and money to produce something that won&#39;t even see the light of day. You can&#39;t gauge the completion or quality of any software project without clear objective(s). . Ex.1: Show most relevant search results. Ex.2: Drive additional sales by recommending to customers items to purchase they otherwise wouldn&#39;t . Step 2: What actions can you take to achieve those objective(s)? . What things can make your goals a reality. Pretty simple. . Ex.1: Ranking the search results will help show the most relevants ones first. Ex.2: Ranking the recommendations will help. . Step 3: What data is needed to take those actions? . If you don&#39;t have the data, you&#39;ll need to get it ... because the data pulls the levers which get you closer to your objective(s). . Ex.1: Seeing what how pages linked to other pages. Ex.2: Collecting data on what customers purchased, what was recommended, and what they did with that info. . Step 4: Build models . Only once you have the data and know what actions you want to be able to take based on the information within it, do you being modeling ... first, defining what models you can even build with that data and second, what data you need to collect for models you can&#39;t. . Ex.1: A model that takes the page relation data and predicts a ranking given a query. Ex.2: Two models that predict the purchasing proabilities conditional on seeing or not seeing a recommendation. . ! pip install fastai -q from fastai.vision import * . . Downloading images, getting the downloaded images, &amp; removing those that are corrupt . Use download_images listed as URLs in a text file urls to download the actual images locally. | Get the file path to the images via get_image_files in an L object. | Get rid of the corrupt images using verify_images and Path.unlink. | path = Path(&#39;bears/grizzly&#39;) download_images(path, urls=image_urls.txt) file_paths = get_image_files(path) failed = verify_images(file_paths) failed.map(Path.unlink) . Notice how L&#39;s map method is used to apply the Path.unlink function to each item in-place. . . Getting help . A few of ways ... . download_images? . download_images?? . doc(download_images) . You can also use pdb.set_trace (in code) or %debug(in a new cell following the one with the error) to step through your code. I use the former all the time ... its a great way to debug and also learn what the code is doing and why. For example, I use it to look at the shape of things as the travel through and out of different layers in my NNs. . import pdb def div_by_zero(): pdb.set_trace() x = 1/0 print(&#39;here&#39;) # uncomment this to see what I&#39;m talking about ... # div_by_zero() . . DataBlock API Basics . In order to make your data &quot;modelable&quot; (via DataLoaders, you need to tell fastai 4 things: . What kind of data you are working with | How to get the data | How to label the data | How to create a validation set | Here&#39;s an example of how this is done with the DataBlock API: . d_block = DataBlock( blocks=(ImageBlock, CategoryBlock), #=&gt; our independent and dependent variable datatypes get_items=get_image_files, #=&gt; how to get our data splitter=RandomSplitter(valid_pct=0.2, seed=42), #=&gt; how to create the validation set get_y=parent_label, #=&gt; how to label our data item_tfms=Resize(128) #=&gt; code that runs against each item as it is fetched . . Important: Use the seed argument to ensure you get the same training/validation set each time you run that code; else you won&#8217;t be able to know if, as you change hyperparameter values, your model performance changed because of those values and/or because of difference in your training/validation sets! &gt; ... a DataBlock object ... is like a template for creating a DataLoaders object . For more detailed discussion of the DataBlock API, see the following resources:1. My article &quot;Finding DataBlock Nirvana with fast.ai v2 - Part 1&quot; . The &quot;Walk with fastai2&quot; videos . | The fastai docs and the DataBlock Tutorials . | Once you&#39;ve defined your blueprint for how to get your modelable data (i.e., your DataLoaders), you need to pass it the &quot;actual source&quot; of your data, which can be a path or a DataFrame or whatever. . dls = d_block.dataloaders(path) . Use dls.show_batch(...) or dls.valid.show_batch(...) to visualize your training/validation data. . Important: You can change the transforms in your DataBlock by reusing the existing DataBlock using d_block.new . d_block = d_block.new(item_tfms=Resize(128, ResizeMethod.squish)) dls = d_block.dataloaders(path) ... d_block = d_block.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeroes&#39;)) dls = d_block.dataloaders(path) ... . . Important: For resizing ... &quot;what we normally do in practice is to randomly select part of the image and then crop to just that part. On each epoch ... we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.&quot; This is done using the RandomResizedCrop transform. . d_block = d_block.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) . min_scale: &quot;how much of the image to select at minimum each time.&quot; 0.5 = select 50% of the image at minimum. . Important: Pass unique=True to your show_batch functions &quot;to have the same image repeated with different versions&quot; of the transforms you&#8217;ve defined. . Data augmentation transorms (e.g., rotation, flipping, perspective warping, brightness changes, contrast changes, etc...) are defined as batch transforms and run on the GPU. . Important: Item Transforms are applied to an item from your dataset when it is fetched, Batch Transforms are applied to a collection of items on the GPU after they have been collated into the same shape. . d_block = d_block.new(item_tfms=RandomResizedCrop(128, min_scale=0.3), batch_tfms=aug_transforms(mult=2)) . batch_tfms: Your batch transforms, or more correctly your after batch transforms . Important: aug_transforms are &quot;a standard set of augmentations that we have found work pretty well&quot; . . Cleaning your model through training . . Important: &quot;It&#8217;s helpful to see where exactly our errors are occuring, to see whether they&#8217;re due to a dataset problem ... or a model problem&quot; using plot_top_losses . interp = ClassificationInterpretation.from_learner(learn) interp.plot_top_losses(5, nrows=1) #&gt; show the 5 examples with the highest loss . . Important: A &quot;model can help you find data issues more quickly ... so we normally prefer to train a quick and simple model first, and then use it to help with data cleaning.&quot; . . Inference . &quot;a model consists of two parts:the architecture and the trained parameters.&quot; You can use it just like any other function . #saves the architecture, the trained parameters, and the definintion of how to create your DataLoaders learn.export() . fastai ... uses your validation set DataLoader for inference by default, so your data augmentation will not be applied. . inf_learn = load_learner(path/&#39;export.pkl&#39;) inf_learn.predict(&#39;images/grizzly.jpg&#39;) inf_learn.dls.vocab # =&gt; To view possible classification categories/labels . For options on how to deploy your app, see the Deployment section in the course website. I personally like to use FastAPI and there is a good starter template here for that. . . How to Avoid Disaster . . Important: Your model is only as good as the data it was trained on . Two problems to watch out for: . out-of-domain data: &quot;data that our model sees in production that is very different to wath it saw during training. | domain shift: &quot;whereby the type of data that our model sees changes over time.&quot; | Mitigation steps: . . Where possible, the first step is to use an entirely manual process with your model running in parallel and not being used to directly drive any actions. . The second step is to try and limit the scop of the model. . The third step is to gradually increase the scope of your rollout. . Important:&quot;Try to think about all the ways in which your system could go wrong, and then think about what measure or report or picture could reflect that problem, and ensure that your regular reporting includes that information.&quot; . . Resources . https://book.fast.ai - The book&#39;s website; it&#39;s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc... . | https://docs.fast.ai/ - The library&#39;s documentation; includes tutorials and other tips for development. . | https://forums.fast.ai/ - If you&#39;re not part of the community yet, you should be. Before posting a question, search to see if it has been answered already (95% of the time, it has). . |",
            "url": "https://ohmeow.com/posts/2020/11/16/ajtfb-chapter-2.html",
            "relUrl": "/posts/2020/11/16/ajtfb-chapter-2.html",
            "date": " ‚Ä¢ Nov 16, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "A Journey Through Fastbook (AJTFB) - Chapter 1",
            "content": "Cervantes once wrote that &quot;the journey is better than the inn&quot;, but I rather like to think that the journey is the inn. . It means that the journey, irrespective to its difficulties (and likely because of them), is what you look back on with fondness at its end rather than the end itself. It&#39;s why I enjoy reading &quot;The Lord of the Rings&quot; every five years or so, where as I age and experience the hand life has dealt me, I find myself appreciating different aspects of the story from the time before and gaining new insights into what I value and want to be as a human being. I find my journey with deep learning to be roughly analgous to that. . I&#39;ve been a part of the fast.ai community for several years. I&#39;ve been through the course multiple times (since it was using theano back in the old days), I&#39;ve contributed to the library, and use it as the basis for one of my own. And as with each course, with a re-reading of the book I find myself deriving new insights and appreciating different ideas than those I had before. . And so, while your journey may bring you different revelations, here are the meandering thoughts of one 49 year old married father of 3 living in San Diego, California, USA, as I embark upon the first chapter in what I consider &quot;The Lord of the Rings&quot; of deep learning. . Chapter 1 . How to Learn Deep Learning . You can do this! . Hi, everybody; I&#39;m Jeremy ... I do not have any formal technical education ... didn&#39;t have great grades. I was much more interested in doing real projects. . This is meaningful to me as someone with a BA in History and a MA in Theology. It&#39;s a reminder that if you want something, it&#39;s within your grasp to make it happen if you are willing to put in the work. It&#39;s also a reminder that key to getting there is actually doing something! If find too many people thinking that if they just get into that school, or if they can just take that class, then they&#39;ll be a good software enginner or deep learning practitioner. The reality is that the only way you get there is by doing it ... just like pull-ups (which aren&#39;t much fun when you&#39;re starting out and/or you&#39;re 49 and overweight). . The problem with traditional education . ... how math is taught - we require students to spend years doing rote memorization and learning dry disconnected fundatmentals that we claim will pay off later, long after most of them quit the subject. . This also is the problem with higher education in general, where young people spend at least four to five years learning things they already learned in High School or else things they don&#39;t really care about and will be forgotten right after finals, spending in excess of $100,000 for the privilege of it and likely going into debt in the tens of thousands of dollars, all with this idea that having done it they will be prepared for the real world. Unfortunately, that&#39;s not how it works. Whether you are in a university of even go to university, what matter is what you do ... not what classes you took or what your GPA is. . Deep Learning (and coding in general) is an art maybe more so than a science . The hardest part of deep learning is artisanal. . I remember going to an iOS conference way back in the day and a conference speaker asking how many folks in the session I was sitting in had a background in music. 80-90% of the audience raised their hands. Sure, there is math and stats and a science to deep learning, but like any coding enterprise, it&#39;s an art ... with some artists being better than others along with room for improvement regardless of whether you&#39;re Van Gough or painting by the numbers. . Doing is how you learn, and what you&#39;ve done is what matters . ... focus on your hobbies and passions ... Common character traits in the people who do well at deep learning include playfulness and curiosity. . at Tesla .. CEO Elon Musk says &#39;A PhD is definitely not required. All that matters is a deep understanding of AI &amp; ability to implement NNs in a way that is actually useful .... Don&#39;t care if you even graduated High School.&#39; . ... the most important thing for learning deep learning is writing code and experimenting.&quot; . . Getting Started . Training &amp; Transfer Learning . ... a model is a special kind of program:it&#39;s one that can do many different things, depending &gt; on the weights. . Weights are just variables, and a weight assignment is a particuarl choice of values for those variables. [Weights] are generally referred to as model parameters ... the term weights being reserved for a particular type of model parameter. . The functional form of the model is called its architecture. | . It is &quot;the template of the model that we&#39;re trying to fit; i.e., the actual mathematical function that we&#39;re passing the input data and parameters to&quot; ... whereas the model is a particular set of parameters + the architecture. . The weights are called parameters. | . These are the things that are &quot;learnt&quot;; the values that can change . The predictions are calculated from your indpendent variables [your X] | The [model&#39;s] measure of performance is called the loss ... [which depends on how well your model is able to predict] the correct labels (also known as targets or the dependent variable) [your y] ... [given the independent variables as input]. | . The loss is a measure of model performance that SGD can use to make your model better. A good loss function provides good gradients (slopes) that can be used to make even very minor changes to your weights so as to improve things. Visually, you want gentle rolling hills rather than abrupt steps or jagged peaks. . Transfer learning is the process of taking a &quot;pretrained model&quot; that has been trained on a very large dataset with proven SOTA results, and &quot;fine tuning&quot; it for your specific task, which while likely similar to the task the pretrained model was trained for to one degree or another, is not the necesarily the same. . What does this mean? . The head of your model (the newly added part specific to your dataset/task) should be trained first since it is the only one with completely random weights. | The degree to which your weights of the pretrained model will need to be updated is proportional to how similar your data is to the data it was trained on. The more dissimilar, the more the weights will need to be changed. | Your model will only be as good as the data it was trained on, so make sure what you have is representative of what it will see in the real world. It &quot;can learn to operate on only the patterns seen in the input data used to train it.&quot; | The process of training (or fitting) the model is the process of finding a set of parameter values (or weights) that specialize that general architecture into a model that works well for our particular kind of data [and task] . fastai&#39;s fine_tune method uses proven tricks and hyperparameters for various DL tasks that the author&#39;s have found works well and works most of the time. See p.33 for more info on what it does. . ... once the model is trained - that is, once we&#39;ve chosen our final weight assignments - then we can think of the weights as being part of the model since we&#39;re not varying them anymore. . This means a trained model can be treated like a typical function. . . Metrics . Metrics are a human-understandable measures of model quality whereas the loss is the machine&#39;s. They are based on your validation set and are what you really care about, whereas the loss is &quot;a measure of performance&quot; that the training system can use to update weights automatically. . A good choice for loss is a function &quot;that is easy for stochastic gradient descent (SGD) to use, whereas a good choies for your metrics are functions that your business users will care about. Seldom are they the same because most metrics don&#39;t provide smooth gradients that SGD can use to update your model&#39;s weights. . Examples of common metrics: . error rate = &quot;the proportion of images that were incorrectly identified. . accuracy = the proportation of images that were correctly identified (1 - error rate) . . Validation &amp; Test Sets . What is a validation set? . A validation set (also know as the &quot;development set&quot;) does not include any data from the training set. It&#39;s purpose to is gauge the generalization prowess of your model and also ensure you are neight overfitting or underfitting. . If [the model] makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by actually having seen that particular item. . Why do we need a validation set? . [because] what we care about is how well our model works on previously unseen images ... the longer you train for, the better your accuracy will get on the training set ... as the model starts to memorize the training set rather than finding generalizable underlying patterns in the data = overfitting . Overfitting happens when the model &quot;remembers specific features of the input data, rather than generalizing well to data not seen during training.&quot; . Important: Your models should always overfit before anything else. It is your training loss gets better while your validation loss gets worse ... in other words, if you&#8217;re validation loss is improving, even if not to the extent of your training loss, you are not overfitting . Important: ALWAYS include a validation set. . Important: ALWAYS measure your accuracy (or any metrics) on the validation set. . Important: Set the seed parameter so that you &quot;get the same validation set every time&quot; so that &quot;if we change our model and retrain it, we know any differences are due to the changes to the model, not due to having a different random validation set.&quot; The validation set also informs us how we may change the hyperparamters (e.g., model architecture, learning rates, data augmentation, etc...) to improve results. These parameters are NOT learned ... they are choices WE make that affect the learning of the model parameters. . What is a test set? . A test set ensures that we aren&#39;t overfitting our hyperparameter choices; it is held back even from ourselves and used to evaulate the model at the very end. . Important: If evaluating 3rd party solutions know how to create a good test set and how to create a good baseline model. Hold these out from the potential consultants and use them to fairly evaluate their work. . How do you define a good validation and test sets? . See pp.50-54 ... . A key property of the validation and test sets is that they must be representative of the new data you will see in the future. . For time series, that means you&#39;ll likely want to make your validation set a continuous section with the latest dates. . You&#39;ll also want to make sure your model isn&#39;t learning particular ancillary features of particular things in your images (e.g., you want to see how your model performs on a person or boat it hasn&#39;t seen before ... see pp.53-54 for examples). . . Q &amp; A &amp; Best Practices . What is a Transform . A Transform conatins code that is applied automatically during training. There are two kinds ... . item_tfms:Applied to each item2. batch_tfms: Applied to a batch of items at a time using the GPU | Why do we make images 224x224 pixels? . This is the standard size for historical reasons (old pretrained models require this size exactly) ... If you increase the size, you&#39;ll often get a model with better results since it will be able to focus on more details. . Important:Train on progressively larger image sizes using the weights trained on smaller sizes as a kind of pretrained model. . What is a ResNet &amp; Why use it for computer vision tasks? . A ResNet is a model architecture that has proven to work well in CV tasks. Several variants exist with different numbers of layers with the larger architectures taking longer to train and more prone to overfitting especially with smaller datasets. . Important: Start with a smaller ResNet (like 18 or 34) and move up as needed. . Important: If you have a lot of data, the bigger resnets will likely give you better results. And what other things can use images recognizers for besides image tasks? Sound, time series, malware classification ... . ... a good rule of thumb for converting a dataset into an image representation:if the human eye can recognize categories from the images, then a deep learning model should be able to do so too. See pp.36-39 . How can we see what our NN&#39;s are actually learning/doing? . See pp.33-36. Being able to inspect what your NN is doing (e.g., looking at the activations and the gradients) is one of the most important things you can learn as they are often the key to improving results. . Important: Learn how to visualize and understand your activations and gradients! . What is the difference between categorical and continuous datatypes? . Categorical data &quot;contains values taht are one of a discrete set of choice&quot; such as gender, occupation, day of week, etc... . Continuous data is numerical that represents a quantity such as age, salary, prices, etc... . Important: For tasks that predict a continuous number, consider using y_range to constrain the network to predicting a value in the known range of valid values. (see p.47) . . Resources . https://book.fast.ai - The book&#39;s website; it&#39;s updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc... . | https://course.fast.ai/datasets - A variety of slimmed down datasets you can use for various DL tasks that support &quot;rapid prototyping and experimentation.&quot; . | https://huggingface.co/docs/datasets/ - Serves a similar purpose to the fastai datasets but for the NLP domain. Includes metrics and full/sub-set datasets that you can use to benchmark your results against the top guns of deep learning. . Important: Start with a smaller dataset and scale up to full size to accelerate modeling! . |",
            "url": "https://ohmeow.com/posts/2020/11/06/ajtfb-chapter-1.html",
            "relUrl": "/posts/2020/11/06/ajtfb-chapter-1.html",
            "date": " ‚Ä¢ Nov 6, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Summarization with blurr",
            "content": "# !pip install ohmeow-blurr -q # !pip install datasets -q # !pip install bert-score -q . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 5.6MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 225kB 12.9MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2MB 17.9MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 204kB 38.2MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.1MB 34.3MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 8.8MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112kB 54.5MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 245kB 58.9MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 9.1MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 901kB 55.8MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.3MB 57.4MB/s Building wheel for seqeval (setup.py) ... done |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 5.3MB/s . import datasets import pandas as pd from fastai.text.all import * from transformers import * from blurr.data.all import * from blurr.modeling.all import * . Data Preparation . We&#39;re going to use to use the datasets library from huggingface to grab your raw data. This package gives you access to all kinds of NLP related datasets, explanations of each, and various task specific metrics to use in evaluating your model. The best part being everything comes down to you in JSON! This makes it a breeze to get up and running quickly! . We&#39;ll just use a subset of the training set to build both our training and validation DataLoaders . raw_data = datasets.load_dataset(&#39;cnn_dailymail&#39;, &#39;3.0.0&#39;, split=&#39;train[:1%]&#39;) . Downloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.28 GiB, post-processed: Unknown size, total: 1.82 GiB) to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234... Dataset cnn_dailymail downloaded and prepared to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234. Subsequent calls will reuse this data. . df = pd.DataFrame(raw_data) df.head() . article highlights id . 0 It&#39;s official: U.S. President Barack Obama wants lawmakers to weigh in on whether to use military force in Syria. Obama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons. The proposed legislation from Obama asks Congress to approve the use of military force &quot;to deter, disrupt, prevent and degrade the potential for future uses of chemical weapons or other weapons of mass destruction.&quot; It&#39;s a step that is set to turn an internat... | Syrian official: Obama climbed to the top of the tree, &quot;doesn&#39;t know how to get down&quot; nObama sends a letter to the heads of the House and Senate . nObama to seek congressional approval on military action against Syria . nAim is to determine whether CW were used, not by whom, says U.N. spokesman . | 0001d1afc246a7964130f43ae940af6bc6c57f01 | . 1 (CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men&#39;s 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has now collected eight gold medals at world championships, equaling the record held by American trio... | Usain Bolt wins third gold of world championship . nAnchors Jamaica to 4x100m relay victory . nEighth gold at the championships for Bolt . nJamaica double up in women&#39;s 4x100m relay . | 0002095e55fcbd3a2f366d9bf92a95433dc305ef | . 2 Kansas City, Missouri (CNN) -- The General Services Administration, already under investigation for lavish spending, allowed an employee to telecommute from Hawaii even though he is based at the GSA&#39;s Kansas City, Missouri, office, a CNN investigation has found. It cost more than $24,000 for the business development specialist to travel to and from the mainland United States over the past year. He is among several hundred GSA &quot;virtual&quot; workers who also travel to various conferences and their home offices, costing the agency millions of dollars over the past three years. Under the program, ... | The employee in agency&#39;s Kansas City office is among hundreds of &quot;virtual&quot; workers . nThe employee&#39;s travel to and from the mainland U.S. last year cost more than $24,000 . nThe telecommuting program, like all GSA practices, is under review . | 00027e965c8264c35cc1bc55556db388da82b07f | . 3 Los Angeles (CNN) -- A medical doctor in Vancouver, British Columbia, said Thursday that California arson suspect Harry Burkhart suffered from severe mental illness in 2010, when she examined him as part of a team of doctors. Dr. Blaga Stancheva, a family physician and specialist in obstetrics, said both Burkhart and his mother, Dorothee, were her patients in Vancouver while both were applying for refugee status in Canada. &quot;I was asked to diagnose and treat Harry to support a claim explaining why he was unable to show up in a small-claims court case,&quot; Stancheva told CNN in a phone intervie... | NEW: A Canadian doctor says she was part of a team examining Harry Burkhart in 2010 . nNEW: Diagnosis: &quot;autism, severe anxiety, post-traumatic stress disorder and depression&quot; nBurkhart is also suspected in a German arson probe, officials say . nProsecutors believe the German national set a string of fires in Los Angeles . | 0002c17436637c4fe1837c935c04de47adb18e9a | . 4 (CNN) -- Police arrested another teen Thursday, the sixth suspect jailed in connection with the gang rape of a 15-year-old girl on a northern California high school campus. Jose Carlos Montano, 18, was arrested on charges of felony rape, rape in concert with force, and penetration with a foreign object, said Richmond Police Lt. Mark Gagan. Montano was arrested Thursday evening in San Pablo, California, a small town about two miles from the city of Richmond, where the crime took place. Montano, who was held in lieu of $1.3 million bail, is accused of taking part in what police said was a 2¬Ω... | Another arrest made in gang rape outside California school . nInvestigators say up to 20 people took part or stood and watched the assault . nFour suspects appeared in court Thursday; three wore bulletproof vests . | 0003ad6ef0c37534f80b55b4235108024b407f0b | . We begin by getting our hugginface objects needed for this task (e.g., the architecture, tokenizer, config, and model). We&#39;ll use blurr&#39;s get_hf_objects helper method here. . pretrained_model_name = &quot;facebook/bart-large-cnn&quot; hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=BartForConditionalGeneration) hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model) . . (&#39;bart&#39;, transformers.models.bart.configuration_bart.BartConfig, transformers.models.bart.tokenization_bart_fast.BartTokenizerFast, transformers.models.bart.modeling_bart.BartForConditionalGeneration) . Next we need to build out our DataBlock. Remember tha a DataBlock is a blueprint describing how to move your raw data into something modelable. That blueprint is executed when we pass it a data source, which in our case, will be the DataFrame we created above. We&#39;ll use a random subset to get things moving along a bit faster for the demo as well. . Notice that the blurr DataBlock as been dramatically simplified given the shift to on-the-fly batch-time tokenization. All we need is to define a single HF_Seq2SeqBeforeBatchTransform instance, optionally passing a list to any of the tokenization arguments to differentiate the values for the input and summary sequences. In addition to specifying a custom max length for the inputs, we can also do the same for the output sequences ... and with the latest release of blurr, we can even customize the text generation by passing in text_gen_kwargs. . We pass noop as a type transform for our targets because everything is already handled by the batch transform now. . text_gen_kwargs = default_text_gen_kwargs(hf_config, hf_model, task=&#39;summarization&#39;); text_gen_kwargs . {&#39;bad_words_ids&#39;: None, &#39;bos_token_id&#39;: 0, &#39;decoder_start_token_id&#39;: 2, &#39;diversity_penalty&#39;: 0.0, &#39;do_sample&#39;: False, &#39;early_stopping&#39;: True, &#39;encoder_no_repeat_ngram_size&#39;: 0, &#39;eos_token_id&#39;: 2, &#39;forced_bos_token_id&#39;: 0, &#39;forced_eos_token_id&#39;: 2, &#39;length_penalty&#39;: 2.0, &#39;max_length&#39;: 142, &#39;min_length&#39;: 56, &#39;no_repeat_ngram_size&#39;: 3, &#39;num_beam_groups&#39;: 1, &#39;num_beams&#39;: 4, &#39;num_return_sequences&#39;: 1, &#39;output_attentions&#39;: False, &#39;output_hidden_states&#39;: False, &#39;output_scores&#39;: False, &#39;pad_token_id&#39;: 1, &#39;remove_invalid_values&#39;: False, &#39;repetition_penalty&#39;: 1.0, &#39;return_dict_in_generate&#39;: False, &#39;temperature&#39;: 1.0, &#39;top_k&#39;: 50, &#39;top_p&#39;: 1.0, &#39;use_cache&#39;: True} . hf_batch_tfm = HF_Seq2SeqBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=256, max_tgt_length=130, text_gen_kwargs=text_gen_kwargs) blocks = (HF_Seq2SeqBlock(before_batch_tfm=hf_batch_tfm), noop) dblock = DataBlock(blocks=blocks, get_x=ColReader(&#39;article&#39;), get_y=ColReader(&#39;highlights&#39;), splitter=RandomSplitter()) . dls = dblock.dataloaders(df, bs=2) . len(dls.train.items), len(dls.valid.items) . (2297, 574) . It&#39;s always a good idea to check out a batch of data and make sure the shapes look right. . b = dls.one_batch() len(b), b[0][&#39;input_ids&#39;].shape, b[1].shape . (2, torch.Size([2, 256]), torch.Size([2, 68])) . Even better, we can take advantage of blurr&#39;s TypeDispatched version of show_batch to look at things a bit more intuitively. We pass in the dls via the dataloaders argument so we can access all tokenization/modeling configuration stored in our batch transform above. . dls.show_batch(dataloaders=dls, max_n=2) . text target . 0 The news from Pakistan is generally bad news. In the past week, which was far from atypical, suicide bombers attacked a court building in the northwestern city of Peshawar taking hostages and killing four people. In the southern city of Karachi the director of a renowned social program working in the megacity&#39;s poorest neighborhoods was shot and killed. And gunmen kidnapped two female Czech tourists in southwestern Pakistan. But this past week also saw more than a glimmer of good news from Pakistan: Saturday, March 16 marked an extraordinary moment in Pakistani history, as this is the first time a civilian government has served its entire five-year term (from 2008 to 2013). And, for the first time in its history, the Pakistani military appears unwilling to mount a coup against the civilian government. The military has successfully executed three coups and attempted a number of others since Pakistan&#39;s independence in 1947. Today the army understands that the most recent coup by General Pervez Musharraf who took power in 1999 has tarnished its brand. Musharraf hung on to power for almost a decade and his imposition of emergency rule in 2007 triggered massive street protests and eventually his ouster. On Saturday, Musharaf announced he is returning to Pakistan from self-imposed exile on March 24 to | Peter Bergen: For the first time, Pakistan government served its full term. nHe says lack of military coup attempt shows government is more stable than many think. nElections in Pakistan, Afghanistan likely to be crucial for those two nations. nBergen: He says Afghan economy is resilient and corruption may be receding. | . 1 (CNN) -- A controversial Colombian senator who has obtained the release of 16 hostages held by Marxist guerrillas is the leading candidate to receive this year&#39;s Nobel Peace Prize, which will be announced Friday, said an independent research institute in Norway. Sen. Piedad Cordoba, right, of Colombia reportedly is one of three top contenders for the Nobel Peace Prize. Sen. Piedad Cordoba is the most likely recipient among three leading contenders, said the Oslo-based International Peace Research Institute. The others the institute named are Jordanian Prince Ghazi bin Muhammad, a philosophy professor in Islamic faith at Jordan University, and Afghan physician and human rights activist Sima Samar. Though the institute considers Cordoba the front-runner, no single candidate has emerged as the clear-cut favorite, as sometimes happens, said Kristian Berg Harpviken, director of the peace institute. &quot;It really is quite open this year,&quot; Harpviken said. This year&#39;s peace prize nominees include 172 people and 33 organizations. The committee does not release the names of the nominees. The 50-year-old peace institute, which is often called PRIO, has no connection with the Nobel committee that awards the peace prize. Harpviken said he believes the | Independent research institute cites three top contenders for Nobel Peace Prize. nNo candidate emerges as clear-cut favorite; winner to be announced Friday. nColombian senator, Jordanian prince, Afghan rights activist among contenders. nVietnamese Buddhist monk, Chinese dissidents also could be awarded prize. | . Training . We&#39;ll prepare our BART model for training by wrapping it in blurr&#39;s HF_BaseModelWrapper object and using the callback, HF_BaseModelCallback, as usual. A new HF_Seq2SeqMetricsCallback object allows us to specify Seq2Seq metrics we want to use, things like rouge and bertscore for tasks like summarization as well as metrics such as meteor, bleu, and sacrebleu for translations tasks. Using huggingface&#39;s metrics library is as easy as specifying a metrics configuration such as below. . Once we have everything in place, we&#39;ll freeze our model so that only the last layer group&#39;s parameters of trainable. See here for our discriminitative learning rates work in fastai. . Note: This has been tested with ALOT of other Seq2Seq models; see the docs for more information. . seq2seq_metrics = { &#39;rouge&#39;: { &#39;compute_kwargs&#39;: { &#39;rouge_types&#39;: [&quot;rouge1&quot;, &quot;rouge2&quot;, &quot;rougeL&quot;], &#39;use_stemmer&#39;: True }, &#39;returns&#39;: [&quot;rouge1&quot;, &quot;rouge2&quot;, &quot;rougeL&quot;] }, &#39;bertscore&#39;: { &#39;compute_kwargs&#39;: { &#39;lang&#39;: &#39;en&#39; }, &#39;returns&#39;: [&quot;precision&quot;, &quot;recall&quot;, &quot;f1&quot;] } } . model = HF_BaseModelWrapper(hf_model) learn_cbs = [HF_BaseModelCallback] fit_cbs = [HF_Seq2SeqMetricsCallback(custom_metrics=seq2seq_metrics)] learn = Learner(dls, model, opt_func=ranger, loss_func=CrossEntropyLossFlat(), cbs=learn_cbs, splitter=partial(seq2seq_splitter, arch=hf_arch)).to_fp16() learn.create_opt() learn.freeze() . . Still experimenting with how to use fastai&#39;s learning rate finder for these kinds of models. If you all have any suggestions or interesting insights to share, please let me know. We&#39;re only going to train the frozen model for one epoch for this demo, but feel free to progressively unfreeze the model and train the other layers to see if you can best my results below. . learn.lr_find(suggestions=True) . SuggestedLRs(lr_min=0.09120108485221863, lr_steep=0.7585775852203369) . It&#39;s also not a bad idea to run a batch through your model and make sure the shape of what goes in, and comes out, looks right. . b = dls.one_batch() preds = learn.model(b[0]) len(preds),preds[0], preds[1].shape . (4, tensor(3.8408, device=&#39;cuda:0&#39;, grad_fn=&lt;NllLossBackward&gt;), torch.Size([2, 68, 50264])) . learn.fit_one_cycle(1, lr_max=3e-5, cbs=fit_cbs) . epoch train_loss valid_loss rouge1 rouge2 rougeL bertscore_precision bertscore_recall bertscore_f1 time . 0 | 1.697474 | 1.692343 | 0.375504 | 0.157680 | 0.253458 | 0.875942 | 0.890704 | 0.883181 | 12:32 | . . And now we can look at the generated predictions using our text_gen_kwargs above . learn.show_results(learner=learn, max_n=2) . text target prediction . 0 (CNN) -- Two weeks. Two gut-wrenching, frustrating, mysterious weeks. That&#39;s how long it&#39;s been since 227 passengers and 12 crew members boarded Malaysia Airlines Flight 370, destined for Beijing. A routine trip, it seemed, to catch up relatives in time for the weekend, start on a work assignment or just get away. Where they got to, still unknown. An exhaustive search -- covering a mind-boggling 2.97 million square miles, which is nearly the size of the continental United States -- has yielded some clues, but no proof of where the Boeing 777 is or definitively what happened to it. The latest, most notable lead revolved around two large objects detected by satellite Sunday floating on waters over 1,400 miles off of Australia&#39;s west coast. The first of several Australian military planes, as well as two long-range commercial jets, resumed their search Saturday morning to find any trace of the objects, amid some skepticism that they or ships in the area ever will and, if they do, that whatever they find will be related to the missing aircraft. Australian Prime Minister Tony Abbott on Friday defended the decision to announce the find, saying Australia owes it to families of those missing &quot;to give them information as soon as it&#39;s | NEW: Planes depart Australia to resume their search for airplane debris. nNEW: Official: Passengers&#39; relatives are moved to a different Kuala Lumpur hotel. nObjects seen on satellite spark intensive search in southern Indian Ocean. nU.S. officials: Files were deleted from flight simulator&#39;s hard drive after February 3. | NEW: Australian military planes resume their search Saturday morning . nThe search area is nearly the size of the continental United States . nIt&#39;s been more than two weeks since Malaysia Airlines Flight 370 disappeared . nA satellite detected two objects floating in waters over 1,400 miles off Australia&#39;s west coast . nAustralia&#39;s prime minister defends the decision to announce the find . | . 1 U.N. weapons inspectors returned &quot;overwhelming and indisputable&quot; evidence of the use of nerve gas in Syria, Secretary-General Ban Ki-moon said Monday, calling the findings &quot;beyond doubt and beyond the pale.&quot; The inspectors&#39; 38-page report was released after Ban briefed Security Council members on its contents. The team found what it called &quot;clear and convincing evidence&quot; that the nerve agent sarin was delivered by surface-to-surface rockets &quot;on a relatively large scale&quot; in the suburbs of the Syrian capital Damascus on August 21. &quot;It is the most significant confirmed use of chemical weapons against civilians since Saddam Hussein used them in Halabja in 1988, and the worst use of weapons of mass destruction in the 21st century,&quot; Ban said. &quot;The international community has a responsibility to ensure that chemical weapons never re-emerge as an instrument of warfare,&quot; he said. Ban called the attack &quot;a war crime&quot; and a violation of treaties banning the use of chemical weapons that date back to 1925. But the inspectors&#39; mandate did not include assigning blame for the attack, and Ban would not speculate on who launched the attack. The team did identify two types or rockets it said were used to deliver the gas and their trajectories, and international | Syria findings &quot;beyond doubt and beyond the pale,&quot; Ban says. nU.S. to provide chemical protective gear to opposition, inspectors. nSarin report demands &quot;a unified and decisive response,&quot; Syrian opposition says. nSyria says helicopter was shot down after straying into Turkish airspace. | Ban Ki-moon calls the findings &quot;beyond doubt and beyond the pale&quot; nInspectors&#39; 38-page report released after Ban briefed Security Council members on its contents . n&quot;The international community has a responsibility to ensure that chemical weapons never re-emerge as an instrument of warfare,&quot; Ban says . nThe inspectors&#39; mandate did not include assigning blame for the attack, and Ban would not speculate on who launched the attack . | . Even better though, blurr augments the fastai Learner with a blurr_summarize method that allows you to use huggingface&#39;s PreTrainedModel.generate method to create something more human-like. . test_article = &quot;&quot;&quot; The past 12 months have been the worst for aviation fatalities so far this decade - with the total of number of people killed if airline crashes reaching 1,050 even before the Air Asia plane vanished. Two incidents involving Malaysia Airlines planes - one over eastern Ukraine and the other in the Indian Ocean - led to the deaths of 537 people, while an Air Algerie crash in Mali killed 116 and TransAsia Airways crash in Taiwan killed a further 49 people. The remaining 456 fatalities were largely in incidents involving small commercial planes or private aircraft operating on behalf of companies, governments or organisations. Despite 2014 having the highest number of fatalities so far this decade, the total number of crashes was in fact the lowest since the first commercial jet airliner took off in 1949 - totalling just 111 across the whole world over the past 12 months. The all-time deadliest year for aviation was 1972 when a staggering 2,429 people were killed in a total of 55 plane crashes - including the crash of Aeroflot Flight 217, which killed 174 people in Russia, and Convair 990 Coronado, which claimed 155 lives in Spain. However this year&#39;s total death count of 1,212, including those presumed dead on board the missing Air Asia flight, marks a significant rise on the very low 265 fatalities in 2013 - which led to it being named the safest year in aviation since the end of the Second World War. Scroll down for videos. Deadly: The past 12 months have been the worst for aviation fatalities so far this decade - with the total of number of people killed if airline crashes reaching 1,158 even before the Air Asia plane (pictured) vanished. Fatal: Two incidents involving Malaysia Airlines planes - one over eastern Ukraine (pictured) and the other in the Indian Ocean - led to the deaths of 537 people. Surprising: Despite 2014 having the highest number of fatalities so far this decade, the total number of crashes was in fact the lowest since the first commercial jet airliner took off in 1949. 2014 has been a horrific year for Malaysia-based airlines, with 537 people dying on Malaysia Airlines planes, and a further 162 people missing and feared dead in this week&#39;s Air Asia incident. In total more than half the people killed in aviation incidents this year had been flying on board Malaysia-registered planes. In January a total of 12 people lost their lives in five separate incidents, while the same number of crashes in February killed 107. &quot;&quot;&quot; . We can override the text_gen_kwargs we specified for our DataLoaders when we generate text using blurr&#39;s Learner.blurr_generate method . outputs = learn.blurr_generate(test_article, early_stopping=True, num_beams=4, num_return_sequences=3) for idx, o in enumerate(outputs): print(f&#39;=== Prediction {idx+1} === n{o} n&#39;) . === Prediction 1 === The past 12 months have been the worst for aviation fatalities so far this decade . The total number of people killed if airline crashes reached 1,158 even before Air Asia plane vanished . 2014 has been a horrific year for Malaysia-based airlines, with 537 people dying on Malaysia Airlines planes, and a further 162 people missing and feared dead in this week&#39;s Air Asia incident . More than half the people killed in aviation incidents this year had been flying on Malaysia-registered planes . === Prediction 2 === The past 12 months have been the worst for aviation fatalities so far this decade . The total number of people killed if airline crashes reached 1,158 even before Air Asia plane vanished . 2014 has been a horrific year for Malaysia-based airlines, with 537 people dying on Malaysia Airlines planes, and a further 162 people missing and feared dead in this week&#39;s Air Asia incident . This year&#39;s total death count of 1,212 marks a significant rise on the very low 265 fatalities in 2013 . === Prediction 3 === The past 12 months have been the worst for aviation fatalities so far this decade . The total number of people killed if airline crashes reached 1,158 even before Air Asia plane vanished . 2014 has been a horrific year for Malaysia-based airlines, with 537 people dying on Malaysia Airlines planes, and a further 162 people missing and feared dead in this week&#39;s Air Asia incident . More than half the people killed in aviation incidents this year had been flying on board Malaysia-registered planes . . What about inference? Easy! . learn.metrics = None learn.export(fname=&#39;ft_cnndm_export.pkl&#39;) . inf_learn = load_learner(fname=&#39;ft_cnndm_export.pkl&#39;) inf_learn.blurr_generate(test_article) . [&#34; The past 12 months have been the worst for aviation fatalities so far this decade . nThe total number of people killed if airline crashes reached 1,158 even before Air Asia plane vanished . n2014 has been a horrific year for Malaysia-based airlines, with 537 people dying on Malaysia Airlines planes, and a further 162 people missing and feared dead in this week&#39;s Air Asia incident . nMore than half the people killed in aviation incidents this year had been flying on Malaysia-registered planes .&#34;] . That&#39;s it . blurr supports a number of huggingface transformer model tasks in addition to summarization (e.g., sequence classification , token classification, and question/answering, causal language modeling, and transation). The docs include examples for each of these tasks if you&#39;re curious to learn more. . For more information about ohmeow or to get in contact with me, head over to ohmeow.com for all the details. . Thanks! .",
            "url": "https://ohmeow.com/posts/2020/05/23/text-generation-with-blurr.html",
            "relUrl": "/posts/2020/05/23/text-generation-with-blurr.html",
            "date": " ‚Ä¢ May 23, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Finding DataBlock Nirvana with fast.ai v2 - Part 1",
            "content": "# !pip install git+https://github.com/fastai/fastai2 # !pip install git+https://github.com/fastai/fastcore . from fastai2.vision.all import * . Intoduction . Flying at 50,000 feet . At a high level, most machine learning and deep learning systems can be summed up as consisting of three primary elements. Data, an architecture/model, and a loss function. It can be visually described as such: . . The data describes the information given to the model for learning a specific task, and the loss function provides the feedback necessary for the model to improve in that task via a number that tells it how well it is doing. . Why is thinking about our data pipeline important? . Simple! You can&#39;t have a good model without a good architecture and proper loss function, but you can&#39;t have anything without data. And getting good data that can be transformed into something modelable isn&#39;t necessarily easy. In the slide deck presentation heard throughout the ML world, Andrej Karpathy, Senior Director of Artifical Intelligence at Tesla, put it this way: . . Coming from academia and the utopia of prepared datasets ready of modeling, he found that in the real world, the bread and butter of a deep learning system and where the blood, sweat, and tears would be shed, was in the data. Data acquisition, cleaning, preparation, and the day-to-day management thereof. This same sentiment can as much be inferred from any of you that watched Jeremy Howard&#39;s v2 walk through in late 20191... every single session was about getting your data modelable using the new v2 bits. That should tell you a lot! . So how do we do it? How do we prepare our datasets for modeling? . While there are many ways, even with fast.ai, most indicators point to it&#39;s DataBlock API as the answer. . What is the DataBlock API? . The DataBlock API is a blueprint for transforming your raw data into something that can fed into a model using the fast.ai framework. It is their high-level data API, one that builds upon their low-level Datasets/DataLoaders API, and also their mid-level Transform based API. . All three incorporate some new ideas for getting your data good to go, and the choice isn&#39;t necessary one or the other. . Dropping down to 30,000 feet ... what is it? . The DataBlock API consists of THREE main components: getters, transforms, and a splitters. . getters tell it how to &quot;get&quot; the raw data (e.g., from the file system as file paths, a Pandas DataFrame). . | transforms tell it how to &quot;transform&quot; that raw data progressively into something that can be fed into a model (e.g., a numeric representation of your inputs and targets). . | splitters define various strategies you can implore to create your training and validation datasets. . | We&#39;ll be talking a lot about transforms in this article, but one of their most interesting characteristics is that they can be defined to transform your raw data into a numerical representation (as &quot;block transforms&quot;), to run on your CPU when an item from your dataset is fetched (as an &quot;item transform&quot;) , or on the GPU after a mini-batch of your data has been collated into a square matrix and right before it is ran through your model (as a &quot;batch transform&quot;). In fact, there are all kinds of hooks into the data processing pipeline whereby you can apply transforms! . An example . Let&#39;s break down one of the DataBlock examples from the documentation: . pets = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=Pipeline([attrgetter(&quot;name&quot;), RegexLabeller(pat = r&#39;^(.*)_ d+.jpg$&#39;)]), item_tfms=Resize(128), batch_tfms=aug_transforms()) . Your getters here are get_items and get_y. The first tells us that our inputs will be coming in the form of filenames returned by the get_image_files function, while the later tells the API to get the labels, or targets, for the inputs by applying a regex to the filename. There is also a get_x method available should you need to apply more specific instructions for defining your input. get_items, get_x, and get_y are all optional. Which you will need to implement depends on your data source and what you need to do to get your inputs/targets. . The splitter parameter tells us that we are going to randomly split this data with 80% for training and 20% for validation. How do I know this? Easy. In your notebook put whatever class/method you are interested followed by two ?? to see it&#39;s source. . RandomSplitter?? . So we got our data and we defined how we&#39;re going to split it for training/validation ... but how do we actually turn that into something we can feed a neural network? That is where transforms come into play and there are three primary kinds: . The data transforms defined in the blocks parameter describe how to &quot;transform&quot; your inputs and targets into what you really want to pass in to your model. Here we apply an ImageBlock to our inputs in order to turn the filenames into numerical representations of our images and a CategoryBlock to turn our targets from string labels to a unique set of numerical indexes for each of the possible labels. Essentially what these transforms do is turn your raw data into numbers because your data HAS to be represented numerically to train any kind of ML or DL model. | Next we define our item transforms via item_tfms. Our only item transform above will resize all our images to 128x128. We do this here because we&#39;ll need squared matrices to pass our images through our network in mini-batches (e.g., a subset of examples), and we can&#39;t create a mini-batch of items until they are all the same shape. These transforms are applied when we fetch an individual item from one of our datasets. | Lastly, we define our batch transforms via batch_tfms for transforms that will be applied to a &quot;mini-batch&quot; of data. Above we&#39;re saying, &quot;There&#39;s a bunch of cool data augmentations we want you to apply to the images in each mini-batch right before you send it through the model.&quot; Again, these transforms are applied on the GPU against a mini-batch of items. | You can apply transforms to a variety of places in the data processing loop, but these three will satisfy your needs 90-95% of the time. . Uh, okay ... so where&#39;s the data? . Remember that the pets DataBlock is just a blueprint, a pipeline for making raw data into modelable data. How do we build something based on this blueprint? Easy. We just call our DataBlock&#39;s dataloaders() method, passing in the one argument our get_items function, get_image_files, needs ... the directory path all your images files are under. . dls = pets.dataloaders(path/&quot;images&quot;) . Once your pets DataBlock knows the &quot;source&quot; of your data, it goes to work. It gets your image filenames, derives each image&#39;s label from that name, creates a training and validation dataset, and then applies the appropirate transforms, at the appropriate time, so that when you pull items from your DataLoaders object (your dls variable), you have something your model understands. This is the object you pass into your Learner to do the actual training. . Here&#39;s some code you can run yourself in colab: . path = untar_data(URLs.PETS) # &lt;-- Download our data; returns the path to that data pets = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=Pipeline([attrgetter(&quot;name&quot;), RegexLabeller(pat = r&#39;^(.*)_ d+.jpg$&#39;)]), item_tfms=Resize(128), batch_tfms=aug_transforms()) dls = pets.dataloaders(path/&quot;images&quot;) # &lt;-- Tell our DataBlock where the &quot;source&quot; is and off it goes dls.show_batch(max_n=9) # dls.valid.show_batch() . The Basics - PyTorch Datasets &amp; Dataloaders . Using the DataBlock API seems magical (well, it kinda is). We&#39;ve seen how easy it is to build this DataLoaders object that can be used to train our models, but in order to see what it is actually going on, we need to start at the beginning, we need to see how this is done natively in PyTorch. . Don&#39;t get confused by the similar concepts and names (e.g., Datasets, DataLoaders, transforms, etc...). Many of these ideas are built into PyTorch and extended to do much more in fast.ai. Just remember ... we&#39;re only working with PyTorch right now. . PyTorch itself provides Dataset and DataLoader classes for getting at our data and being able to iteratively run it through our model via mini-batches. Let&#39;s see how! . Dataset . A Pytorch Dataset (see torch.utils.data.Dataset) is defined as &quot;an abstract class representing a dataset&quot;2. That&#39;s just a fancy way to say it represents a collection of our data. We inherit from it and implement two key methods: . __len__: To return the size of our dataset . __getitem__: To get at a particular item in our dataset. . Let&#39;s start breaking down our DataBlock above by converting the underlying data representation as one of these Dataset classes. We&#39;ll import some new packages that will be using and create a PetCategories class that will allow us to map our target labels with their indexes (and vice-versa). . import pdb, re from torchvision import transforms . class PetCategories(): def __init__(self, image_fpaths, lbl_regex): # not all things are images self.lbl_regex = re.compile(lbl_regex) fpaths = [ f for f in image_fpaths if self.lbl_regex .match(f.name) ] # build our vocab self.vocab = dict(enumerate(set([self.lbl_regex.match(f.name).groups(0)[0] for f in fpaths if self.lbl_regex.match(f.name) ]))) # build a reverse lookup self.o2i = L(self.vocab.values()).val2idx() def get_label(self, fname): return self.lbl_regex.match(fname).groups(0)[0] . class PetsDataset(torch.utils.data.Dataset): def __init__(self, image_fpaths, pet_categories, item_tfms=None): # not all things are images self.fpaths = [ f for f in image_fpaths if f.name.endswith(&#39;.jpg&#39;)] # our &quot;item transforms&quot; self.tfm_pipeline = item_tfms # our labels vocab self.pet_categories = pet_categories def __len__(self): return len(self.fpaths) def __getitem__(self, idx): img_fpath = self.fpaths[idx] img_label = self.pet_categories.get_label(img_fpath.name) # you can think of this as a &quot;block&quot; or an &quot;data transform&quot; img = Image.open(img_fpath) lbl_idx = self.pet_categories.o2i[img_label] if self.tfm_pipeline: img = self.tfm_pipeline(img) return img, torch.tensor(lbl_idx) . There is a lot for you to explore above (step through the code, riddle it with pdb.set_trace statements, change it up and see what happens, etc....), but note the following in particular: . __getitem__ needs to return an &quot;example&quot;, which is two things ... your inputs/targets and they both need to be tensors. | item_tfms represents the PyTorch (not fast.ai) transforms we need to apply to our inputs/targets. We&#39;re going to use a special class named Compose from torchvision to set these up. For now, these transforms will just make sure our images are resized to the same size and converted to a tensor. Again, there is nothing fast.ai here (with the exception of me using the L class) ... we&#39;re just dealing with PyTorch righ now. :) | Notice how we have to create our own vocab and o2i method so we can return an integer representing the &quot;category&quot; rather than the category name (e.g. &quot;Maine_Coon&quot;) itself. Everything has to be a number! | TIP: Run all this code in colab ... do it! Make sure you understand what is going on and why. One of the most valuable techniques I use for learning all things Python, PyTorch, and fast.ai, is using pdb.set_trace() to step through and debug code. It&#39;s great way to build inutition by printing out the shapes of tensors, running parts of the code interactively, etc.... . Now ...we&#39;re going to need TWO Datasets ... one for training and one for validation. We&#39;ll split our examples up randomly and set aside 20% for our validation set. There&#39;s many ways to do this (most better and more efficient that below). . all_images = (path/&#39;images&#39;).ls(); len(all_images) . 7393 . rnd_idxs = np.random.permutation(len(all_images)); len(rnd_idxs) . 7393 . cut = int(len(rnd_idxs) * .2); cut . 1478 . train_idxs, valid_idxs = rnd_idxs[cut:], rnd_idxs[:cut] print(len(train_idxs), len(valid_idxs), len(train_idxs) + len(valid_idxs)) . 5915 1478 7393 . TIP: Notice how I print out lengths and shapes of tensors as I go? Doing that provides both a sanity check and ensure you are seeing what you expect before going further down the rabbit hole. . Now, we can create our training and validation Datasets. . Again, we are NOT using fast.ai transforms here ... these are all built into the torchvision package. They serve the same purpose here as the fast.ai &quot;item transforms&quot;, but for now, we&#39;re doing this all using just the PyTorch bits. . item_tfms = transforms.Compose([ transforms.RandomResizedCrop(224), transforms.ToTensor() ]) . categories = PetCategories(all_images[train_idxs], lbl_regex=r&#39;^(.*)_ d+.jpg$&#39;) len(categories.vocab) . 37 . train_ds = PetsDataset(all_images[train_idxs], pet_categories=categories, item_tfms=item_tfms) valid_ds = PetsDataset(all_images[valid_idxs], pet_categories=categories, item_tfms=item_tfms) print(len(train_ds), len(valid_ds)) print(train_ds[20][0].shape, train_ds[20][1]) . 5913 1477 torch.Size([3, 224, 224]) tensor(33) . DataLoader . With that we can create a torch.utils.data.DataLoader from each Dataset. The primary reason we need this object is to yield mini-batches of data into our model, but as you can see, it also provides us the ability to do much more (e.g., shuffle data, provide a collate function, etc...). Check out the docs for more info! . Note: fast.ai has it&#39;s own DataLoader class that extends THIS one from PyTorch. Yah, I know it can seem confusing, but just remember for now, we are only working with functionality built-in to PyTorch. . bsz = 64 train_dl = torch.utils.data.DataLoader(train_ds, batch_size=bsz, shuffle=True) valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=bsz*2, shuffle=False) . And voila, we can now iterate through our dataset, mini-batch by mini-batch . b = next(iter(valid_dl)) len(b), b[0].shape, b[1].shape . (2, torch.Size([128, 3, 224, 224]), torch.Size([128])) . Wow ... that took quite a bit more work than the 6 lines of code to create a DataBlock, and it&#39;s still not as functional. For example, we haven&#39;t built anything that can decode items, show batches, or allow us to easily adjust/extend the objects we created above. . So let&#39;s keep going. Starting with the low-level API, we can take these PyTorch Dataset and DataLoader objects more friendly for fast.ai Learners. . Using the Low-Level API - fast.ai DataLoaders . It&#39;s actually pretty easy to get your PyTorch Dataset class incorporated into fast.ai and get it to play nicely with fast.ai&#39;s custom DataLoaders. . from fastai2.data.core import DataLoaders . dls = DataLoaders.from_dsets(train_ds, valid_ds) . b = dls.one_batch() . len(b), b[0].shape, b[1].shape . (2, torch.Size([64, 3, 224, 224]), torch.Size([64])) . I told you it was simple, didn&#39;t I? . Notice that we didn&#39;t have to change anything in our PyTorch Dataset to create a DataLoaders object we can pass to our Learner for training. This is nice because it means, given a standard PyTorch Dataset, you can use all the wonderful fast.ai bits for training in less than 3 lines of code. . Tip: If you don&#39;t care about being able to show batches, show results, and this satisfies your needs ... STOP! You&#39;re good to go. Don&#39;t overthink you&#39;re problem or over-engineer a solution to a problem that doesn&#39;t necessarily exist. Remember: You don&#39;t have to use the mid-level API or DataBlocks to use fast.ai! . Using the Mid-Level API - Converting Your Dataset into a Transform . BUT what if we want to apply/change our transforms, or run transforms on the GPU after we have a batch, or be able to visualize our data in our datasets and dataloaders or even our predictions? To begin with, we can convert our Dataset into a Transform by doing 4 things: . Inherit from Transform instead of torch.utils.data.Dataset | Change your __getitem__ into encodes. According to the docs ... &quot;a Transform in fastai calls the encodes method when you apply it on an item (a bit like PyTorch modules call forward when applied on something).&quot;3 Here it will return the numerical representations of our data in the form of tensors. | Change your return type to be a tuple and optionally use fastai&#39;s semantic types (here we wrap our image in TensorImage which knows how to show itself). From the docs: &quot;If you then return a tuple (or a subclass of a tuple), and use fastai&#39;s semantic type, you can then apply any other fastai&#39;s transform on your data and it will be dispatched properly.&quot;4 That simply means we can add on more transforms that know how to work with TensorImage objects and they&#39;ll do the right thing. | Get rid of __len__ | class PetsTransform(Transform): def __init__(self, image_fpaths, pet_categories, item_tfms=None): # not all things are images self.fpaths = [ f for f in all_images if f.name.endswith(&#39;.jpg&#39;)] # our pytorch &quot;item transforms&quot; self.tfm_pipeline = item_tfms # our labels vocab self.pet_categories = pet_categories def __len__(self): return len(self.fpaths) def encodes(self, idx): img_fpath = self.fpaths[idx] img_label = self.pet_categories.get_label(img_fpath.name) # you can think of this as a &quot;block&quot; or an &quot;data transform&quot; img = Image.open(img_fpath) lbl_idx = self.pet_categories.o2i[img_label] if self.tfm_pipeline: img = self.tfm_pipeline(img) return (TensorImage(img), torch.tensor(lbl_idx)) . Now that we are using a Transform, we have to use a new kind of object to build our dataset: TfmdLists . A TfmdList is &quot;just an object that lazily applies a collection of Transforms on a list.&quot;5 Think of it as a fancy Dataset object that knows how to work with Transform objects. . train_fpaths = all_images[train_idxs] valid_fpaths = all_images[valid_idxs] train_tl= TfmdLists(range(len(train_idxs)), PetsTransform(train_fpaths, pet_categories=categories, item_tfms=item_tfms)) valid_tl= TfmdLists(range(len(valid_idxs)), PetsTransform(valid_fpaths, pet_categories=categories, item_tfms=item_tfms)) . Since this is just another kind of dataset, we can pass these TfmdLists objects to DataLoaders just like before. But notice, we can now add fast.ai transforms to it just like we did in the DataBlock example at the top. We&#39;re already resizing and converting the examples to tensors, so we&#39;ll add some after_batch transforms for normalization and augmentations. . dls = DataLoaders.from_dsets(train_tl, valid_tl, after_batch=[Normalize.from_stats(*imagenet_stats), *aug_transforms()]) dls = dls.cuda() . b = dls.one_batch() len(b), b[0].shape, b[1].shape . (2, torch.Size([64, 3, 224, 224]), torch.Size([64])) . Let&#39;s see if we can show a batch of our data. Uncomment the line below, run it, and yah ... it throws an exception. But why? . . If you guessed it is because show_batch doesn&#39;t know what to do with the target&#39;s numerical index, bingo! You&#39;re right. . Let&#39;s start to fix that by actually creating our own class that represents our inputs/targets. Notice that besides inheriting from Tuple, all we are providing is a show method that tells a PetImage object how to show itself. According to the docs, &quot;fastai will call [your transforms decodes methods] until it arrives at a type that knows how to show itself, then call the show method on this type.&quot;6 . BTW, a lot of this code is just ripped from the &quot;Siamese tutorial&quot; in the docs, so don&#39;t be too impressed. If you want to really do a deep dive and work though all this given a different task, check it out here. . class PetImage(Tuple): def show(self, ctx=None, **kwargs): img, category_idx = self if not isinstance(img, Tensor): img_tensor = tensor(img) img_tensor = img_tensor.permute(2,0,1) else: img_tensor = img return show_image(img_tensor, title=categories.vocab[category_idx], ctx=ctx, **kwargs) . The show method knows how to work with tensors or PIL images. The last method is a helper method available in fast.ai to actually show an image and print it&#39;s title above it. If you pass in a ctx it will use that to format and place the images appropriate. A context can be something like a matplotlib axis or a DataFrame ... it &quot;represents the object where we will show our thing.&quot;7 . Now let&#39;s make some changes to our PetsTransform to make it a bit more fastai&#39;sh. . First, we&#39;ll use PILImage.create to create the image in encodes. We do this because that object allows us to apply fast.ai transform liks Resize and ToTensor directly on it. . Second, we&#39;re going to move to using fast.ai transforms for everything, so we&#39;ll get rid of the PyTorch transforms! . Third, notice our encodes now returns a PetsImage. It&#39;s just a tuple ... but because its a particular kind of tuple, we can use the typdispatched show_batch and show_results to actually visualize our data/results. . class PetsTransform2(Transform): def __init__(self, image_fpaths, pet_categories): # not all things are images self.fpaths = [ f for f in all_images if f.name.endswith(&#39;.jpg&#39;)] # our labels vocab self.pet_categories = pet_categories def __len__(self): return len(self.fpaths) def encodes(self, img_fpath): img = PILImage.create(img_fpath) img_label = self.pet_categories.get_label(img_fpath.name) lbl_idx = self.pet_categories.o2i[img_label] return PetImage(img, lbl_idx) . Because of these changes, instead of creating the separate TfmdLists ourselves, we can now further do things the &quot;fast.ai way&quot; by using a splitter to do that for us. Here we&#39;ll use RandomSplitter which gives us that same 80/20 training/validation split. . splits = RandomSplitter()(all_images) tfm = PetsTransform2(all_images, categories) . Now we can get both our datasets in one line of code! When we pass splits to TfmdLists, it takes care of creating our training and validation datasets! . tls = TfmdLists(all_images, tfm, splits=splits) . And thanks for our PetImage class, fast.ai can show an item from our dataset. . show_at(tls.valid, 0) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fee17e84518&gt; . Even better, we can now specify all our transforms using fast.ai in the call to dataloaders(). And because these are fast.ai DataLoader objects, we can add tranforms at any point in our data processing pipeline (not just after_item and after_batch). . dls = tls.dataloaders(after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]) . In the process, notice how we&#39;ve also refactored our code into something much more reusable. For example, if we want to resize our images to something else, its as easy as ... . new_dl = dls.new(after_item=[Resize(64), ToTensor]) new_dl.one_batch()[0].shape . torch.Size([64, 3, 64, 64]) . And what about showing a batch of data? Unfortunately it still won&#39;t work. show_batch is designed primarily to work with the DataBlock API, but here, we&#39;re returning the whole thing as a single transform. . The solution is easy: use the @typedispatch mechanism and override show_batch so that our x (our input) is &quot;typed&quot;. . b = dls.one_batch() . dls._types, type(b) . ({__main__.PetImage: [fastai2.torch_core.TensorImage, torch.Tensor]}, __main__.PetImage) . @typedispatch def show_batch(x:PetImage, y, samples, ctxs=None, max_n=6, nrows=None, ncols=3, figsize=None, **kwargs): if figsize is None: figsize = (ncols*6, max_n//ncols * 3) if ctxs is None: ctxs = get_grid(min(x[0].shape[0], max_n), nrows=None, ncols=ncols, figsize=figsize) for i,ctx in enumerate(ctxs): PetImage(x[0][i], x[1][i].item()).show(ctx=ctx) . dls.show_batch() . When dls.show_batch() runs, it will find the closes matching version of show_batch() available to execute given chat the batch is. We could even write a typedispatched show_results() to look at our predictions alongside our targets using the same technique we applied to show_batch(). . Using the mid-level API, you not only have a Dataloaders object good to go for training ... you have one that you can use to show your data and extend by applying/changing as many transforms to wherever you want in the data processing pipeline. . What could be better than this? . Answer: Doing all this with &lt; 10 lines of code using the DataBlock API. . We&#39;ve already looked at how it works above, now, we&#39;ll look at the questions you need to ask to construct it in accordance with your data and task. Again, if the above gets you where you need to be, you don&#39;t need to use the high-level DataBlock API. There is no right option for every task and there are many ways to get where you need to go. . Using the High-Level API - DataBlocks . Having looked at the basic data-processing units in PyTorch, then to the low and mid-level APIs available in fast.ai, you&#39;re probably wondering, &quot;Ok, how can I do all that by drawing up a DataBlock blueprint for my task?&quot; . The path to enlightment comes in the form of answering 7 questions. . Asking the right questions . Assuming you understand your task and data, once you&#39;ve answered these 7 questions you&#39;ll know everything you need to construct your own DataBlock. These come right out of the DataBlock tutorial so check that for even more details and example implementations! . What are the types of your inputs and targets? (e.g., images/categories) | Where is your data? (e.g., filenames in folders, a DataFrame, a database) | Do we need to do anything special to get our &quot;inputs&quot;? If so, use get_x | Do we need to do anything special to get our &quot;targets&quot;? If yes, use get_y | How do you want to split the data into training and validation sets? Use splitter | Do we need to do anything when we get an item? If yes, define that in item_tfms | Do we need to do anything to a &quot;mini-batch&quot; of data? If yes, define that in batch_tfms | Getting the right answers . Looking back at our example DataBlock ... . pets = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=Pipeline([attrgetter(&quot;name&quot;), RegexLabeller(pat = r&#39;^(.*)_ d+.jpg$&#39;)]), item_tfms=Resize(128), batch_tfms=aug_transforms()) . We knew how to construct it as such because: . 1.What are the types of your inputs and targets? Answer: inputs=pet images | targets=37 categories. So we need an ImageBlock to handle the images and a CategoryBlock to handle the labels. Those blocks will add the needed transforms for each of their respective pieces. . 2.Where is your data? Answer: filenames . 3.Do we need to do anything special to get our &quot;inputs&quot;? Answer: No, get_items will get our input images. . 4.Do we need to do anything special to get our &quot;targets&quot;? Answer: Yes, we need to implement a get_y to get our labels from the image file name. . 5.How do you want to split the data into training and validation sets? Answer: We just want a random 80/20 split, so use RandomSplitter . 6.Do we need to do anything when we get an item? Answer: Yes, we need to resize our images so they are the same shape and can be included together in a mini-batch. Do this in item_tfms . 7.Do we need to do anything to a &quot;mini-batch&quot; of data? Answer: Yes, we&#39;d like to add some randomization to the images by applying data augmentations on the GPU. Do this with batch_tfms . Tips, Tricks, Best Practices, &amp; A Bunch of Good Things to Know . Below are some of the more important things and best practices to be aware of when working with the DataBlock API. It&#39;s in no way exhaustive, but anything I&#39;ve had to lookup multiple times is listed here. . What happens if I don&#39;t define how to get my targets (my y)? . If you don&#39;t specify your labels, the DataBlock API will assume they are the same as your inputs. This is atypical for most tasks, but not entirely useless. According to the docs, &quot;by default, the data block API assumes we have an input and a target, which is why we see our filename repeated twice&quot; whenever you view the results of your datasets/dataloaders without a y specified.8 . Can I have multiple inputs/targets? . Yes! According to the docs ... &quot;You can also have more than two blocks (if you have multiple inputs and/or targets), you would just need to pass n_inp to the DataBlock to tell the library how many inputs there are (the rest would be targets) and pass a list of functions to get_x and/or get_y (to explain how to process each item to be ready for his type).&quot;9 We&#39;ll explore this in Part 2 of this series where I attempt to update my v1 MixedTabluarList object (incorporates tabular + text) into something v2 friendly. In the meantime, here&#39;s a nice example from the docs on setting up a dataset for object detection: . coco = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=[lambda o: img2bbox[o.name][0], lambda o: img2bbox[o.name][1]], item_tfms=Resize(128), batch_tfms=aug_transforms(), n_inp=1) . You see that n_inp? It&#39;s saying, &quot;Use the ImageBlock for my inputs (I only have 1), but I&#39;ll need TWO targets this time as I&#39;m trying to predict the location of an object (BBoxBlock) and it&#39;s label (BBoxLblBlock).&quot; Notice also because we are predicting TWO things, our get_y returns a list of, you guessed it, two things. If we didn&#39;t need to do anything special with either of these targets, we&#39;d simply pass noop in it&#39;s place in that list. . Where can I learn about the baked in bits of the DataBlock API? . The API already has a lot of useful classes and functions suitable for defining your getters, splitter, and transforms across a number of application types. The full list is here: http://dev.fast.ai/data.transforms . What if something goes wrong? Or what if I want to make sure my DataBlock is doing what I think it is? . Use dblock.summary(path). If there is an error, this thing will bomb out where it is encountered ... else, you&#39;ll be able to verify that all the wonderful things your 5-10 lines of code above does what you expect. . Do I need to always use get_items? . No. For example, if your &quot;source&quot; data is a DataFrame ... . pascal = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=ColSplitter(), get_x=ColReader(0, pref=pascal_source/&quot;train&quot;), get_y=ColReader(1, label_delim=&#39; &#39;), item_tfms=Resize(224), batch_tfms=aug_transforms()) dls = pascal.dataloaders(df) . According to the docs ... &quot;we wont have to use a get_items function here because we already have all our data in one place.&quot;10 . What are different ways I can get my x and y from a DataFrame? . Using ColReader: . get_x=ColReader(0, pref=pascal_source/&quot;train&quot;), get_y=ColReader(1, label_delim=&#39; &#39;) . Using lambda functions: . get_x=lambda x:pascal_source/&quot;train&quot;/f&#39;{x[0]}&#39;, get_y=lambda x:x[1].split(&#39; &#39;), . Using column names: . get_x=lambda o:f&#39;{pascal_source}/train/&#39;+o.fname, get_y=lambda o:o.labels.split(), . Using from_columns: . def _pascal_items(x): return (f&#39;{pascal_source}/train/&#39;+x.fname, x.labels.str.split()) valid_idx = df[df[&#39;is_valid&#39;]].index.values pascal = DataBlock.from_columns(blocks=(ImageBlock, MultiCategoryBlock), get_items=_pascal_items, splitter=IndexSplitter(valid_idx), item_tfms=Resize(224), batch_tfms=aug_transforms()) . According to the docs, this is &quot;the most efficient way (to avoid iterating over the rows of the dataframe, which can take a long time) .... It will use get_items to convert the columns in numpy arrays. The drawback is that since we lose the dataframe after extracting the relevant columns, we can&#39;t use a ColSplitter anymore.&quot;11 . What about tabular data? . We&#39;ll explore the tabular bits in a later part, but as the docs say, the &quot;tabular data doesn&#39;t really use the data block API as it&#39;s relying on another API with TabularPandas for efficient preprocessing and batching.&quot;12 Of course, where there is a will, there is a way, and so we&#39;ll see a possible solution in Part 2 or 3 of this series :). . Summary . As the famous song goes, &quot;we&#39;ve only just begun ....&quot; In future installments we&#39;ll dig into more of the particulars of the entire fast.ai data stack, and see how we can use it to solve some &quot;out-of-the-box&quot; tasks. . In the meantime, the best way for you to get a better handle on what&#39;s what, is to mess around with the many examples found in the v2 documentation here. . References . http://dev.fast.ai/tutorial.datablock | http://dev.fast.ai/tutorial.siamese | http://dev.fast.ai/data.block | http://dev.fast.ai/data.transforms | fastai v2 walk-thru playlist | Zach Mueller&#39;s &quot;A Guided Walk-through of 2.0&quot;: Lesson 1 | 1. See full playlist here‚Ü© . 2. https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class‚Ü© . 3. http://dev.fast.ai/tutorial.siamese#Using-the-mid-level-API‚Ü© . 4. Ibid.‚Ü© . 5. http://dev.fast.ai/tutorial.siamese#Using-the-mid-level-API‚Ü© . 6. http://dev.fast.ai/tutorial.siamese#Making-show-work‚Ü© . 7. Ibid.‚Ü© . 8. http://dev.fast.ai/tutorial.datablock‚Ü© . 9. Ibid.‚Ü© . 10. Ibid.‚Ü© . 11. Ibid.‚Ü© . 12. Ibid.‚Ü© .",
            "url": "https://ohmeow.com/posts/2020/04/11/finding-datablock-nirvana-part-1.html",
            "relUrl": "/posts/2020/04/11/finding-datablock-nirvana-part-1.html",
            "date": " ‚Ä¢ Apr 11, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Loss Functions: Cross Entropy Loss and You!",
            "content": "!pip install fastai . import torch from torch.nn import functional as F from fastai2.vision.all import * . We&#39;ve been doing multi-classification since week one, and last week, we learned about how a NN &quot;learns&quot; by evaluating its predictions as measured by something called a &quot;loss function.&quot; . So for multi-classification tasks, what is our loss function? . path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.loss_func . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth . . FlattenedLoss of CrossEntropyLoss() . Negative Log-Likelihood &amp; CrossEntropy Loss . To understand CrossEntropyLoss, we need to first understand something called Negative Log-Likelihood . Negative Log-Likelihood (NLL) Loss . Let&#39;s imagine a model who&#39;s objective is to predict the label of an example given five possible classes to choose from. Our predictions might look like this ... . preds = torch.randn(3, 5); preds . tensor([[-0.3139, 0.6737, -0.0143, 1.9929, -0.6949], [ 0.5285, 0.1311, 0.2628, 0.6450, 1.7745], [-1.7458, 2.0199, -0.1365, 1.4622, -0.0940]]) . Because this is a supervised task, we know the actual labels of our three training examples above (e.g., the label of the first example is the first class, the label of the 2nd example the 4th class, and so forth) . targets = torch.tensor([0, 3, 4]) . Step 1: Convert the predictions for each example into probabilities using softmax. This describes how confident your model is in predicting what it belongs to respectively for each class . probs = F.softmax(preds, dim=1); probs . tensor([[0.0635, 0.1704, 0.0856, 0.6372, 0.0433], [0.1421, 0.0955, 0.1089, 0.1596, 0.4939], [0.0126, 0.5458, 0.0632, 0.3125, 0.0659]]) . If we sum the probabilities across each example, you&#39;ll see they add up to 1 . probs.sum(dim=1) . tensor([1.0000, 1.0000, 1.0000]) . Step 2: Calculate the &quot;negative log likelihood&quot; for each example where y = the probability of the correct class . loss = -log(y) . We can do this in one-line using something called tensor/array indexing . example_idxs = range(len(preds)); example_idxs . range(0, 3) . correct_class_probs = probs[example_idxs, targets]; correct_class_probs . tensor([0.0635, 0.1596, 0.0659]) . nll = -torch.log(correct_class_probs); nll . tensor([2.7574, 1.8349, 2.7194]) . Step 3: The loss is the mean of the individual NLLs . nll.mean() . tensor(2.4372) . ... or using PyTorch . F.nll_loss(torch.log(probs), targets) . tensor(2.4372) . Cross Entropy Loss . ... or we can do this all at once using PyTorch&#39;s CrossEntropyLoss . F.cross_entropy(preds, targets) . tensor(2.4372) . As you can see, cross entropy loss simply combines the log_softmax operation with the negative log-likelihood loss . So why not use accuracy? . def plot_function(f, tx=None, ty=None, title=None, min=-2, max=2, figsize=(6,4)): x = torch.linspace(min,max) fig,ax = plt.subplots(figsize=figsize) ax.plot(x,f(x)) if tx is not None: ax.set_xlabel(tx) if ty is not None: ax.set_ylabel(ty) if title is not None: ax.set_title(title) . def f(x): return -torch.log(x) plot_function(f, &#39;x (prob correct class)&#39;, &#39;-log(x)&#39;, title=&#39;Negative Log-Likelihood&#39;, min=0, max=1) . NLL loss will be higher the smaller the probability of the correct class . What does this all mean? The lower the confidence it has in predicting the correct class, the higher the loss. It will: . 1) Penalize correct predictions that it isn&#39;t confident about more so than correct predictions it is very confident about. . 2) And vice-versa, it will penalize incorrect predictions it is very confident about more so than incorrect predictions it isn&#39;t very confident about . Why is this better than accuracy? . Because accuracy simply tells you whether you got it right or wrong (a 1 or a 0), whereast NLL incorporates the confidence as well. That information provides you&#39;re model with a much better insight w/r/t to how well it is really doing in a single number (INF to 0), resulting in gradients that the model can actually use! . Rember that a loss function returns a number. That&#39;s it! . Or the more technical explanation from fastbook: . &quot;The gradient of a function is its slope, or its steepness, which can be defined as rise over run -- that is, how much the value of function goes up or down, divided by how much you changed the input. We can write this in maths:(y_new-y_old) / (x_new-x_old). Specifically, it is defined when x_new is very similar to x_old, meaning that their difference is very small. But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. So the problem is that a small change in weights from x_old to x_new isn&#39;t likely to cause any prediction to change, so (y_new - y_old) will be zero. In other words, the gradient is zero almost everywhere. As a result, a very small change in the value of a weight will often not actually change the accuracy at all. This means it is not useful to use accuracy as a loss function. When we use accuracy as a loss function, most of the time our gradients will actually be zero, and the model will not be able to learn from that number. That is not much use at all!&quot; 1 . Summary . So to summarize, accuracy is a great metric for human intutition but not so much for your your model. If you&#39;re doing multi-classification, your model will do much better with something that will provide it gradients it can actually use in improving your parameters, and that something is cross-entropy loss. . References . https://pytorch.org/docs/stable/nn.html#crossentropyloss | http://wiki.fast.ai/index.php/Log_Loss | https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/ | https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy | https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/ | 1. fastbook chaper 4‚Ü© .",
            "url": "https://ohmeow.com/posts/2020/04/04/understanding-cross-entropy-loss.html",
            "relUrl": "/posts/2020/04/04/understanding-cross-entropy-loss.html",
            "date": " ‚Ä¢ Apr 4, 2020"
        }
        
    
  
    
  
    
  
    
        ,"post20": {
            "title": "Understanding the F-Beta metric",
            "content": "Overview . scikit-learn describes the F-Beta score &quot;as the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0&quot; with the &quot;beta parameter [determining] the weight of recall in the combined score.&quot; It is one of the most common metrics enlisted in demonstrating the performance of binary, multi-classification, and multi-label classifiers. . So what does all that mean? . In a nutshell, it says that this metric can be used to help you understand how good your classification model is based on the relative importance you ascribe to precision and recall in making that determination. Common beta values are 0.5 (precision is king), 1 (precision and recall are equally important), and 2 (recall is king). . When you look at the documentation, you&#39;ll notice there are several other interesting arguments you can pass into it. Two of the more mysterious ones being average and sample_weight. Will explore what they mean how you may want to use them based on your dataset. . The two metrics, along with other important terms, are described well in this post. Let&#39;s imagine a multi-classification model that tries to determine whether a photo show a picture of a dog, cat, or bird. . Precision vs. Recall . The two metrics, along with other important terms, are described really well in this post. Let&#39;s imagine a multi-classification model that tries to determine whether a given photo is a picture of a dog, cat, or bird. . Precision . Definition: When your classifier predicted a label, how often was it correct? . Example: When you predicted &#39;cat&#39;, how often were you right? . Formula: True Positive (TP) / PREDICTED Label (TP + False Positive or FP) . # TP = number of cat prediction you got right tp = 100 # FP = number of cat predictions you got wrong fp = 10 precision = tp / (tp + fp) # = 0.91 . Recall . Definition: For every actual label in your dataset, how often did your classifier pick the correct one? . Example: When it&#39;s actually &#39;cat&#39;, how often did it predict &#39;cat&#39;? . Formula: True Positive (TP) / ACTUAL Label (TP + False Negative or FN) . # TP = number of cat prediction you got right tp = 100 # FN = number of actual cats you predicted as something else fn = 5 recall = tp / (tp + fn) # = 0.95 . Okay, so which one should I use? . This depends on your task. . If you&#39;re task is to predict whether a patient has cancer given set of symptoms and test results, it&#39;s going to be far more important to you that all actual cancer patients get flagged even at the expense of non-cancer patients being flagged incorrectly. This is recall. In this particular kind of task, you&#39;re also likely going to be facing a dataset were the vast majority of examples are &quot;not cancer.&quot; A case where using metrics like precision and accuracy will likely look really good but be completely misleading. Other examples where you want to maximize recall include fraud and network anomaly detection. . On the otherhand, if you&#39;re task is to predict whether an e-mail is spam or not (1=spam|0=not spam), you recognize that it&#39;s not the end of the world if your user gets a junk e-mail. If fact, it would be worse if an actual e-mail got flagged as junk and they didn&#39;t see it. Getting it wrong is more acceptable than making sure all the true cases are gotten right. This is precision. Here, you&#39;re more concerned about your classifiers overall predictive capability in coming up with the right answer, yes or no. . What about our cats, dogs, birds? . Good question, again it depends on the task. All things be equal, most likely we care more about precision or we care about both equally in this case. Fortunately, the F-Beta metric gives us the power to determine the worth of our model regardless of how we want to weight the two. .",
            "url": "https://ohmeow.com/temp-posts/fbeta-metric",
            "relUrl": "/temp-posts/fbeta-metric",
            "date": " ‚Ä¢ Jan 1, 1999"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey there! Wayde here. . I‚Äôm the owner of ohmeow.com and have been engaged in the development of mid-to-enterprise level application development for over 20 years. An active member in the fast.ai community and contributor to their deep learning framework, you can usually find me on their forums and/or tweeting about the latest and greatest from the world of AI. I also have the privilege to mentor a number of High School students on a local FIRST Robotics FRC team (go team 2102!). . I‚Äôm not one for most social media (honestly, most of it‚Äôs nonsense and a net negative to our species), however, you can find me on twitter where my account is primarily professional in nature or via e-mail. If you want to talk shop or see where we can help your organization, we‚Äôd love to hear from you! . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://ohmeow.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Deployment Guide",
          "content": "Deployment Techniques . . &gt; How can I deploy my full-stack application as Docker containers? . See: Flask + Vue + MySQL on Docker. Part III ‚Äî Let‚Äôs Dockerize . &gt; What containers should I consider deploying? . See: Web deployment using AWS Lightsail . &gt; What to deploy a containerized application to AWS LightSail . See: Lightsail Containers: An Easy Way to Run your Containers in the Cloud Amazon Lightsail Tutorial: Deploy an NGINX Reverse Proxy How to build a Pocket Platform-as-a-Service . . AWS Tips &amp; Tricks . . &gt; How can I use multiple AWS accounts from the CLI? . See: How to use multiple AWS accounts from the command line? . .",
          "url": "https://ohmeow.com/guides/deployment",
          "relUrl": "/guides/deployment",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "FastAPI Guide",
          "content": "Designing your API . . . What are some good reference architectures? See: Using FastAPI to Build Python Web APIs Full Stack FastAPI and PostgreSQL - Base Project Generator Up and running with fastapi series . . How do I do user registration/login, e-mail verification, password resets, etc... with fastapi? See: Handle Registration in FastAPI and Tortoise ORM Handling Email Confirmation During Registration in Flask Flask Rest API -Part:5- Password Reset E-commerce API with FastAPI | Sending Verification Emails | FastAPI-Mail . . What does a star(*) mean in a method parameter? It simply allows you to order your arguments so those without default values can be placed ahead of those that can. It also ensures that keyword arguments are used everywhere (which may or may not be desirable when refactoring code). I usually find it unnecessary except in places where I‚Äôm using BackgroundTasks or a depency injected argument somewhere, for example: . @app.get(&quot;/&quot;) def get_username(*, db: Session = Depends(get_db), user_id: int) -&gt; str: return db.query(User).get(user_id).username . See: What does a star(*) mean in a method parameter? Order the parameters as you need, tricks . Working with Pydantic objects . . . How to cast one pydantic object into another? MyModel(**my_model_from.dict()) . Dockerizing a FastAPI application . . . How can I Dockerize my app? See: How to Dockerize a Python App with FastAPI An Extremely Simple Docker, Traefik, and Python FastAPI Example . Dealing with common errors . . . I get an Unprocessable Entity (422) error when I post/put to my API It usually means the body of your request doesn‚Äôt mesh with what your API method is expected. Make sure that the object your passing in matches what you‚Äôve specified, including using the Body(...[,embed=True]) types correctly . See: Body - Multiple Parameters Python: FastApi (Unprocessable Entity) error . . AttributeError: &#39;coroutine&#39; object has no attribute &#39;X&#39; Usually means you are missing ‚Äúawait‚Äù from async method (fastapi/python) . . I&#39;m using [Encode Databases](https://github.com/encode/databases) against a postgres database, but my `RETURNING` statements don&#39;t return all the columns I specify RETURNING statements work okay with fetch_one/fetch_all. If you are using execute, it won‚Äôt work See: Support for RETURNING . . When I deploy using Nginx reverse-proxy, I get mixed content errors like the one below... Mixed Content: The page at &#39;https://page.com&#39; was loaded over HTTPS, but requested an insecure XMLHttpRequest endpoint &#39;http://page.com?filter=xxxx&#39;. This request has been blocked; the content must be served over HTTPS. . If you add a trailing slash (/) in your API requests it will fix the problem BUT a much easier solution is to the correct proxy headers for Uvicorn. They should look something like this: . upstream api_server { server ${API_HOST}:${API_PORT} fail_timeout=0; } ... server { ... location ~ /api/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; proxy_pass http://api_server; } . See the following resources: Failure to load any static files when deploying with HTTPS fastapi-react nginx.conf Add option for adding a trailing slash automatically Ajax Product Filter does not work in https - Fixed . .",
          "url": "https://ohmeow.com/guides/fastapi",
          "relUrl": "/guides/fastapi",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Guides",
          "content": "Full-stack web application . Vue / Quasar . FastAPI . PostgreSQL . Deployment . . Machine Learning / Deep Learning . How to learn (Deep Learning) .",
          "url": "https://ohmeow.com/guides/",
          "relUrl": "/guides/",
          "date": ""
      }
      
  

  

  
      ,"page6": {
          "title": "PostgreSQL Guide",
          "content": "Database Design . . &gt; How can I restrict the possible values for a column? . You have two options: . A CHECK CONSTRAINT: ALTER TABLE distributors ADD CONSTRAINT check_types CHECK (element_type = &#39;lesson&#39; OR element_type = &#39;quiz&#39;); . | A ENUM: CREATE TYPE element_type AS ENUM (&#39;lesson&#39;, &#39;quiz&#39;); . See: In Postgres, how do you restrict possible values for a particular column? . | .",
          "url": "https://ohmeow.com/guides/postgresql",
          "relUrl": "/guides/postgresql",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
      ,"page11": {
          "title": "Vue 3 Guide",
          "content": "Core . . Computed properties . &gt; How to I define a computed property using the Composition API? . // just a getter const userEmail = computed(() =&gt; { return $store.getters.userEmail }) ... // a getter and setter const selectedUserClientId = computed({ get: () =&gt; { return $store.getters.selectedUserClientId }, set: (newVal) =&gt; { $store.dispatch(&#39;setSelectedUserClientId&#39;, { client_id: newVal }) } }) . See: How to type a computed property in the new composition API? How to call setter for object in computed properties . . Watchers / WatchEffect . &gt; How to I define an asynchronous watcher or watchEffect? . import { ref, watch, watchEffect } from &#39;vue&#39; import { useRoute } from &#39;vue-router&#39; export default { setup() { const route = useRoute() ... // watch watch(route, async (nv, ov) =&gt; { const { data } = await api.get(`get-something/blah`) console.log(data) }) // watchEffect const stateChanged = watchEffect(async () =&gt; { const { data } = await api.get(`get-something/blah`) console.log(data) }) ... . &gt; When should I use watch or watchEffect? . Use watch when ‚Ä¶ . You want to do something with one or more specific reactive objects changes | You need both the old value and the new value | You need it to be called lazily | Use watchEffect when ‚Ä¶ . You want to do something whenever the state changes for any of your reactive objects | You don‚Äôt care about the old value(s) | You need it to run immediately when reactive dependencies change | See: Vue 3 Composition API - watch and watchEffect How to use Vue Watch and Vue watchEffect . . Routes . &gt; How can I pass data between routes using params? . Simply add the data to your params attribute: . &lt;router-link :to=&quot;{ name: &#39;my-named-route&#39;, params: { id: 1, another_param: &#39;something else&#39; } }&quot;&gt; Go to my page &lt;/router-link&gt; . See: Vue, is there a way to pass data between routes without URL params? . . Vuex . &gt; How can I watch for changes in the store‚Äôs values? . Simply add the data to your params attribute: . setup(props) { const store = useStore(); watch(()=&gt;store.getters.myvalue, function() { console.log(&#39;value changes detected&#39;); }); return { myvalue: computed(() =&gt; store.getters.myvalue) } }, . See: Vue3 composition api watch store value . &gt; How can I watch for changes in the store‚Äôs values across browser tabs and windows? . const app = new Vue({ el: &#39;#app&#39;, data: { name: &#39;&#39; }, methods: { onStorageUpdate(event) { if (event.key === &quot;name&quot;) { this.name = event.newValue; } } }, mounted() { if (localStorage.name) { this.name = localStorage.name; } window.addEventListener(&quot;storage&quot;, this.onStorageUpdate); }, beforeDestroy() { window.removeEventListener(&quot;storage&quot;, this.onStorageUpdate); }, watch: { name(newName) { localStorage.name = newName; } } }); . See: Reactive localStorage object across browser tabs using Vue.js . &gt; How can I store complex objects like arrays and dictionaries in localStorage? . By using JSON.stringfy(X) when storing the data and JSON.parse(X) when you fetch the data . var names = []; names[0] = prompt(&quot;New member name?&quot;); localStorage.setItem(&quot;names&quot;, JSON.stringify(names)); var storedNames = JSON.parse(localStorage.getItem(&quot;names&quot;)); // ... or ... localstorage.names = JSON.stringify(names); var storedNames = JSON.parse(localStorage.names); . See: How do I store an array in localStorage? . . Quasar . . Working with q-table . &gt; How can I create grid for CRUD operations using a modal form for inserts/upserts with filtering, sorting, etc‚Ä¶? . See: QTable with Action Buttons Quasar QTable: Editing with QPopupEdits and QButtons to add/delete/update rows Quasar - q-table with toggle filters and text search .",
          "url": "https://ohmeow.com/guides/vue3",
          "relUrl": "/guides/vue3",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  
      ,"page17": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://ohmeow.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}