<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Wayde Gilliam">
<meta name="dcterms.date" content="2024-07-14">
<meta name="description" content="In generative NLP applications, the effectiveness of your model hinges on the quality and relevance of the contextual data your provide it. Whether tailored to a specific use case or unique business domain, whether generated synthetically or pulled from existing data sources, good contextual data is key for later curating a dataset that can be used in effective evaluation pipelines and potential fine tuning. To get this right, you need to clearly define your objectives … and that’s what we will be talking about in this post.">

<title>LLM Workshop #2 - From Noise to Knowledge: Mastering the Art of Objective Definition and Data Refinement” – ohmeow</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/ohmeow_favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7f5c32456752ae48d544551be6ddc928.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<meta property="og:title" content="LLM Workshop #2 - From Noise to Knowledge: Mastering the Art of Objective Definition and Data Refinement” – ohmeow">
<meta property="og:description" content="In generative NLP applications, the effectiveness of your model hinges on the quality and relevance of the contextual data your provide it. Whether tailored to a specific use case or unique business domain, whether generated synthetically or pulled from existing data sources, good contextual data is key for later curating a dataset that can be used in effective evaluation pipelines and potential fine tuning. To get this right, you need to clearly define your objectives … and that’s what we will be talking about in this post.">
<meta property="og:image" content="https://ohmeow.com/posts/images/blog-20240714/blog-20240714-logo.png">
<meta property="og:site_name" content="ohmeow">
<meta property="og:image:height" content="1024">
<meta property="og:image:width" content="1792">
<meta name="twitter:title" content="LLM Workshop #2 - From Noise to Knowledge: Mastering the Art of Objective Definition and Data Refinement” – ohmeow">
<meta name="twitter:description" content="In generative NLP applications, the effectiveness of your model hinges on the quality and relevance of the contextual data your provide it. Whether tailored to a specific use case or unique business domain, whether generated synthetically or pulled from existing data sources, good contextual data is key for later curating a dataset that can be used in effective evaluation pipelines and potential fine tuning. To get this right, you need to clearly define your objectives … and that’s what we will be talking about in this post.">
<meta name="twitter:image" content="https://ohmeow.com/posts/images/blog-20240714/blog-20240714-logo.png">
<meta name="twitter:image-height" content="1024">
<meta name="twitter:image-width" content="1792">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../images/ohmeow_logo.png" alt="ohmeow.com" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">ohmeow</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../blog.html"> <i class="bi bi-journal-text" role="img">
</i> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-guides" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-tools" role="img">
</i> 
 <span class="menu-text">Guides</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-guides">    
        <li>
    <a class="dropdown-item" href="../pages/guides/vue3-guide.html"><i class="bi bi-box-arrow-up-right" role="img">
</i> 
 <span class="dropdown-text">Vue3</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/guides/fastapi-guide.html"><i class="bi bi-box-arrow-up-right" role="img">
</i> 
 <span class="dropdown-text">Fast API</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/guides/postgresql-guide.html"><i class="bi bi-box-arrow-up-right" role="img">
</i> 
 <span class="dropdown-text">PostgreSQL</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/guides/deployment-guide.html"><i class="bi bi-box-arrow-up-right" role="img">
</i> 
 <span class="dropdown-text">Deployment</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-stack" role="img">
</i> 
 <span class="menu-text">Projects</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="https://ohmeow.github.io/blurr/" target="blank"><i class="bi bi-layers" role="img">
</i> 
 <span class="dropdown-text">blurr</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-study-groups" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-book" role="img">
</i> 
 <span class="menu-text">Study Groups</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-study-groups">    
        <li>
    <a class="dropdown-item" href="https://www.youtube.com/playlist?list=PLD80i8An1OEF8UOb9N9uSoidOGIMKW96t" target="blank"><i class="bi bi-youtube" role="img">
</i> 
 <span class="dropdown-text">fastai x Hugging Face Study Group</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/waydegilliam" target="blank"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ohmeow" target="blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-do-you-want-to-build-and-should-you-build-it" id="toc-what-do-you-want-to-build-and-should-you-build-it" class="nav-link active" data-scroll-target="#what-do-you-want-to-build-and-should-you-build-it">What Do You Want To Build and Should You Build It?</a></li>
  <li><a href="#objective-definition" id="toc-objective-definition" class="nav-link" data-scroll-target="#objective-definition">Objective Definition</a>
  <ul class="collapse">
  <li><a href="#version-1.0" id="toc-version-1.0" class="nav-link" data-scroll-target="#version-1.0">Version 1.0</a></li>
  <li><a href="#the-surface-area" id="toc-the-surface-area" class="nav-link" data-scroll-target="#the-surface-area">The Surface Area</a></li>
  <li><a href="#retrospective" id="toc-retrospective" class="nav-link" data-scroll-target="#retrospective">Retrospective</a></li>
  </ul></li>
  <li><a href="#data-refinement" id="toc-data-refinement" class="nav-link" data-scroll-target="#data-refinement">Data Refinement</a>
  <ul class="collapse">
  <li><a href="#build-some-datasets" id="toc-build-some-datasets" class="nav-link" data-scroll-target="#build-some-datasets">Build Some Datasets</a></li>
  </ul></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next Steps</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">LLM Workshop #2 - From Noise to Knowledge: Mastering the Art of Objective Definition and Data Refinement”</h1>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">LLMS</div>
    <div class="quarto-category">data cleaning</div>
    <div class="quarto-category">learning</div>
    <div class="quarto-category">projects</div>
  </div>
  </div>

<div>
  <div class="description">
    In generative NLP applications, the effectiveness of your model hinges on the quality and relevance of the contextual data your provide it. Whether tailored to a specific use case or unique business domain, whether generated synthetically or pulled from existing data sources, good contextual data is key for later curating a dataset that can be used in effective evaluation pipelines and potential fine tuning. To get this right, you need to clearly define your objectives … and that’s what we will be talking about in this post.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Wayde Gilliam </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 14, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="what-do-you-want-to-build-and-should-you-build-it" class="level2">
<h2 class="anchored" data-anchor-id="what-do-you-want-to-build-and-should-you-build-it">What Do You Want To Build and Should You Build It?</h2>
<p>If you want to avoid a lot of needless back-and-forth as you go through the process of curating datasets for evaluation and/or fine tuning, building your eval pipeleines, training models, and so forth, stop and ask yourself, “What do I want to build and why?”. How you answer those questions will inform everything going forward!</p>
<p>Why am I making a big deal about this?</p>
<p>Simple, when I started … I didn’t do it.</p>
<p>I had a general idea of building a tool calling system for work and just dove in, and as I would get into various aspects of development I’d be like, “Hold on, this isn’t right … why am I trying to do this when all I really need is this?” In the end, I had to start over a few times, and before the last time, I decided to go outside with the dogs and a good cup of coffee and <strong><em>really</em></strong> think about what I wanted to accomplish and why.</p>
<p>Below is the result of that quiet time well spent.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip: Give your objectices a good think!
</div>
</div>
<div class="callout-body-container callout-body">
<p>It’s crucial to deeply consider what you aim to achieve and understand the reasons behind it. Reflecting on your objectives and motivations helps ensure that your efforts are purposeful and aligned with your aspirations. It will also help prevent you from needlessly starting over, as well as, give you confidence that what you “think you want” is realistic, useful, and will add value.</p>
<p>Things to ask yourself:</p>
<ul>
<li><p>What do you want the model to do for your end users?</p></li>
<li><p>Can you define specifically what the model is supposed to do, or is it so general that you find it difficult to scope or measure quality?</p></li>
<li><p>How diverse and varied does your data need to be to get the model to do what you want?</p></li>
</ul>
</div>
</div>
</section>
<section id="objective-definition" class="level2">
<h2 class="anchored" data-anchor-id="objective-definition">Objective Definition</h2>
<p>For this project I’ll be building a system where a user can ask an AI to perform various NLP tasks on one or more text documents. These documents represent survey comments sourced from several kinds of surveys delievered in higher education institutions (e.g., staff, faculty, student satisfaction and engagement surveys). There are core tasks that I need to perform like machine translation and sentiment analysis, but I’d also like to build a system whereby a user can provide it their own tools focused on NLP and the system figure out how to use them correctly.</p>
<section id="version-1.0" class="level3">
<h3 class="anchored" data-anchor-id="version-1.0">Version 1.0</h3>
<p>Currently I have a pipeline, that for every task, makes a call to GPT-4 for every document. I did this because I couldn’t get good results by giving GPT several somewhat complex tools for it to use based on what the user asked. The tools would get called but the results weren’t great. Evaluation has been left to the eyeballs of myself and other human data scientists, which of course isn’t optimal either.</p>
<p>Given that this current system is slow to process any substantial set of survey comments, relatively expensive given the number of calls to OpenAI I’m making, and doesn’t have a real evaluation-first workflow … I figured it was a good candidate for the course project. At the very least I could build an eval framework to really understand how well things were working and maybe fine tune a smaller model that could understand how to properly call all the tools to satisfy an end user’s ask in a single go.</p>
</section>
<section id="the-surface-area" class="level3">
<h3 class="anchored" data-anchor-id="the-surface-area">The Surface Area</h3>
<p>If your objectives are too general, for example a general purpose chatbot or even a general purpose tool calling machine, you are likely headed to some rough times. Considering my objective above, what can I infer about what I really want to build here?</p>
<ol type="1">
<li><p>The model should use <strong>NLP tools</strong> for specifically understanding <strong>survey comments from staff, students, and faculty in higher education</strong>. These comments are sourced from a number of surveys delievered to higher education audiences all over the country</p></li>
<li><p><strong>Some clients allow for their surveys to be taken into Spanish</strong>, so the machine translation tool will need to be called initially for each of these comments so that the remaining tasks can be performed wholistically on English texts.</p></li>
<li><p>Aside from machine translation, <strong>the core tools consist of functions for sentiment analysis, NER, summarization, and thematic analysis.</strong></p></li>
<li><p><strong>Each task operations on either whole survey comments or on semantically chunked comments</strong> that need to be associated to a “topic” for thematic analysis.</p></li>
<li><p>The <strong>tools should be focused on performing NLP tasks well, particular in the business domain of survey comments in higher educational settings.</strong></p></li>
</ol>
</section>
<section id="retrospective" class="level3">
<h3 class="anchored" data-anchor-id="retrospective">Retrospective</h3>
<p>Considering the above, this system definitely needs a solid eval workflow but also presents a potential great argument for fine tuning.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Narrow domains are great for fine tuning
</div>
</div>
<div class="callout-body-container callout-body">
<p>The more specific and specialized the task and/or data, the more likely you are to find success in fine tuning</p>
</div>
</div>
<p>Both the domain and tasks are very specific to higher education and NLP analysis in that setting, I have access to a lot of diverse data, and there is definite business value in creating a model that might reduce costs, speed up processing, and perform at the same level as GPT-4.</p>
<p>Given all this, I feel pretty good about proceeding.</p>
</section>
</section>
<section id="data-refinement" class="level2">
<h2 class="anchored" data-anchor-id="data-refinement">Data Refinement</h2>
<p>Based on my objectives, I need to chiefly be able to support two uses cases:</p>
<ol type="1">
<li><p><strong>Tasks that need to be performed on a single document</strong> (e.g., translation, summary, NER, sentiment). Traditionally we have only done sentiment on chunks because it is the chunks that are associated to topics and we want to report sentiment by topic. This still needs to happen but it might also be interesting to predict sentiment across the full comment as well.</p></li>
<li><p><strong>Tasks that need to be performed on a collection of related documents</strong> (.e.,g thematic analysis, topic sentiment).</p></li>
</ol>
<p>In this section we ask, “How can I curate a set of survey comments and semantically related chunks that are varied enough to support these two use cases across all our survey clients?”</p>
<section id="build-some-datasets" class="level3">
<h3 class="anchored" data-anchor-id="build-some-datasets">Build Some Datasets</h3>
<p>I need a diverse set of documents from different survey clients, delivered to different audiences (staff, student, and faculty), and with a subset being in Spanish. I also need to clean up these documents so they are actually useful. If they are too short or represent useless statements like “N/A” or “I don’t know”, they probably won’t be very helpful.</p>
<p>I don’t need to synthetically create any of the raw documents since I created the survey sytem and have access to the Microsoft SQL Server database it lies in.</p>
<p>Thinking about the use cases I need to support, the following four datasets will be created:</p>
<ol type="1">
<li><p><code>_clean</code>: A cleaned up version of the raw document dataset</p></li>
<li><p><code>_sample_Xk</code>: A sample of <code>X</code> rows from <code>cleaned</code> (mine will end up to be roughly 14k survey comments)</p></li>
<li><p><code>_sample_Xk_chunked</code>: A properly chunked version of <code>_sample_Xk</code> that includes predicted topics as well.</p></li>
<li><p><code>_sample_Xk_topics</code>: The chunks associated to each topic</p></li>
</ol>
<p>Document analysis tasks will use a mix of datasets #2 and #3 so they can learn to operate of both full and chunked comments. Related document analysis will use dataset #4 for thematic analysis and sentiment.</p>
<p>I am prohibited from sharing the actual data, but I can share the code I wrote to create these datasets for this project.</p>
<section id="step-1-cleanup" class="level4">
<h4 class="anchored" data-anchor-id="step-1-cleanup">Step 1: Cleanup</h4>
<p>I have almost 500k comments that span decades of surveys. Before building a representative sample, I want to remove those comments that aren’t likely to be used or even of much use for NLP tasks. For example, comments that are too short or uninformative (e.g., like “N/A”, or “Great”) shouldn’t be part of eval/training datasets. Preprocessing in version 1.0 takes these comments out in current pipeline so we’ll do so here as well.</p>
<p>After loading the full dataset and filtering on the columns I need to build up the datasets above, my DataFrame looks like this: <img src="images/blog-20240714/raw-dataframe.png" class="img-fluid" alt="The raw data from the production DB"></p>
<p>In particular, we’ll run the functions below to slim this dataset down and give us quality comments we can confidently use in building out our sampled datasets.</p>
<section id="a.-remove-nans-and-trim-whitespaces" class="level5">
<h5 class="anchored" data-anchor-id="a.-remove-nans-and-trim-whitespaces">A. Remove NaNs and Trim Whitespaces</h5>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_document_preprocessing(df: pd.DataFrame, text_col: <span class="bu">str</span>):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Remove empty documents and clean up whitespace."""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.copy()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove all rows where `text_attr` is Nan</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.dropna(subset<span class="op">=</span><span class="bu">list</span>(<span class="bu">set</span>([attr <span class="cf">for</span> attr <span class="kw">in</span> [text_col] <span class="cf">if</span> attr <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>])), how<span class="op">=</span><span class="st">"all"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove all whitespace from `text_attr`, `language_attr`, `non_english_text_attr`</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> attr <span class="kw">in</span> <span class="bu">list</span>(<span class="bu">set</span>([text_col, <span class="st">"AnswerLang"</span>])):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> attr <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            df[attr] <span class="op">=</span> df[attr].<span class="bu">apply</span>(<span class="kw">lambda</span> v: <span class="bu">str</span>(v).strip())</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> init_document_preprocessing(raw_df, text_col<span class="op">=</span><span class="st">"AnswerText"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="b.-remove-duplicate-documents" class="level5">
<h5 class="anchored" data-anchor-id="b.-remove-duplicate-documents">B. Remove Duplicate Documents</h5>
<p>Duplicate comments won’t be helpful and they definitely exist even within a single survey. For example, some users try to game the system by repeating the same comment thoughout the survey in hopes of making sure their points float to the top.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_duplicate_documents(df: pd.DataFrame, text_col: <span class="bu">str</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Eliminate duplicate documents."""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.copy()</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    unique_attrs <span class="op">=</span> [text_col]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove duplicate rows (keeping the first instance)</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df.drop_duplicates(subset<span class="op">=</span>unique_attrs).copy()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> remove_duplicate_documents(verbatims_df, text_col<span class="op">=</span><span class="st">"AnswerText"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="c.-remove-duplicated-sentences" class="level5">
<h5 class="anchored" data-anchor-id="c.-remove-duplicated-sentences">C. Remove Duplicated Sentences</h5>
<p>The same folks above also like to try and game the system by repeating the same phrase over and over again. The below code does a decent job at eliminating a string of repetitive content.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_duplicate_sentences(text: <span class="bu">str</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Remove duplicated sentences throughout an entire text sequence."""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> text <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">""</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r"(?&lt;=[.!?])\s*"</span>, <span class="st">" "</span>, text)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> nltk.sent_tokenize(text)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    unique_sentences <span class="op">=</span> <span class="bu">list</span>(<span class="bu">dict</span>.fromkeys(sentences))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenize the text into sentences and remove duplicates by converting to set and back to list</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">" "</span>.join(unique_sentences)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_duplicate_sentences_in_documents(df: pd.DataFrame, text_col: <span class="bu">str</span>):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.copy()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    df[text_col] <span class="op">=</span> df[text_col].<span class="bu">apply</span>(<span class="kw">lambda</span> v: remove_duplicate_sentences(v))</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> remove_duplicate_sentences_in_documents(verbatims_df, text_col<span class="op">=</span><span class="st">"AnswerText"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="d.-remove-meaningless-and-short-documents" class="level5">
<h5 class="anchored" data-anchor-id="d.-remove-meaningless-and-short-documents">D. Remove Meaningless and Short Documents</h5>
<p>Comments that are too short or uninformative aren’t helpful to any NLP tasks. The code below removes much of this based on my examination of the data and running classification models that predicts whether a comment <code>is_nonsense</code>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>remove_texts <span class="op">=</span> [</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"none"</span>,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"nothing"</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not sure"</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"no suggestions"</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"no comment"</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"no comments"</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"nothing to add"</span>,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n/a"</span>,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"none at this time"</span>,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"no comments at all"</span>,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"see previous comment"</span>,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"not really"</span>,</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_bad_value(s, remove_texts<span class="op">=</span>[]):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    is_bad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> rt <span class="kw">in</span> remove_texts:</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        is_bad <span class="op">=</span> <span class="bu">bool</span>(re.match(<span class="vs">rf"^</span><span class="sc">{</span>rt<span class="sc">.</span>lower()<span class="sc">.</span>strip()<span class="sc">}</span><span class="vs">[!.,;:?]*$"</span>, <span class="bu">str</span>(s).lower().strip()))</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_bad:</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> is_bad</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_meaningless_documents(df, text_col: <span class="bu">str</span>):</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.copy()</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    df[text_col] <span class="op">=</span> df[text_col].<span class="bu">apply</span>(<span class="kw">lambda</span> v: <span class="bu">str</span>(v) <span class="cf">if</span> v <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> <span class="kw">not</span> is_bad_value(v, remove_texts) <span class="cf">else</span> <span class="st">""</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_short_documents(df: pd.DataFrame, text_col: <span class="bu">str</span>, min_text_length: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>):</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.copy()</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df[pd.notna(df[text_col]) <span class="op">&amp;</span> (df[text_col].<span class="bu">str</span>.<span class="bu">len</span>() <span class="op">&gt;=</span> min_text_length)]</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> remove_meaningless_documents(verbatims_df, text_col<span class="op">=</span><span class="st">"AnswerText"</span>)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> remove_short_documents(verbatims_df, text_col<span class="op">=</span><span class="st">"AnswerText"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That’s it for dataset #1, let’s save it</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df.to_parquet(<span class="ss">f"</span><span class="sc">{</span>DATA_DIR<span class="sc">}</span><span class="ss">/clean/documents_all.parquet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="step-2-sample" class="level4">
<h4 class="anchored" data-anchor-id="step-2-sample">Step 2: Sample</h4>
<p>There are a lot of documents so we’ll create a subset of &lt; 15k with enough variation for meaningful scoring functions and finetunes</p>
<section id="a.-build-a-representative-subset" class="level5">
<h5 class="anchored" data-anchor-id="a.-build-a-representative-subset">A. Build a Representative Subset</h5>
<p>There are two core types of surveys, satisfaction and engagement. There are many more instances of the satisfaction surveys so we’ll want to have more of those in our sampled dataset to mimic reality.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>filtered_df <span class="op">=</span> df[(df[<span class="st">"BenchmarkSurveyType"</span>].<span class="bu">str</span>.startswith(<span class="st">"CSS-"</span>))]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the number of samples for each BenchmarkSurveyType</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> filtered_df[<span class="st">"BenchmarkSurveyType"</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>) <span class="op">*</span> <span class="dv">10000</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> sample_sizes.<span class="bu">round</span>().astype(<span class="bu">int</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample records proportionally with a bias towards longer comments</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>sampled_df <span class="op">=</span> pd.DataFrame()</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> survey_type, size <span class="kw">in</span> sample_sizes.items():</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    subset <span class="op">=</span> filtered_df[filtered_df[<span class="st">"BenchmarkSurveyType"</span>] <span class="op">==</span> survey_type]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> subset[<span class="st">"AnswerTextCharacterCount"</span>] <span class="op">/</span> subset[<span class="st">"AnswerTextCharacterCount"</span>].<span class="bu">sum</span>()</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    sampled_records <span class="op">=</span> subset.sample(n<span class="op">=</span>size, weights<span class="op">=</span>weights, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    sampled_df <span class="op">=</span> pd.concat([sampled_df, sampled_records])</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset index if needed</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>sampled_df <span class="op">=</span> sampled_df.reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># This gets us down to 10k examples of customer satisfaction like surveys.</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>sampled_df <span class="op">=</span> pd.concat([sampled_df, df[(df[<span class="st">"BenchmarkSurveyType"</span>].<span class="bu">str</span>.startswith(<span class="st">"SAW"</span>))]])</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding in these staff engagement surveys get us to a total of almost 14k examples</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="b.-add-in-some-spanish-documents" class="level5">
<h5 class="anchored" data-anchor-id="b.-add-in-some-spanish-documents">B. Add in some Spanish Documents</h5>
<p>I lied when I said there was no syntetically generated data because we’re going to use an LLM to generate some Spanish content from our English survey comments. We’ll use one of my favorite libraries, <a href="https://www.langchain.com/" target="_blank">LangChain</a>, to generate 250 Spanish examples.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SpanishTranslation(BaseModel):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The translation of a document from English to Spanish."""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    spanish_translation: <span class="bu">str</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"The English tranlsation"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_openai_translation_messages(domain: <span class="bu">str</span> <span class="op">=</span> <span class="st">"survey comments"</span>):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    system_msg <span class="op">=</span> <span class="ss">f"You are a world class translator. Translate the English </span><span class="sc">{</span>domain<span class="sc">}</span><span class="ss"> below to Spanish. Properly escape strings."</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    human_msg <span class="op">=</span> <span class="st">"</span><span class="sc">{input}</span><span class="st">"</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    prompt_msgs <span class="op">=</span> [SystemMessage(content<span class="op">=</span>system_msg), HumanMessagePromptTemplate.from_template(human_msg)]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prompt_msgs</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> ChatOpenAI(model<span class="op">=</span><span class="st">"gpt-4"</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>messages <span class="op">=</span> get_openai_translation_messages()</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> ChatPromptTemplate(messages<span class="op">=</span>messages)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>functions <span class="op">=</span> [convert_to_openai_function(SpanishTranslation)]</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>chain <span class="op">=</span> prompt <span class="op">|</span> llm.bind(function_call<span class="op">=</span>{<span class="st">"name"</span>: SpanishTranslation.<span class="va">__name__</span>}, functions<span class="op">=</span>functions) <span class="op">|</span> JsonOutputFunctionsParser()</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>spanish_sample_df <span class="op">=</span> sampled_df.sample(<span class="dv">250</span>, random_state<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>spanish_translations <span class="op">=</span> []</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> r_idx, r <span class="kw">in</span> spanish_sample_df.iterrows():</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    rsp <span class="op">=</span> chain.invoke({<span class="st">"input"</span>: <span class="bu">str</span>(r[<span class="st">"AnswerText"</span>])})</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    spanish_translations.append(rsp[<span class="st">"spanish_translation"</span>])</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>sampled_df[<span class="st">"AnswerText_NonEnglish"</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>sampled_df[<span class="st">"AnswerLang"</span>] <span class="op">=</span> <span class="st">"English"</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>spanish_sample_df[<span class="st">"AnswerText_NonEnglish"</span>] <span class="op">=</span> spanish_translations</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>spanish_sample_df[<span class="st">"AnswerLang"</span>] <span class="op">=</span> <span class="st">"Spanish"</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Set 'MLVerbatId' as the index for both DataFrames</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>sampled_df.set_index(<span class="st">"MLVerbatimId"</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>spanish_sample_df.set_index(<span class="st">"MLVerbatimId"</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Update df1 with values from df2</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>sampled_df.update(spanish_sample_df)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Reset index if needed</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>sampled_df.reset_index(inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That’s it for dataset #2, let’s save it</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>df.to_parquet(<span class="ss">f"</span><span class="sc">{</span>DATA_DIR<span class="sc">}</span><span class="ss">/clean/documents_sample_14k.parquet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="step-3-chunk" class="level4">
<h4 class="anchored" data-anchor-id="step-3-chunk">Step 3: Chunk</h4>
<p>The core thematic analysis task we need to support operates on semantically related survey comment “chunks”. In addition to predicting themes for these collections, we also need to report the sentiment for each chunk so that we can visualize sentiment as it relates to each topic.</p>
<p>We’ll use the <a href="https://github.com/aurelio-labs/semantic-chunkers" target="\_blank">semantic-chunkers</a> library to build semantically related chunks which is something the current pipeline doesn’t use yet, but makes a lot of sense to me after exploring chunking in depth over the past few months or so. I really like this library and it comes with some nifty visualization capabilities for tuning its hyperparameters.</p>
<p>Chunking can be somewhat complex and use case specific, but for the puposes of survey comments we are usually working with paragraphs and sentences sometimes containing bullet points to further deliniate different topics. Given this, I’ve asked chatGPT about general token usage for English paragraphs and sentences to set a few of these hyperparameters and do some basic preliminary chunking before using the <code>semantic-chunkers</code> library.</p>
<p>According to chatGPT, the average number of characters in an English sentence and paragraph can vary based on factors such as writing style, purpose, and medium. However, general estimates are as follows:</p>
<ol type="1">
<li><p><strong>Average Characters in an English Sentence:</strong></p>
<ul>
<li>An average English sentence typically contains around 15 to 20 words.</li>
<li>Assuming an average word length of 5 characters (including spaces and punctuation), an average sentence would be approximately 75 to 100 characters.</li>
</ul></li>
<li><p><strong>Average Characters in an English Paragraph:</strong></p>
<ul>
<li>An average English paragraph usually contains about 3 to 5 sentences.</li>
<li>Using the upper bound of 5 sentences and assuming each sentence is 100 characters, an average paragraph would be around 300 to 500 characters.</li>
</ul></li>
</ol>
<p>These averages can fluctuate based on the type of text (e.g., academic writing, casual writing, technical documentation) and individual writing styles but it seems reasonable to assume on average:</p>
<ul>
<li>A min sentence has 15 * 1.5 = 23 tokens</li>
<li>A max paragrpah as 20 _ 5 _ 1.5 = 150 tokens</li>
</ul>
<p>If you’re interested in learning about the <code>semantic-chunkers</code>, check out these resources:</p>
<p><a href="https://github.com/aurelio-labs/semantic-chunkers/blob/main/docs/00-chunkers-intro.ipynb" target="\_blank">Semantic Chunkers Into (Colab)</a></p>
<p><a href="https://www.youtube.com/watch?v=TcRRfcbsApw" target="\_blank">Semantic Chunking for RAG (James Briggs)</a></p>
<section id="a.-preliminary-chunking" class="level5">
<h5 class="anchored" data-anchor-id="a.-preliminary-chunking">A. Preliminary Chunking</h5>
<p>We start with the assumption that paragraphs likely represent distinct topics a user is trying to get at in any given survey comment. If there are bullet points, regardless of format, we also assume that each of these likely represent a distinct idea or “chunk.” Given this, we will do some initial chunking based on those assumptions.</p>
<p>Also, I have and continue to look at a lot of this data, so they aren’t really assumptions as much as they simply reflect what I see.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chunk_paragraphs(text):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    paragraphs <span class="op">=</span> re.split(</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"</span><span class="ch">\n\n</span><span class="st">|</span><span class="ch">\r\n</span><span class="st">|</span><span class="ch">\\\\</span><span class="st">n</span><span class="ch">\\\\</span><span class="st">n|</span><span class="ch">\\\\</span><span class="st">r</span><span class="ch">\\\\</span><span class="st">n|"</span> <span class="op">+</span> <span class="vs">r"\\n\s*[-•*o]|\\n\s*\d+[.)]"</span>, text</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># text.split("\n\n")  # Split text by double line breaks to identify paragraphs</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [chunk.strip() <span class="cf">for</span> chunk <span class="kw">in</span> paragraphs <span class="cf">if</span> <span class="bu">len</span>(chunk.strip()) <span class="op">&gt;</span> <span class="dv">4</span>]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chunk_texts(df: pd.DataFrame):</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Chunk the paragraphs keeping any bullet points alongside their context."""</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.copy()</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    df.insert(<span class="dv">1</span>, <span class="st">"_seq"</span>, df[<span class="st">"AnswerText"</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> v: chunk_paragraphs(v)))</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.explode(<span class="st">"_seq"</span>).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"_seq"</span>] <span class="op">=</span> df[<span class="st">"_seq"</span>].<span class="bu">str</span>.strip()</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    df.insert(<span class="dv">2</span>, <span class="st">"_seq_id"</span>, df.groupby([<span class="st">"MLVerbatimId"</span>]).cumcount())</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"_seq_length"</span>] <span class="op">=</span> df[<span class="st">"_seq"</span>].<span class="bu">str</span>.<span class="bu">len</span>()</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_bullet_points(text):</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> re.sub(<span class="vs">r"(^|\n)\s*[-•*o]\s*|\n\s*\d+[.)]\s*"</span>, <span class="st">" "</span>, text).strip()</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>chunked_df <span class="op">=</span> chunk_texts(sampled_df)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>chunked_df[<span class="st">"_seq"</span>] <span class="op">=</span> chunked_df[<span class="st">"_seq"</span>].<span class="bu">apply</span>(remove_bullet_points)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="b.-semantic-chunking" class="level5">
<h5 class="anchored" data-anchor-id="b.-semantic-chunking">B. Semantic Chunking</h5>
<p>From the <code>semantic-chunkers</code> intro notebook mentioned above:</p>
<blockquote class="blockquote">
<p>The statistical chunking method our most robust chunking method, it uses a varying similarity threshold to identify more dynamic and local similarity splits. It offers a good balance between accuracy and efficiency but can only be used for text documents (unlike the multi-modal ConsecutiveChunker).</p>
</blockquote>
<blockquote class="blockquote">
<p>The StatisticalChunker can automatically identify a good threshold value to use while chunking our text, so it tends to require less customization than our other chunkers.</p>
</blockquote>
<p>I did some review of specific examples and played with the hyperparameters to get what looked like decent results. Here is what I came up with ultimately for perform the final chunking of the dataset.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> HuggingFaceEncoder(name<span class="op">=</span><span class="st">"thenlper/gte-large"</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>chunker <span class="op">=</span> StatisticalChunker(</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    encoder<span class="op">=</span>encoder,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    threshold_adjustment<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    dynamic_threshold<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    window_size<span class="op">=</span><span class="dv">5</span>,  <span class="co"># 5,</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    min_split_tokens<span class="op">=</span><span class="dv">23</span>,  <span class="co"># 100,</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    max_split_tokens<span class="op">=</span><span class="dv">300</span>,  <span class="co"># 500</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    split_tokens_tolerance<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    plot_chunks<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    enable_statistics<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_semantic_chunks(txt: <span class="bu">str</span>, min_chars_to_chunk: <span class="bu">int</span> <span class="op">=</span> <span class="dv">90</span>):</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(txt.strip()) <span class="op">&lt;</span> min_chars_to_chunk:</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> [txt.strip()]</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        chunks <span class="op">=</span> chunker(docs<span class="op">=</span>[txt])</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [chunk.content.strip() <span class="cf">for</span> chunk <span class="kw">in</span> chunks[<span class="dv">0</span>]]</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [txt.strip()]</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>seqs <span class="op">=</span> chunked_df[<span class="st">"_seq"</span>].values.tolist()</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>chunked_docs <span class="op">=</span> []</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> seq <span class="kw">in</span> tqdm(seqs):</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    chunked_docs.append(get_semantic_chunks(seq))</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>chunked_df.insert(<span class="dv">1</span>, <span class="st">"_chunk"</span>, chunked_docs)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>chunked_df <span class="op">=</span> chunked_df.explode(<span class="st">"_chunk"</span>).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>chunked_df[<span class="st">"_chunk"</span>] <span class="op">=</span> chunked_df[<span class="st">"_chunk"</span>].<span class="bu">str</span>.strip()</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>chunked_df.insert(<span class="dv">2</span>, <span class="st">"_chunk_id"</span>, chunked_df.groupby([<span class="st">"MLVerbatimId"</span>, <span class="st">"_seq_id"</span>]).cumcount())</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>chunked_df[<span class="st">"_chunk_length"</span>] <span class="op">=</span> chunked_df[<span class="st">"_chunk"</span>].<span class="bu">str</span>.<span class="bu">len</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="c.-use-berttopic-associate-each-chunk-to-a-topic" class="level5">
<h5 class="anchored" data-anchor-id="c.-use-berttopic-associate-each-chunk-to-a-topic">C. Use <code>BertTopic</code> Associate Each Chunk To A Topic</h5>
<p>I’m not going to go into the specifics of this step since I don’t get detract from the focuse of this article anymore than I likely already have. Suffice to say, I use <a href="https://maartengr.github.io/BERTopic/index.html" target="\_blank">BERTopic</a> to create and assign topics to each “chunk”. This is another great and feature rich library that I’ve been using for a few years.</p>
<p>The artifact produced at the conclusion of this step is a DataFrame with topic identiferis associated to each <code>_chunk</code>. It looks like this: <img src="images/blog-20240714/raw-chunked-df.png" class="img-fluid" alt="The chunked dataset with topic identifiers"></p>
<p>That’s it for dataset #3, let’s save it</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>chunked_df.to_parquet(<span class="ss">f"</span><span class="sc">{</span>DATA_DIR<span class="sc">}</span><span class="ss">/clean/documents_sample_14k_chunked.parquet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="step-4-topic-summaries" class="level4">
<h4 class="anchored" data-anchor-id="step-4-topic-summaries">Step 4: Topic Summaries</h4>
<p>We need to evaluate the ability for the model to summarize and define action plans for related “chunks” as identified by a topic model. We’ll use the chunked dataset create above to put something together we can use to predict/evaluate the themes and action plans we assign to topics.</p>
<p>Fortunately for you, the reader, there isn’t alot of code to make this happen :)</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>topics_df <span class="op">=</span> chunked_df.copy()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> [<span class="st">"pred_theme_id"</span>, <span class="st">"pred_orig_theme_name"</span>]</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>topics_df[<span class="st">"pred_theme_id"</span>] <span class="op">=</span> topics_df[<span class="st">"pred_theme_id"</span>].astype(<span class="bu">int</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>topics_df[<span class="st">"_chunk"</span>] <span class="op">=</span> topics_df[<span class="st">"_chunk"</span>].astype(<span class="bu">str</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>summaries_df <span class="op">=</span> topics_df.groupby(by<span class="op">=</span>cols)[<span class="st">"_chunk"</span>].agg(<span class="bu">list</span>).reset_index()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That’s it for dataset #4, let’s save it</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>summaries_df.to_parquet(<span class="ss">f"</span><span class="sc">{</span>DATA_DIR<span class="sc">}</span><span class="ss">/clean/documents_sample_14k_topics.parquet"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<p>With some high quality context, we’ll move on to getting a “vibe check” for how likely what we want to build is possible by seeing what kinds of structured outputs we get when using the big dogs. In particular we’ll be running some tests with OpenAI, Anthropic, Fireworks, and Replicate to develop a good intutition of how well to expect things might work with both closed and open source models.</p>
<p>Again, I’m using <a href="https://github.com/parlance-labs/ftcourse" target="\_blank">Hamel’s ftcourse repo</a> as a general guide for building this project out so make sure to check it out as y’all start your own journeys. The topic in this blog post isn’t necessarily covered in any of his notebooks so consider this the <code>00</code> notebook that is more or less implied in the course.</p>
<p>Also, I welcome any ideas on improving anything and everything presented above. Especially if you notice any really egregious and glaring errors in my thinking or workflow, I’d defintely appreciate your thoughts. Either way, thanks for reading to the end :)</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("^(?:http:|https:)\/\/?(www)\.ohmeow\.com\/**");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="ohmeow/ohmeow_website" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2025 ohmeow.com</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><a href="mailto:wgilliam@ohmeow.com">E-mail</a> | <a href="https://x.com/waydegilliam">X</a> | <a href="https://github.com/ohmeow">GitHub</a> | <a href="https://ohmeow.com">Website</a></p>
</div>
  </div>
</footer>




</body></html>