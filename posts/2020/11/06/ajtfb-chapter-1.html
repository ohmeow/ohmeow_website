<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning | ohmeow</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning" />
<meta name="author" content="Wayde Gilliam" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The first in a weekly-ish series where I revisit the fast.ai book, ‚ÄúDeep Learning for Coders with fastai &amp; PyTorch‚Äù, and provide commentary on the bits that jumped out to me chapter by chapter. So without further adieu, let‚Äôs go!" />
<meta property="og:description" content="The first in a weekly-ish series where I revisit the fast.ai book, ‚ÄúDeep Learning for Coders with fastai &amp; PyTorch‚Äù, and provide commentary on the bits that jumped out to me chapter by chapter. So without further adieu, let‚Äôs go!" />
<link rel="canonical" href="https://ohmeow.com/posts/2020/11/06/ajtfb-chapter-1.html" />
<meta property="og:url" content="https://ohmeow.com/posts/2020/11/06/ajtfb-chapter-1.html" />
<meta property="og:site_name" content="ohmeow" />
<meta property="og:image" content="https://ohmeow.com/images/articles/fastbook.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-06T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"The first in a weekly-ish series where I revisit the fast.ai book, ‚ÄúDeep Learning for Coders with fastai &amp; PyTorch‚Äù, and provide commentary on the bits that jumped out to me chapter by chapter. So without further adieu, let‚Äôs go!","mainEntityOfPage":{"@type":"WebPage","@id":"https://ohmeow.com/posts/2020/11/06/ajtfb-chapter-1.html"},"url":"https://ohmeow.com/posts/2020/11/06/ajtfb-chapter-1.html","@type":"BlogPosting","image":"https://ohmeow.com/images/articles/fastbook.jpg","author":{"@type":"Person","name":"Wayde Gilliam"},"headline":"A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning","dateModified":"2020-11-06T00:00:00-06:00","datePublished":"2020-11-06T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ohmeow.com/feed.xml" title="ohmeow" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163296836-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-163296836-1');
</script>




<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header row">

    <div class="col"><a class="site-title" rel="author" href="/"><img style="height: 60px;" src="/images/ohmeow_logo.png" alt="ohmeow.com"></a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger"><a class="page-link" href="/about/"><span class="top-menu-text">About Me</span></a><a class="page-link" href="/guides/"><span class="top-menu-text">Guides</span></a><a class="page-link" href="/search/"><span class="top-menu-text">Search</span></a><a class="page-link" href="/categories/"><span class="top-menu-text">Tags</span></a></div>
        </nav></div>
  </header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">A Journey Through Fastbook (AJTFB) - Chapter 1: The Basics of Deep Learning</h1><p class="page-description">The first in a weekly-ish series where I revisit the fast.ai book, <a href='https://github.com/fastai/fastbook'>"Deep Learning for Coders with fastai & PyTorch"</a>, and provide commentary on the bits that jumped out to me chapter by chapter.  So without further adieu, let's go!</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-11-06T00:00:00-06:00" itemprop="datePublished">
        Nov 6, 2020
      </time>‚Ä¢ 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Wayde Gilliam</span></span>
       ‚Ä¢ <span class="read-time" title="Estimated read time">
    
    
      28 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#fastai">fastai</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#fastbook">fastbook</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/ohmeow/ohmeow_website/tree/master/_notebooks/2020-11-06-ajtfb-chapter-1.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          
          <div class="px-2">
    <a href="https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/_notebooks/2020-11-06-ajtfb-chapter-1.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#How-to-learn-Deep-Learning">How to learn Deep Learning </a>
<ul>
<li class="toc-entry toc-h3"><a href="#You-can-do-this!">You can do this! </a></li>
<li class="toc-entry toc-h3"><a href="#The-problem-with-traditional-education">The problem with traditional education </a></li>
<li class="toc-entry toc-h3"><a href="#Deep-Learning-(and-coding-in-general)-is-an-art-maybe-more-so-than-a-science">Deep Learning (and coding in general) is an art maybe more so than a science </a></li>
<li class="toc-entry toc-h3"><a href="#Doing-is-how-you-learn,-and-what-you've-done-is-what-matters">Doing is how you learn, and what you&#39;ve done is what matters </a></li>
<li class="toc-entry toc-h3"><a href="#Folks-to-follow">Folks to follow </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#What-is-machine-learning?">What is machine learning? </a>
<ul>
<li class="toc-entry toc-h3"><a href="#A-picture">A picture </a></li>
<li class="toc-entry toc-h3"><a href="#An-explanation">An explanation </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Architecture-vs.-model">Architecture vs. model </a></li>
<li class="toc-entry toc-h4"><a href="#Parameters">Parameters </a></li>
<li class="toc-entry toc-h4"><a href="#Inputs-vs.labels">Inputs vs.labels </a></li>
<li class="toc-entry toc-h4"><a href="#Loss">Loss </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Transfer-learning">Transfer learning </a>
<ul>
<li class="toc-entry toc-h3"><a href="#How-does-it-work?">How does it work? </a></li>
<li class="toc-entry toc-h3"><a href="#What-is-the-high-level-approach-in-fastai?">What is the high-level approach in fastai? </a></li>
<li class="toc-entry toc-h3"><a href="#What-do-we-have-at-the-end-of-training-(or-finetuning)?">What do we have at the end of training (or finetuning)? </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Metrics">Metrics </a>
<ul>
<li class="toc-entry toc-h3"><a href="#A-definition">A definition </a></li>
<li class="toc-entry toc-h3"><a href="#Examples">Examples </a></li>
<li class="toc-entry toc-h3"><a href="#Metrics-to-use-based-on-task">Metrics to use based on task </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Training,-validation,-and-test-datasets">Training, validation, and test datasets </a>
<ul>
<li class="toc-entry toc-h3"><a href="#What-is-a-training-set?">What is a training set? </a>
<ul>
<li class="toc-entry toc-h5"><a href="#Why-do-we-need-a-training-set?">Why do we need a training set? </a></li>
<li class="toc-entry toc-h4"><a href="#How-to-use-a-training-set?">How to use a training set? </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#What-is-a-validation-set?">What is a validation set? </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Why-do-we-need-a-validation-set?">Why do we need a validation set? </a></li>
<li class="toc-entry toc-h4"><a href="#How-to-use-a-validation-set?">How to use a validation set? </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#What-is-a-test-set?">What is a test set? </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Why-do-we-need-a-test-set?">Why do we need a test set? </a></li>
<li class="toc-entry toc-h4"><a href="#How-to-use-a-test-set?">How to use a test set? </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#How-to-create-good-validation-and-test-sets">How to create good validation and test sets </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Categorical-datatypes">Categorical datatypes </a>
<ul>
<li class="toc-entry toc-h3"><a href="#What-if-our-target-is-categorical?">What if our target is categorical? </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Multi-classification-tasks">Multi-classification tasks </a></li>
<li class="toc-entry toc-h4"><a href="#Multi-label-tasks">Multi-label tasks </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#What-if-our-input-is-categorical?">What if our input is categorical? </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Continuous-datatypes">Continuous datatypes </a>
<ul>
<li class="toc-entry toc-h3"><a href="#What-if-our-target-is-continuous?">What if our target is continuous? </a></li>
<li class="toc-entry toc-h3"><a href="#What-if-our-input-is-continuous?">What if our input is continuous? </a></li>
<li class="toc-entry toc-h3"><a href="#Normalization">Normalization </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#ResNets">ResNets </a>
<ul>
<li class="toc-entry toc-h3"><a href="#What-is-a-ResNet-&-Why-use-it-for-computer-vision-tasks?">What is a ResNet &amp; Why use it for computer vision tasks? </a>
<ul>
<li class="toc-entry toc-h4"><a href="#What-other-things-can-use-images-recognizers-for-besides-image-tasks?">What other things can use images recognizers for besides image tasks? </a></li>
<li class="toc-entry toc-h4"><a href="#How-does-it-fare-against-more-recent-architectures-like-vision-transformers?">How does it fare against more recent architectures like vision transformers? </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Best-practices">Best practices </a></li>
<li class="toc-entry toc-h3"><a href="#An-example">An example </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Step-1:-Build-our-DataLoaders">Step 1: Build our DataLoaders </a></li>
<li class="toc-entry toc-h4"><a href="#Step-2:-Build-our-cnn_learner">Step 2: Build our cnn_learner </a></li>
<li class="toc-entry toc-h4"><a href="#Step-3:-Train">Step 3: Train </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Visualizing-what-a-NN-is-learning">Visualizing what a NN is learning </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Why-is-it-important?">Why is it important? </a></li>
<li class="toc-entry toc-h3"><a href="#Computer-vision-models">Computer vision models </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Examples">Examples </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Resources">Resources </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-11-06-ajtfb-chapter-1.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Cervantes once wrote that "the journey is better than the inn", but I rather like to think that the journey <em>is</em> the inn.</p>
<p>It means that the journey, irrespective to its difficulties (and likely because of them), is what you look back on with fondness at its end rather than the end itself.  It's why I enjoy reading "The Lord of the Rings" every five years or so, where as I age and experience the hand life has dealt me, I find myself appreciating different aspects of the story from the time before and gaining new insights into what I value and want to be as a human being. I find my journey with deep learning to be roughly analgous to that.</p>
<p>I've been a part of <a href="https://forums.fast.ai/u/wgpubs/summary">the fast.ai community</a> for several years. I've been through <a href="https://course.fast.ai/">the course</a> multiple times (since it was using theano back in the old days), I've contributed to the library, and use it as the basis for one of <a href="https://ohmeow.github.io/blurr/">my own</a>.  And as with each course, with a re-reading of the book I find myself deriving new insights and appreciating different ideas than those I had before.</p>
<p>And so, while your journey may bring you different revelations, here are the meandering thoughts of one 49 year old married father of 3 living in San Diego, California, USA, as I embark upon the first chapter in what I consider "The Lord of the Rings" of deep learning.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="How-to-learn-Deep-Learning">
<a class="anchor" href="#How-to-learn-Deep-Learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to learn Deep Learning<a class="anchor-link" href="#How-to-learn-Deep-Learning"> </a>
</h2>
<h3 id="You-can-do-this!">
<a class="anchor" href="#You-can-do-this!" aria-hidden="true"><span class="octicon octicon-link"></span></a>You can do this!<a class="anchor-link" href="#You-can-do-this!"> </a>
</h3>
<blockquote>
<p>Hi, everybody; I'm Jeremy ... I do not have any formal technical education ... didn't have great grades.
I was much more interested in doing real projects.</p>
</blockquote>
<p>This is meaningful to me as someone with a BA in History and a MA in Theology. It's a reminder that if you want something, it's within your grasp to make it happen if you are willing to put in the work. It's also a reminder that key to getting there is actually doing something!  If find too many people thinking that if they just get into that school, or if they can just take that class, then they'll be a good software enginner or deep learning practitioner.  The reality is that the <em>only</em> way you get there is by doing it ... just like pull-ups (which aren't much fun when you're starting out and/or you're 49 and overweight).</p>
<h3 id="The-problem-with-traditional-education">
<a class="anchor" href="#The-problem-with-traditional-education" aria-hidden="true"><span class="octicon octicon-link"></span></a>The problem with traditional education<a class="anchor-link" href="#The-problem-with-traditional-education"> </a>
</h3>
<blockquote>
<p>... how math is taught - we require students to spend years doing rote memorization and learning dry
disconnected <em>fundatmentals</em> that we claim will pay off later, long after most of them quit the subject.</p>
</blockquote>
<p>This also is the problem with higher education in general, where young people spend at least four to five years learning things they already learned in High School or else things they don't really care about and will be forgotten right after finals, spending in excess of $100,000 for the privilege of it and likely going into debt in the tens of thousands of dollars, all with this idea that having done it they will be prepared for the real world.  Unfortunately, that's not how it works. Whether you are in a university of even go to university, what matter is what you do ... not what classes you took or what your GPA is.</p>
<h3 id="Deep-Learning-(and-coding-in-general)-is-an-art-maybe-more-so-than-a-science">
<a class="anchor" href="#Deep-Learning-(and-coding-in-general)-is-an-art-maybe-more-so-than-a-science" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep Learning (and coding in general) is an art maybe more so than a science<a class="anchor-link" href="#Deep-Learning-(and-coding-in-general)-is-an-art-maybe-more-so-than-a-science"> </a>
</h3>
<blockquote>
<p>The hardest part of deep learning is artisanal.</p>
</blockquote>
<p>I remember going to an iOS conference way back in the day and a conference speaker asking how many folks in the session I was sitting in had a background in music. 80-90% of the audience raised their hands.  Sure, there is math and stats and a science to deep learning, but like any coding enterprise, it's an art ... with some artists being better than others along with room for improvement regardless of whether you're Van Gough or painting by the numbers.</p>
<h3 id="Doing-is-how-you-learn,-and-what-you've-done-is-what-matters">
<a class="anchor" href="#Doing-is-how-you-learn,-and-what-you've-done-is-what-matters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Doing is how you learn, and what you've done is what matters<a class="anchor-link" href="#Doing-is-how-you-learn,-and-what-you've-done-is-what-matters"> </a>
</h3>
<blockquote>
<p>... focus on your hobbies and passions ... Common character traits in the people who do well at deep learning include playfulness and curiosity.</p>
<p>at Tesla .. CEO Elon Musk says 'A PhD is definitely not required. All that matters is a deep understanding of AI &amp; ability to implement NNs in a way that is actually useful .... Don't care if you even graduated High School.'</p>
<p>... the most important thing for learning deep learning is writing code and experimenting."</p>
</blockquote>
<h3 id="Folks-to-follow">
<a class="anchor" href="#Folks-to-follow" aria-hidden="true"><span class="octicon octicon-link"></span></a>Folks to follow<a class="anchor-link" href="#Folks-to-follow"> </a>
</h3>
<p>It's always helpful to have some role models; folks who practice the lessons learned above and can help you along your journey.</p>
<p>For starters, consider this image of the top 12 users based on most likes in the fast.ai forums:<img src="https://github.com/ohmeow/ohmeow_website/blob/master/images/articles/20211102-fastai-forums-top-12.png?raw=1" alt="">
Aside from the founders of <a href="https://www.fast.ai/">fast.ai</a> and a bunch of them working for noteable ML companies like <a href="https://huggingface.co/">Hugging Face</a> and <a href="https://wandb.ai/site">Weights &amp; Biases</a>, I can think of at least <strong><em>FOUR</em></strong> things these folks have in common:</p>
<ol>
<li>
<p>They are <strong>fearless in asking what they may have even considered, dumb questions</strong>.</p>
</li>
<li>
<p>They are <strong>active in researching the answers to their own questions</strong> (even the dumb ones) and those asked by others.</p>
</li>
<li>
<p>They are <strong>active in teaching</strong> others through blogs, books, open source libraries, study groups, and podcasts.</p>
</li>
<li>
<p><strong>They build</strong> things! That is, they all have experience building models and making them usable via deployed applications and/or in kaggle compeititions.  Anyone can bake a half-cooked model in a Jupyter notebook, but few can turn it into something others can use.</p>
</li>
</ol>
<p>These traits aren't just key to learning deep learning; they are key to learning anything!  Practice them and you guarantee yourself success in learning anything you've set your mind on.</p>
<p>If you had to choose just three ...</p>
<p>Aside from Jeremy (<a href="https://twitter.com/jeremyphoward">@jeremyphoward</a>), who's a given, if I could only follow three people who have mastered to art of learning deep learning, they would be ...</p>
<p><strong>Radek Osmulsk</strong>: (twitter: <a href="https://twitter.com/radekosmulski">@radekosmulski</a>)

</p>
<center>
    <div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">If you found this of value, you might be interested in a book on learning deep learning that I wrote<br><br>check it out here &gt;&gt;&gt; <a href="https://t.co/ApKlm8BRmy">https://t.co/ApKlm8BRmy</a></p>‚Äî Radek Osmulski (@radekosmulski) <a href="https://twitter.com/radekosmulski/status/1455527697661169664?ref_src=twsrc%5Etfw">November 2, 2021</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</center>

<p><strong>Zach Mueller</strong>: (twitter: <a href="https://twitter.com/TheZachMueller">@TheZachMueller</a>)

</p>
<center>
    <div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">To me, I think it boiled down to how I learned. I took those two courses essentially over the course of a year or so. Approaching each lesson slowly, and letting myself wander in the related concepts, learning as much as I could through online communities.</p>‚Äî Zach Mueller (@TheZachMueller) <a href="https://twitter.com/TheZachMueller/status/1451941577841127433?ref_src=twsrc%5Etfw">October 23, 2021</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</center>

<p><strong>Sanyam Bhutani</strong>: (twitter: <a href="https://twitter.com/bhutanisanyam1">@bhutanisanyam1</a>)

</p>
<center>
    <div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">The <a href="https://twitter.com/PyTorch?ref_src=twsrc%5Etfw">@PyTorch</a> book reading group <a href="https://twitter.com/weights_biases?ref_src=twsrc%5Etfw">@weights_biases</a> comes to an endüôè<br><br>We had an incredible 10 weeks of learning!<br><br>As a group wanted to extend our gratitude to the incredible authors: Eli, <a href="https://twitter.com/lantiga?ref_src=twsrc%5Etfw">@lantiga</a> &amp; <a href="https://twitter.com/ThomasViehmann?ref_src=twsrc%5Etfw">@ThomasViehmann</a> <br><br>A few words from our community:<a href="https://t.co/3ODz6J1vad">https://t.co/3ODz6J1vad</a></p>‚Äî Sanyam Bhutani (@bhutanisanyam1) <a href="https://twitter.com/bhutanisanyam1/status/1452599997493481472?ref_src=twsrc%5Etfw">October 25, 2021</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</center>

<p>Personally, I <strong>do</strong> follow each of these individuals on twitter and you should too! Though I've never met any of them IRL, I consider the colleagues, friends, and amongst the most helpful for those looking to get started in machine learning.
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>Twitter is imo the best place to network with fellow ML/DL practioners and stay up-to-date with the latest developments in ML in general
</div>
<p>Here's a tl;dr for folks too lazy to read the above ...</p>
<p>youtube: <a href="https://youtu.be/_QUEXsHfsA0?t=935">https://youtu.be/_QUEXsHfsA0?t=935</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="What-is-machine-learning?">
<a class="anchor" href="#What-is-machine-learning?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is machine learning?<a class="anchor-link" href="#What-is-machine-learning?"> </a>
</h2>
<p>Here we look at machine learning in general (of which deep learning is a subset) as well as the process of finetuning a pretrained ML model.  When you think of deep learning ... think neural networks.</p>
<p><img src="https://raw.githubusercontent.com/ohmeow/ohmeow_website/master/images/articles/what-is-ai-ml-dl.jpg" alt=""></p>
<h3 id="A-picture">
<a class="anchor" href="#A-picture" aria-hidden="true"><span class="octicon octicon-link"></span></a>A picture<a class="anchor-link" href="#A-picture"> </a>
</h3>
<p><img src="https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/images/ajtfb-ch-1-deep_learning_overview.png?raw=1" alt="" title="Credit: Fastbook p.25"></p>
<h3 id="An-explanation">
<a class="anchor" href="#An-explanation" aria-hidden="true"><span class="octicon octicon-link"></span></a>An explanation<a class="anchor-link" href="#An-explanation"> </a>
</h3>
<blockquote>
<p>"Suppowe we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignemnt so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would 'learn' from its experince" - Arthur Samuel</p>
</blockquote>
<h4 id="Architecture-vs.-model">
<a class="anchor" href="#Architecture-vs.-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Architecture vs. model<a class="anchor-link" href="#Architecture-vs.-model"> </a>
</h4>
<blockquote>
<p>... a <strong><em>model</em></strong> is a special kind of program:it's one that can do <em>many different things</em>, depending &gt; on the <strong>weights</strong>.</p>
<p>The functional form of the <em>model</em> is called its <strong><em>architecture</em></strong>.</p>
<p>Note:The <strong>architecture</strong> is "the <em>template</em> of the model that we're trying to fit; i.e., the actual mathematical function that we're passing the input data and parameters to" ... whereas the <strong>model</strong> is a particular set of parameters + the architecture.</p>
<h4 id="Parameters">
<a class="anchor" href="#Parameters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parameters<a class="anchor-link" href="#Parameters"> </a>
</h4>
<p><strong>Weights</strong> are just variables, and a <strong>weight assignment</strong> is a particuarl choice of values for those 
variables. [Weights] are generally referred to as model <strong><em>parameters</em></strong> ... the term <em>weights</em> being
reserved for a particular type of model parameter.</p>
<p>The <em>weights</em> are called <strong><em>parameters</em></strong>.</p>
<p>Note:These parameters are the things that are "learnt"; the values that can be updated, whereas <strong>activations</strong> in a neural network are simply numbers as the result of some calculation.</p>
<h4 id="Inputs-vs.labels">
<a class="anchor" href="#Inputs-vs.labels" aria-hidden="true"><span class="octicon octicon-link"></span></a>Inputs vs.labels<a class="anchor-link" href="#Inputs-vs.labels"> </a>
</h4>
</blockquote>
<p>The <strong><em>inputs</em></strong>, also known as your <strong><em>independent variable(s)</em></strong> [your <code>X</code>] is what your model uses to make <strong><em>predictions</em></strong>.</p>
<p>The <strong><em>labels</em></strong>, also known as your <strong><em>dependent variable(s)</em></strong> [your <code>y</code>] represent the correct target value for your task.</p>
<h4 id="Loss">
<a class="anchor" href="#Loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loss<a class="anchor-link" href="#Loss"> </a>
</h4>
<blockquote>
<p>The [model's] measure of performance is called the <strong><em>loss</em></strong> ... [the value of which depends on how well your model is able to predict] the correct <strong><em>labels</em></strong>.</p>
</blockquote>
<p>The <strong><em>loss</em></strong> is a measure of model performance that SGD can use to make your model better. A good loss function provides good gradients (slopes) that can be used to make even very minor changes to your weights so as to improve things. Visually, you want gentle rolling hills rather than abrupt steps or jagged peaks.</p>
<blockquote>
<p>Note:You can think of the <strong>loss</strong> as the model's <strong>metric</strong>, that is, how it both understands how good it is and can help it improve.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Transfer-learning">
<a class="anchor" href="#Transfer-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transfer learning<a class="anchor-link" href="#Transfer-learning"> </a>
</h2>
<p><strong><em>Transfer learning</em></strong> is the process of taking a <strong>"pretrained model"</strong> that has been trained on a very large dataset with proven SOTA results, and <strong>"fine tuning"</strong> it for your specific task, which while likely similar to the task the pretrained model was trained for to one degree or another, is not the necesarily the same.</p>
<h3 id="How-does-it-work?">
<a class="anchor" href="#How-does-it-work?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How does it work?<a class="anchor-link" href="#How-does-it-work?"> </a>
</h3>
<ol>
<li>The <strong>head</strong> of your model (the newly added part specific to your dataset/task) should be trained first since it is the only one with completely random weights. </li>
<li>The degree to which your weights of the pretrained model will need to be updated is proportional to how similar your data is to the data it was trained on. The more dissimilar, the more the weights will need to be changed.</li>
<li>Your model will only be as good as the data it was trained on, so make sure what you have is representative of what it will see in the real world. It "can learn to operate on only the patterns seen in the input data used to train it."</li>
</ol>
<blockquote>
<p>The process of <em>training</em> (or <em>fitting</em>) the model is the process of finding a set of <em>parameter values</em> 
(or <em>weights</em>) that specialize that general architecture into a model that works well for our 
particular kind of data [and task]</p>
</blockquote>
<h3 id="What-is-the-high-level-approach-in-fastai?">
<a class="anchor" href="#What-is-the-high-level-approach-in-fastai?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is the high-level approach in fastai?<a class="anchor-link" href="#What-is-the-high-level-approach-in-fastai?"> </a>
</h3>
<p>fastai provides a <code>fine_tune</code> method that uses proven tricks and hyperparameters for various DL tasks that the author's have found works well most of the time.</p>
<h3 id="What-do-we-have-at-the-end-of-training-(or-finetuning)?">
<a class="anchor" href="#What-do-we-have-at-the-end-of-training-(or-finetuning)?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What do we have at the end of training (or finetuning)?<a class="anchor-link" href="#What-do-we-have-at-the-end-of-training-(or-finetuning)?"> </a>
</h3>
<blockquote>
<p>... once the model is trained - that is, once we've chosen our final weight assignments - then we can think of the weights as being <em>part of the model</em> since we're not varying them anymore.</p>
</blockquote>
<p>This means a trained model can be treated like a typical function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Metrics">
<a class="anchor" href="#Metrics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Metrics<a class="anchor-link" href="#Metrics"> </a>
</h2>
<h3 id="A-definition">
<a class="anchor" href="#A-definition" aria-hidden="true"><span class="octicon octicon-link"></span></a>A definition<a class="anchor-link" href="#A-definition"> </a>
</h3>
<p><strong>Metrics</strong> are a human-understandable measures of model quality whereas the <strong>loss</strong> is the machine's.  They are based on your validation set and are what you really care about, whereas the loss is "a measure of performance" that the training system can use to update weights automatically.</p>
<p>A good choice for loss is a function "that is easy for <strong><em>stochastic gradient descent (SGD)</em></strong> to use, whereas a good choies for your metrics are functions that your business users will care about. Seldom are they the same because most metrics don't provide smooth gradients that SGD can use to update your model's weights.
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>Again, they are based on your validation/test sets (not your training set). Ultimately, we want to have a model that generalizes well to inputs it was <em>not</em> trained on, and this is what our validation/test sets represent. This is how we relay our model quality.
</div>
<h3 id="Examples">
<a class="anchor" href="#Examples" aria-hidden="true"><span class="octicon octicon-link"></span></a>Examples<a class="anchor-link" href="#Examples"> </a>
</h3>
<p>There are a whole list of metrics built into the fastai library, <a href="https://docs.fast.ai/metrics.html">see here</a>. Below I begin a listing of the most common ones as they come up in the fastbook (and from personal experience).</p>
<p><strong>error rate</strong> = "the proportion of images that were incorrectly identified."</p>
<p><strong>accuracy</strong> = the proportation of images that were correctly identified (<code>1 - error rate</code>)</p>
<h3 id="Metrics-to-use-based-on-task">
<a class="anchor" href="#Metrics-to-use-based-on-task" aria-hidden="true"><span class="octicon octicon-link"></span></a>Metrics to use based on task<a class="anchor-link" href="#Metrics-to-use-based-on-task"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>*</code> indicates that other metrics may be better for the given task.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Training,-validation,-and-test-datasets">
<a class="anchor" href="#Training,-validation,-and-test-datasets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training, validation, and test datasets<a class="anchor-link" href="#Training,-validation,-and-test-datasets"> </a>
</h2>
<h3 id="What-is-a-training-set?">
<a class="anchor" href="#What-is-a-training-set?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is a training set?<a class="anchor-link" href="#What-is-a-training-set?"> </a>
</h3>
<p>A <strong><em>training set</em></strong> consits of the data your model sees during training. These are the inputs and labels your model will use to determine the loss and update it's parameters in a way that will hopefully lead to a model that works well for its given task.</p>
<h5 id="Why-do-we-need-a-training-set?">
<a class="anchor" href="#Why-do-we-need-a-training-set?" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why do we need a training set?<a class="anchor-link" href="#Why-do-we-need-a-training-set?"> </a>
</h5>
<p>Because a model needs something to train on. It should be representative of the data the model will see in the future, and it should be updated if/when you discover that is not the case.</p>
<h4 id="How-to-use-a-training-set?">
<a class="anchor" href="#How-to-use-a-training-set?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to use a training set?<a class="anchor-link" href="#How-to-use-a-training-set?"> </a>
</h4>
<ol>
<li>
<p>To train a model on examples resembling that which the model will seen in the future. More is generally better, but quality is king (e.g., bad data in, bad data out).</p>
</li>
<li>
<p>To provide augmented examples for your model to see so as to increase the number of examples and better reflect what the model may see in the real world.</p>
</li>
</ol>
<h3 id="What-is-a-validation-set?">
<a class="anchor" href="#What-is-a-validation-set?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is a validation set?<a class="anchor-link" href="#What-is-a-validation-set?"> </a>
</h3>
<p>A <strong><em>validation set</em></strong> (also know as the "development set") does not include any data from the <strong><em>training set</em></strong>.  It's purpose to is gauge the generalization prowess of your model and also ensure you are neight overfitting or underfitting.</p>
<blockquote>
<p>"If [the model] makes an accurate prediction for a data item, that should be because it has learned characteristics of that kind of item, and not because the model has been shaped by <em>actually having seen that particular item</em>."</p>
</blockquote>
<h4 id="Why-do-we-need-a-validation-set?">
<a class="anchor" href="#Why-do-we-need-a-validation-set?" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why do we need a validation set?<a class="anchor-link" href="#Why-do-we-need-a-validation-set?"> </a>
</h4>
<blockquote>
<p>"[because] what we care about is how well our model works on <em>previously unseen images</em> ... the longer
you train for, the better your accuracy will get on the training set ... as the model starts to memorize
the training set rather than finding generalizable underlying patterns in the data = <strong>overfitting</strong>"</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/fastai/fastbook/41a60e44d588139a03452f1907359fc2322f8d5f/images/att_00000.png" alt="">
<strong><em>Overfitting</em></strong> happens when the model "remembers specific features of the input data, rather than generalizing well to data not seen during training." 
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong><strong>ALWAYS</strong> overfit before anything else.  It is your training loss gets better while your validation loss gets worse ... in other words, if you‚Äôre validation loss is improving, even if not to the extent of your training loss, you are <em>not</em> overfitting
</div>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong><strong>ALWAYS</strong> include a validation set.
</div>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong><strong>ALWAYS</strong> use the validation set to measure your accuracy (or any metrics).
</div>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong><strong>ALWAYS</strong> set the <code>seed</code> parameter so that you "get the same validation set every time" so that "if we change our model and retrain it, we know any differences are due to the changes to the model, not due to having a different random validation set."
</div>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>For a good discussion of how to achieve predictable randomness, see <a href="https://forums.fast.ai/t/lesson1-reproducible-results-setting-seed-not-working/37921/5">this discussion</a> on the fast.ai forums. There are actually several seeds you need to set and in several places when using fast.ai to achieve reproducibility.
</div>
<h4 id="How-to-use-a-validation-set?">
<a class="anchor" href="#How-to-use-a-validation-set?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to use a validation set?<a class="anchor-link" href="#How-to-use-a-validation-set?"> </a>
</h4>
<ol>
<li>
<p>It gives us a sense of how well our model is doing on examples <em>it hasn't seen</em>, which makes sense since the ultimate worth of a model is in how well it generalizes to things unseen in the future.</p>
</li>
<li>
<p>The validation set also informs us how we may change the  <strong><em>hyperparamters</em></strong> (e.g., model architecture, learning rates, data augmentation, etc...) to improve results.  These parameters are NOT learned ... they are choices WE make that affect the learning of the model parameters.</p>
</li>
</ol>
<h3 id="What-is-a-test-set?">
<a class="anchor" href="#What-is-a-test-set?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is a test set?<a class="anchor-link" href="#What-is-a-test-set?"> </a>
</h3>
<p>A <strong><em>test set</em></strong> ensures that we aren't overfitting our hyperparameter choices; it is held back even from ourselves and used to evaulate the model at the very end.</p>
<blockquote>
<p>"[Since we] are evaluating the model by looking at predictions on the validation data when we decide to explore new hyperparameter values ... subsequent version of the model are, indirectly, shaped by us having seen the validation data ... [and therefore], we are in danger of overfitting the validation data through human trial and error and exploration."</p>
<p>Note:A key property of the validation and test sets is that they must be representative of the new data you will see in the future.</p>
</blockquote>
<h4 id="Why-do-we-need-a-test-set?">
<a class="anchor" href="#Why-do-we-need-a-test-set?" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why do we need a test set?<a class="anchor-link" href="#Why-do-we-need-a-test-set?"> </a>
</h4>
<p>To ensure we aren't inadvertently causing the model to overfit via our hyperparameter tuning which happens as a result of us looking at the validation set. It is a completely hidden dataset; it isn't used for training or tuning, only for measuring performance.</p>
<h4 id="How-to-use-a-test-set?">
<a class="anchor" href="#How-to-use-a-test-set?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to use a test set?<a class="anchor-link" href="#How-to-use-a-test-set?"> </a>
</h4>
<ol>
<li>
<p>If evaluating 3rd party solutions. You'll want to know how to create a good test set and how to create a good baseline model.  Hold these out from the potential consultants and use them to fairly evaluate their work.</p>
</li>
<li>
<p>To ensure you aren't overfitting your model as a result of validation set examination. As with the validation set, a good test set offers further assurance your model isn't learning particular ancillary features of particular things in your images.</p>
</li>
</ol>
<h3 id="How-to-create-good-validation-and-test-sets">
<a class="anchor" href="#How-to-create-good-validation-and-test-sets" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to create good validation and test sets<a class="anchor-link" href="#How-to-create-good-validation-and-test-sets"> </a>
</h3>
<p>It isn't always as easy as randomly shuffling your data!</p>
<p>Again, what both of these sets should haven in common is that they "must be representative of the new data you will see in the future."  And what this looks like often dependes on your use case and task. 
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>You really need to think about what you need to predict and what you‚Äôd look at to make that prediction. You also need to make sure your training data is qualitatively different enough from your real world data (e.g., what the validation and test sets represent) as to learn patterns and not specific examples.
</div>
<p><strong>First</strong>, consider cases where historical data is required to predict the future, for example of quant traders use "<em>backtesting</em> to check whether their models are predictive of future periods, based on past data"
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>"For a <strong>time series</strong> ... (where you are using historical data to build a model for use in the future ... you will want to choose a continuous section with the latest dates as your validation set" 
</div>
<p>"A <strong>second</strong> common case occurs when you can easily anticipate ways the data you will be making predictions for in production may be <em>qualitatively different</em> from the data you have to train your model with."</p>
<p>As an example of this, <a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection">the Kaggle distracted driver competition</a> is used. In it, based on pictures of drivers you need to predict categories of distraction. Since the goal of such a model would be to make good predictions against <strong><em>drivers the model hasn't seen</em></strong>, it would make sense to create a validation and also a test set consiting of specific drivers the training set doesn't include (in fact, the competition's test set is exactly that!). "If you used all the people in training your model, your model might be overfitting to the paricipants of those specific people and not just learning the states (texting, eating, etc.)."</p>
<p>Another example of this is <a href="https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring">the Kaggle fisheries competition</a> where the objective is to predict the species of fish caught on fishing boats. As the goal of such a model is to predict the species on other/future boats, it makes sense then that "the test set consisted of images from boats that didn't appear in the training data, so in this case you'd want your validation set to also include boats that are not in the training set." 
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>Start with training a model and let the results guide your EDA!
</div>
<p>For a stellar example of how this looks in practice, <a href="https://twitter.com/borisdayma/status/1447939363296489473">see this thread from Boris Dayma</a> on an issue he noticed when looking at his results on the training and validation sets.  <strong><em>Note how his EDA was directed via training a model</em></strong> ... and also make sure to read through all the comments, replies, etc... for other things to pay attention too when seeing unusual results during training (there is a lot of good stuff there). Ultimately, in his case, what he found out was that the dataset was imbalanced and the imbalanced data was getting lumped together in the same batches due to poor shuffling strategy.  He documents <a href="https://twitter.com/borisdayma/status/1448355381374242816">his fix in a subsequent thread</a> so check that out too.
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>Knowing how to read your training/validation results drives EDA and will lead to better train/validation/test splits.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Categorical-datatypes">
<a class="anchor" href="#Categorical-datatypes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Categorical datatypes<a class="anchor-link" href="#Categorical-datatypes"> </a>
</h2>
<p><strong><em>Categorical</em></strong> data "contains values that are one of a discrete set of choice" such as gender, occupation, day of week, etc...</p>
<h3 id="What-if-our-target-is-categorical?">
<a class="anchor" href="#What-if-our-target-is-categorical?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What if our <strong>target</strong> is categorical?<a class="anchor-link" href="#What-if-our-target-is-categorical?"> </a>
</h3>
<p>If your target/lables are categorical, then you have either a <strong>multi-classification classification</strong> problem (e.g., you are trying to predict a single class) or a <strong>multi-label classification problem</strong> (e.g., you are trying to predict whether your example belongs to zero or multiple classes).</p>
<h4 id="Multi-classification-tasks">
<a class="anchor" href="#Multi-classification-tasks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multi-classification tasks<a class="anchor-link" href="#Multi-classification-tasks"> </a>
</h4>
<p>For multi-classification tasks, a sensible loss function would be <a href="https://ohmeow.com/posts/2020/04/04/understanding-cross-entropy-loss.html">cross entropy loss</a> (<code>nn.CrossEntropyLoss</code>) and <a href="https://ohmeow.com/what-is/a-metric#Metrics-to-use-based-on-task">useful metrics</a> are likely to include error rate, accuracy, F1, recall, and/or precision depending on your business objectices and the make up of your dataset.  For example, if you're dealing with a highly imbalanced dataset, choosing accuracy would lead to an inflated sense of model performance since it may be learning to just predict the most common class.
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>What if you need to predict "None"? This is more real world and covered nicely in Zach Mueller‚Äôs <a href="https://walkwithfastai.com/Unknown_Labels">Recognizing Unknown Images (or the Unknown Label problem)</a>.
</div>
<h4 id="Multi-label-tasks">
<a class="anchor" href="#Multi-label-tasks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multi-label tasks<a class="anchor-link" href="#Multi-label-tasks"> </a>
</h4>
<p>For multi-label tasks, a sensible loss function would be binary cross entropy loss (BCE) (<code>nn.BCEWithLogitsLoss</code>) and useful metrics are likely to include F1, recall, and/or precision depending on your business objectices and the make up of your dataset. Notice that I didn't include error rate, or its opposite accuracy, as their datasets are generally highly imbalanced.</p>
<h3 id="What-if-our-input-is-categorical?">
<a class="anchor" href="#What-if-our-input-is-categorical?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What if our <strong>input</strong> is categorical?<a class="anchor-link" href="#What-if-our-input-is-categorical?"> </a>
</h3>
<p>Categorical inputs are generally represented by an <strong>embedding</strong> (e.g., a vector of numbers). <strong><em>Why?</em></strong> Mostly because it gives your model the ability to provide a more complex representation of your category than a single numer would.</p>
<p>For example, imagine that one of your inputs is day of week (e.g., Sunday, Monday, etc.) ... what does that mean? When combined with other inputs, its likely that the meaning of it is going to be much more nuanced than a single number can represent, and so we'd like to use multiple learned numbers.  This is what an embedding is.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Continuous-datatypes">
<a class="anchor" href="#Continuous-datatypes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Continuous datatypes<a class="anchor-link" href="#Continuous-datatypes"> </a>
</h2>
<p><strong><em>Continuous</em></strong> data is numerical that represents a quantity such as age, salary, prices, etc...</p>
<h3 id="What-if-our-target-is-continuous?">
<a class="anchor" href="#What-if-our-target-is-continuous?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What if our <strong>target</strong> is continuous?<a class="anchor-link" href="#What-if-our-target-is-continuous?"> </a>
</h3>
<p>If your target/labels are continuous, then you have a regression problem and the most likely loss function you would choose would be mean-square-error loss (MSE) (<code>nn.MSELoss</code>)  and your metric MSE as well</p>
<blockquote>
<p>"... MSE is already a a useful metric for this task (although its' probably more interpretable after we take the square root)" ... the <strong>RMSE</strong> (% fn 3 %}</p>
<p>Note:For tasks that predict a continuous number, consider using <code>y_range</code> to constrain the network to predicting a value in the known range of valid values.</p>
</blockquote>
<h3 id="What-if-our-input-is-continuous?">
<a class="anchor" href="#What-if-our-input-is-continuous?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What if our <strong>input</strong> is continuous?<a class="anchor-link" href="#What-if-our-input-is-continuous?"> </a>
</h3>
<p>In many cases there isn't anything special you need to do, in others, it makes sense to scale these numbers so they are in the same range (usually 0 to 1) as the rest of your continuous inputs.  This process is called <strong>normalization</strong>. The reason you would want to do this is so continuous values with bigger range of values (say 1000) don't drown out those with a smaller range (say 5) during model training.</p>
<h3 id="Normalization">
<a class="anchor" href="#Normalization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Normalization<a class="anchor-link" href="#Normalization"> </a>
</h3>
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>"When training a model, if helps if your input data is <em>normalizaed</em> - that is, has a mean of 0 and a standard deviation of 1.
</div>
<p>See <a href="https://towardsdatascience.com/how-to-calculate-the-mean-and-standard-deviation-normalizing-datasets-in-pytorch-704bd7d05f4c">How To Calculate the Mean and Standard Deviation ‚Äî Normalizing Datasets in Pytorch</a></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Example 1'</span><span class="p">)</span>
<span class="n">nums</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Some raw values: </span><span class="si">{</span><span class="n">nums</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># 1. calculate their mean and standard deviation</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nums</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">nums</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Their mean is </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s1"> and their standard deviation is </span><span class="si">{</span><span class="n">std</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># 2. normalize their values </span>
<span class="n">normalized</span> <span class="o">=</span> <span class="p">(</span><span class="n">nums</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Here are their values after normalization: </span><span class="si">{</span><span class="n">normalized</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Example 2'</span><span class="p">)</span>
<span class="n">nums</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Some raw values: </span><span class="si">{</span><span class="n">nums</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># 1. calculate their mean and standard deviation</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nums</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">nums</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Their mean is </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s1"> and their standard deviation is </span><span class="si">{</span><span class="n">std</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># 2. normalize their values </span>
<span class="n">normalized</span> <span class="o">=</span> <span class="p">(</span><span class="n">nums</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Here are their values after normalization: </span><span class="si">{</span><span class="n">normalized</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>fastai supplies a <code>Normalize</code> transform you can use to do this ... "it acts on a whole mini-batch at once, so you can add it to the <code>batch_tfms</code> secion of your data block ... you need to pass to this transform the mean and standard deviation that you want to use. If you don't, "fastai will automatically calculate them from a single batch of your data). p.241
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>"This means that when you distribute a model, you need to also distribute the statistics used for normalization." (p.242) 
</div>
<div class="flash flash-warn">
    <svg class="octicon octicon-zap octicon octicon-zap" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z"></path></svg>
    <strong>Important: </strong>"... if you‚Äôre using a model that someon else has trained, make sure you find out what normalization statistics they used an match them" (p.242)
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="ResNets">
<a class="anchor" href="#ResNets" aria-hidden="true"><span class="octicon octicon-link"></span></a>ResNets<a class="anchor-link" href="#ResNets"> </a>
</h2>
<h3 id="What-is-a-ResNet-&amp;-Why-use-it-for-computer-vision-tasks?">
<a class="anchor" href="#What-is-a-ResNet-&amp;-Why-use-it-for-computer-vision-tasks?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is a ResNet &amp; Why use it for computer vision tasks?<a class="anchor-link" href="#What-is-a-ResNet-&amp;-Why-use-it-for-computer-vision-tasks?"> </a>
</h3>
<p>A <strong>ResNet</strong> is a model architecture that has proven to work well in CV tasks. Several variants exist with different numbers of layers with the larger architectures taking longer to train and more prone to overfitting especially with smaller datasets.</p>
<p>The number represents the number of layers in this particular ResNet variant ... "(other options are 18, 50, 101, and 152) ... model architectures with more layers take longer to train and are more prone to overfitting ... on the other hand, when using more data, they can be qite a bit more accurate."</p>
<h4 id="What-other-things-can-use-images-recognizers-for-besides-image-tasks?">
<a class="anchor" href="#What-other-things-can-use-images-recognizers-for-besides-image-tasks?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What other things can use images recognizers for besides image tasks?<a class="anchor-link" href="#What-other-things-can-use-images-recognizers-for-besides-image-tasks?"> </a>
</h4>
<p>Sound, time series, malware classification ... "a good rule of thumb for converting a dataset into an image representation: if the human eye can recognize categories from the images, then a deep learning model should be able to do so too."</p>
<h4 id="How-does-it-fare-against-more-recent-architectures-like-vision-transformers?">
<a class="anchor" href="#How-does-it-fare-against-more-recent-architectures-like-vision-transformers?" aria-hidden="true"><span class="octicon octicon-link"></span></a>How does it fare against more recent architectures like vision transformers?<a class="anchor-link" href="#How-does-it-fare-against-more-recent-architectures-like-vision-transformers?"> </a>
</h4>
<p>Pretty well apparently (at least at the time this post was written) ...

</p>
<center>
    <div class="jekyll-twitter-plugin">
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">I'm pleased to announce that the 'ResNet strikes back' paper is now on arxiv! Moving the baseline forward to 80.4% top-1 for a vanilla ResNet-50 arch w/ better training recipes. No extra data, no distillation. <a href="https://t.co/WP3UDXfV0r">https://t.co/WP3UDXfV0r</a></p>‚Äî Ross Wightman (@wightmanr) <a href="https://twitter.com/wightmanr/status/1444852719773122565?ref_src=twsrc%5Etfw">October 4, 2021</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</center>

<h3 id="Best-practices">
<a class="anchor" href="#Best-practices" aria-hidden="true"><span class="octicon octicon-link"></span></a>Best practices<a class="anchor-link" href="#Best-practices"> </a>
</h3>
<p></p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>Start with a smaller ResNet (like 18 or 34) and move up as needed.
</div>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>If you have a lot of data, the bigger resnets will likely give you better results.
</div>
<h3 id="An-example">
<a class="anchor" href="#An-example" aria-hidden="true"><span class="octicon octicon-link"></span></a>An example<a class="anchor-link" href="#An-example"> </a>
</h3>
<h4 id="Step-1:-Build-our-DataLoaders">
<a class="anchor" href="#Step-1:-Build-our-DataLoaders" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 1: Build our DataLoaders<a class="anchor-link" href="#Step-1:-Build-our-DataLoaders"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">PETS</span><span class="p">)</span><span class="o">/</span><span class="s1">'images'</span>

<span class="k">def</span> <span class="nf">is_cat</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
  <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">isupper</span><span class="p">()</span>                  <span class="c1"># if filename is Capitalized, its a cat image</span>

<span class="n">dls</span> <span class="o">=</span> <span class="n">ImageDataLoaders</span><span class="o">.</span><span class="n">from_name_func</span><span class="p">(</span>
    <span class="n">path</span><span class="p">,</span>                                <span class="c1"># where our image are</span>
    <span class="n">get_image_files</span><span class="p">(</span><span class="n">path</span><span class="p">),</span>               <span class="c1"># how to build our inputs (our x)</span>
    <span class="n">label_func</span><span class="o">=</span><span class="n">is_cat</span><span class="p">,</span>                   <span class="c1"># how to build our labels (our y)</span>
    <span class="n">valid_pct</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>                       <span class="c1"># how to build our validation set</span>
    <span class="n">item_tfms</span><span class="o">=</span><span class="n">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>               <span class="c1"># things we want to do to each image when we fetch it</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">42</span>                              <span class="c1"># for reproducibility</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Why do we make images 224x224 pixels?</strong></p>
<p>"This is the standard size for historical reasons (old pretrained models require this size exactly) ... If you increase the size, you'll often get a model with better results since it will be able to focus on more details."
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>Train on progressively larger image sizes using the weights trained on smaller sizes as a kind of pretrained model.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Step-2:-Build-our-cnn_learner">
<a class="anchor" href="#Step-2:-Build-our-cnn_learner" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 2: Build our <code>cnn_learner</code><a class="anchor-link" href="#Step-2:-Build-our-cnn_learner"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">error_rate</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Step-3:-Train">
<a class="anchor" href="#Step-3:-Train" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 3: Train<a class="anchor-link" href="#Step-3:-Train"> </a>
</h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fine_tune</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For more information on how transfer learning works, and the <code>fine_tune</code> method in particuarl, see this section in my <a href="https://ohmeow.com/what-is/machine-learning#Transfer-learning">"What is machine learning" post</a>.</p>
<p>For more metrics like <code>error_rate</code>, see my <a href="https://ohmeow.com/what-is/a-metric">"What is a metric" post</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Visualizing-what-a-NN-is-learning">
<a class="anchor" href="#Visualizing-what-a-NN-is-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualizing what a NN is learning<a class="anchor-link" href="#Visualizing-what-a-NN-is-learning"> </a>
</h2>
<h3 id="Why-is-it-important?">
<a class="anchor" href="#Why-is-it-important?" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why is it important?<a class="anchor-link" href="#Why-is-it-important?"> </a>
</h3>
<p>Because it allows you to know both what your NN is doing/learning and whether it is learning anything at all. The former is helpful because it gives you confidence that your model is learning to look at the right information and insights on how to improve it, the later because a model that isn't learning anything (e.g., able to update its parameters so as to improve itself) isn't a helpful or useful model.
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>Learn how to visualize and understand your activations and gradients
</div>
<h3 id="Computer-vision-models">
<a class="anchor" href="#Computer-vision-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Computer vision models<a class="anchor-link" href="#Computer-vision-models"> </a>
</h3>
<p>The top of this image is a visualization of the weights (what the model is learning), and the one below is a visualization of the activations, in particular, the parts of training images that most strongly match each set of weights above.</p>
<p><img src="https://github.com/fastai/fastbook/raw/e57e3155824c81a54f915edf9505f64d5ccdad84/images/layer1.png" alt="">
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>This kind of visualization is particularly helpful in transfer learning as it allows us to infer which layers may require more or less training for our task. For example, the layer above probably requires little to no training as it looks to be identifying edges and gradients, thing likely helpful and necessary for all computer vision tasks.
</div>
<h4 id="Examples">
<a class="anchor" href="#Examples" aria-hidden="true"><span class="octicon octicon-link"></span></a>Examples<a class="anchor-link" href="#Examples"> </a>
</h4>
<p><strong>Vectors into 2D grayscale images (MNIST)</strong></p>
<p>*Courtesy of Abishek Thakur's, "Approaching (almost) any Machine Learning Problem"</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_openml</span><span class="p">(</span><span class="s1">'mnist_784'</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">targets</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># always helpful to see the shape of things</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># see https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Vectors as clusters  (MNIST)</strong></p>
<p>*Courtesy of Abishek Thakur's, "Approaching (almost) any Machine Learning Problem"</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tsne</span> <span class="o">=</span> <span class="n">manifold</span><span class="o">.</span><span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">transformed_data</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:</span><span class="mi">1000</span><span class="p">])</span> <span class="c1"># reduces dimensionality of each vector to 2</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cluster_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">transformed_data</span><span class="p">,</span> <span class="n">targets</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]))</span>
<span class="n">cluster_data</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># transformed_data 2 dims (call them x and y) + targets 1 dim = 3</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tsne_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cluster_data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'x'</span><span class="p">,</span> <span class="s1">'y'</span><span class="p">,</span> <span class="s1">'targets'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tsne_df</span><span class="p">))</span>
<span class="n">tsne_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">viz</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">tsne_df</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">'targets'</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">viz</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">,</span> <span class="s1">'x'</span><span class="p">,</span> <span class="s1">'y'</span><span class="p">)</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Resources">
<a class="anchor" href="#Resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resources<a class="anchor-link" href="#Resources"> </a>
</h2>
<ol>
<li>
<p><a href="https://book.fast.ai">https://book.fast.ai</a> - The book's website; it's updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc...</p>
</li>
<li>
<p><a href="https://course.fast.ai/datasets">https://course.fast.ai/datasets</a> - A variety of slimmed down datasets you can use for various DL tasks that support "rapid prototyping and experimentation."</p>
</li>
<li>
<p><a href="https://huggingface.co/docs/datasets/">https://huggingface.co/docs/datasets/</a> - Serves a similar purpose to the fastai datasets but for the NLP domain. Includes metrics and full/sub-set datasets that you can use to benchmark your results against the top guns of deep learning.
</p>
<div class="flash flash-warn">
    <svg class="octicon octicon-zap octicon octicon-zap octicon octicon-zap" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z"></path></svg>
    <strong>Important: </strong>Start with a smaller dataset and scale up to full size to accelerate modeling!
</div>
</li>
</ol>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ohmeow/ohmeow_website"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/posts/2020/11/06/ajtfb-chapter-1.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card row">
  <data class="u-url" href="/"></data>

  <div class="wrapper col">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p>A full-stack web application and ML development company.</p>
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a class="fa-icon" rel="me" href="mailto:wgilliam@ohmeow.com" title="wgilliam@ohmeow.com"><i class="far fa-envelope"></i></a></li><li><a rel="me" href="https://github.com/ohmeow" title="ohmeow"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/waydegilliam" title="waydegilliam"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul></div>

  </div>

</footer>

<script>
  // method will ensure the floating header isn't covering the note header
  function goToNote(elId) {
    const el = document.getElementById(elId);
    const elPos = el.getBoundingClientRect().top + document.documentElement.scrollTop;
      
    window.focus();
    window.scrollTo(0, elPos - 90);
  }

  // ensure target note shows below floating header on # change
  window.addEventListener("hashchange", (ev) => { 
    const hash = ev.target.location.hash
    if (hash) {
      goToNote(hash.substring(1))
    }
  });

  // onload, ensure note shows below floating header
  window.addEventListener("load", (ev) => { 
    const hash = ev.target.location.hash
    if (hash) {
      goToNote(hash.substring(1))
    }
  });
</script></body>

</html>
