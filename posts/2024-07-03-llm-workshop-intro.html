<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Wayde Gilliam">
<meta name="dcterms.date" content="2024-07-03">
<meta name="description" content="Welcome to the inaugural post of my series on the intricacies of my course project from Hamel Husain and Dan Barker’s expansive LLM Workshop/Course. In this opening article, I’ll be diving into a post-mortem analysis of the course, sharing key takeaways, and offering insights on how to effectively navigate a course of this nature.">

<title>LLM Workshop #1 - How to take a course that never ends – ohmeow</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../images/ohmeow_favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7a0b31a7f61027fd71bbba04eea7db2e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<meta property="og:title" content="LLM Workshop #1 - How to take a course that never ends – ohmeow">
<meta property="og:description" content="Welcome to the inaugural post of my series on the intricacies of my course project from Hamel Husain and Dan Barker’s expansive LLM Workshop/Course. In this opening article, I’ll be diving into a post-mortem analysis of the course, sharing key takeaways, and offering insights on how to effectively navigate a course of this nature.">
<meta property="og:image" content="https://ohmeow.com/posts/images/blog-20240603/header.png">
<meta property="og:site_name" content="ohmeow">
<meta property="og:image:height" content="478">
<meta property="og:image:width" content="1007">
<meta name="twitter:title" content="LLM Workshop #1 - How to take a course that never ends – ohmeow">
<meta name="twitter:description" content="Welcome to the inaugural post of my series on the intricacies of my course project from Hamel Husain and Dan Barker’s expansive LLM Workshop/Course. In this opening article, I’ll be diving into a post-mortem analysis of the course, sharing key takeaways, and offering insights on how to effectively navigate a course of this nature.">
<meta name="twitter:image" content="https://ohmeow.com/posts/images/blog-20240603/header.png">
<meta name="twitter:image-height" content="478">
<meta name="twitter:image-width" content="1007">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../images/ohmeow_logo.png" alt="ohmeow.com" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">ohmeow</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-guides" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-tools" role="img">
</i> 
 <span class="menu-text">Guides</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-guides">    
        <li>
    <a class="dropdown-item" href="../pages/guides/vue3-guide.html"><i class="bi bi-box-arrow-up-right" role="img">
</i> 
 <span class="dropdown-text">Vue3</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/guides/fastapi-guide.html"><i class="bi bi-box-arrow-up-right" role="img">
</i> 
 <span class="dropdown-text">Fast API</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/guides/postgresql-guide.html"><i class="bi bi-box-arrow-up-right" role="img">
</i> 
 <span class="dropdown-text">PostgreSQL</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/guides/deployment-guide.html"><i class="bi bi-box-arrow-up-right" role="img">
</i> 
 <span class="dropdown-text">Deployment</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-projects" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-stack" role="img">
</i> 
 <span class="menu-text">Projects</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-projects">    
        <li>
    <a class="dropdown-item" href="https://ohmeow.github.io/blurr/"><i class="bi bi-layers" role="img">
</i> 
 <span class="dropdown-text">blurr</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-study-groups" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-book" role="img">
</i> 
 <span class="menu-text">Study Groups</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-study-groups">    
        <li>
    <a class="dropdown-item" href="https://www.youtube.com/playlist?list=PLD80i8An1OEF8UOb9N9uSoidOGIMKW96t"><i class="bi bi-youtube" role="img">
</i> 
 <span class="dropdown-text">fastai x Hugging Face Study Group</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> <i class="bi bi-person-circle" role="img">
</i> 
<span class="menu-text">About Me</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/waydegilliam"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ohmeow"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#mastering-llms-a-conference-for-developers-data-scientists" id="toc-mastering-llms-a-conference-for-developers-data-scientists" class="nav-link active" data-scroll-target="#mastering-llms-a-conference-for-developers-data-scientists">Mastering LLMs: A Conference For Developers &amp; Data Scientists</a></li>
  <li><a href="#how-to-take-this-course" id="toc-how-to-take-this-course" class="nav-link" data-scroll-target="#how-to-take-this-course">How to take this course</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a>
  <ul class="collapse">
  <li><a href="#trust-fucking-no-one-tfno" id="toc-trust-fucking-no-one-tfno" class="nav-link" data-scroll-target="#trust-fucking-no-one-tfno">Trust fucking no one (TFNO)</a></li>
  <li><a href="#look-at-your-data" id="toc-look-at-your-data" class="nav-link" data-scroll-target="#look-at-your-data">Look at your data</a></li>
  <li><a href="#build-an-evals-first-development-mindset" id="toc-build-an-evals-first-development-mindset" class="nav-link" data-scroll-target="#build-an-evals-first-development-mindset">Build an Evals-First Development Mindset</a></li>
  <li><a href="#prove-you-need-to-finetune" id="toc-prove-you-need-to-finetune" class="nav-link" data-scroll-target="#prove-you-need-to-finetune">Prove You Need to Finetune</a></li>
  <li><a href="#dont-try-and-build-a-general-purpose-chatbot" id="toc-dont-try-and-build-a-general-purpose-chatbot" class="nav-link" data-scroll-target="#dont-try-and-build-a-general-purpose-chatbot">Don’t Try and Build a General Purpose Chatbot</a></li>
  <li><a href="#use-your-eval-pipeline-to-curate-training-datasets" id="toc-use-your-eval-pipeline-to-curate-training-datasets" class="nav-link" data-scroll-target="#use-your-eval-pipeline-to-curate-training-datasets">Use your Eval Pipeline to Curate Training Datasets</a></li>
  <li><a href="#evallogging-tools-i-like-so-far" id="toc-evallogging-tools-i-like-so-far" class="nav-link" data-scroll-target="#evallogging-tools-i-like-so-far">Eval/Logging Tools I like (so far)</a></li>
  <li><a href="#minimize-friction" id="toc-minimize-friction" class="nav-link" data-scroll-target="#minimize-friction">Minimize Friction</a></li>
  <li><a href="#get-up-and-running-quick" id="toc-get-up-and-running-quick" class="nav-link" data-scroll-target="#get-up-and-running-quick">Get Up and Running Quick</a></li>
  <li><a href="#prefer-cloud-over-local-training" id="toc-prefer-cloud-over-local-training" class="nav-link" data-scroll-target="#prefer-cloud-over-local-training">Prefer Cloud over Local Training</a></li>
  <li><a href="#prefer-the-base-model-over-the-instruct-version-when-fine-tuning" id="toc-prefer-the-base-model-over-the-instruct-version-when-fine-tuning" class="nav-link" data-scroll-target="#prefer-the-base-model-over-the-instruct-version-when-fine-tuning">Prefer the Base model over the Instruct Version When Fine-tuning</a></li>
  <li><a href="#finetuning-examples-should-mimic-what-your-llm-will-see-in-production" id="toc-finetuning-examples-should-mimic-what-your-llm-will-see-in-production" class="nav-link" data-scroll-target="#finetuning-examples-should-mimic-what-your-llm-will-see-in-production">Finetuning Examples Should Mimic what your LLM will see in production</a></li>
  <li><a href="#for-structured-output-use-instructor-or-outlines" id="toc-for-structured-output-use-instructor-or-outlines" class="nav-link" data-scroll-target="#for-structured-output-use-instructor-or-outlines">For Structured Output, Use Instructor or Outlines</a></li>
  </ul></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next Steps</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">LLM Workshop #1 - How to take a course that never ends</h1>
  <div class="quarto-categories">
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">LLMS</div>
    <div class="quarto-category">datasets</div>
    <div class="quarto-category">learning</div>
    <div class="quarto-category">projects</div>
  </div>
  </div>

<div>
  <div class="description">
    Welcome to the inaugural post of my series on the intricacies of my course project from Hamel Husain and Dan Barker’s expansive LLM Workshop/Course. In this opening article, I’ll be diving into a post-mortem analysis of the course, sharing key takeaways, and offering insights on how to effectively navigate a course of this nature.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Wayde Gilliam </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 3, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="mastering-llms-a-conference-for-developers-data-scientists" class="level2">
<h2 class="anchored" data-anchor-id="mastering-llms-a-conference-for-developers-data-scientists">Mastering LLMs: A Conference For Developers &amp; Data Scientists</h2>
<div>
<p><img src="images/blog-20240727/hamel-eval-flow.png" alt="LLM Workshop" style="float:right;border-radius: 15px;"></p>
<p>These days, it goes by many names, but officially the “workshop” is known simply as <a href="https://maven.com/parlance-labs/fine-tuning" target="_blank">“Mastering LLMs: A Conference For Developers &amp; Data Scientists”</a>.</p>
<p>Initially envisioned as a four-week course, it quickly evolved into something much more dynamic — a conference brimming with talks, office hours, and a wealth of insights shared on their Discord. Participants also received a generous amount of credits from various companies in the field to experiment on their own.</p>
<p>Although the event has technically concluded, I’m remain only 95% convinced that it’s truly over (a new “30 minute” 90 minute session could pop-up in my calendar at any moment).</p>
<p>At it’s core, the event is about equipping participants with the necessary tools and techniques to create a comprehensive pipeline for building generative NLP solutions, from dataset curation to deployment. Its spans 4 core workshops, 20 conference sessions, 6 office hours, and a bunch of async discussions via discord. If that sounds like a lot of content, good … because it is! My friend Sanyam summed my feelings in week 3 with this message I got from him on Discord …</p>
<blockquote class="blockquote">
<p>Are you feeling overwhelemed by the course as well? I feel this is fastai by at 1000x speed … but for real world, energy. It’s impossible for me to just keep up with the lectures, barely getting time to play with examples or run some code.</p>
</blockquote>
<p>So yah, to start, I want to share my thoughts on the major lessons learned and help folks figure out how to navigate and succeed in course/workshop/whatever of this scale. This is an opinionated take from someone who’s been around awhile, but I hope an experienced take worth consideration.</p>
</div>
</section>
<section id="how-to-take-this-course" class="level2">
<h2 class="anchored" data-anchor-id="how-to-take-this-course">How to take this course</h2>
<p>Here I want to share 4 tips to getting the most of this course.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip 1: Focus Your Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p>My advice is to <strong>definitely watch the main conference workshops … TWICE</strong>.</p>
<p>This is how I do the fastai courses as well. I watch the lecture more as an active participant the first time, watching it live if possible, asking (and answering question), jotting down thoughts or other parts of the lecture I want to dive deeper into later. On my second watch, I usually have a “printed” version of the session notes/slides on my iPad that I annotate as I got through it again a bit slower. I usually do this 2nd watch a few days after to give my brain time to rest and reflect.</p>
<p>From there I suggest <strong>following the same approach with any conference talks you’re interested in … but only doing that 2nd watch for those you care about</strong>. Honestly, I still haven’t watched all of them myself and have been content to really spend time with those most close to the things I’m working with IRL and/or I want to learn (e.g., I’ve watched Ankur Goyal’s talk on using BrainTrust about 3-4 times because I like the tool and I want to explore it as something I use in both my personal and professional life).</p>
<p>As <strong>for the office hours, I’ve tried to attend as many as I could and ask a lot of questions</strong>. I don’t watch these a 2nd time and I don’t do a lot of detailed note taking except where something really lands as important or useful to me personally (like an answer to one of the really great questions that come up in these).</p>
<p>Time is precious. One of the keys to succeeding in this course and walking away without a mental breakdown is to manage it by spending your time on the things that matter to you.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip 2: The Discord Is The Alpha
</div>
</div>
<div class="callout-body-container callout-body">
<p>The like $3,500 in free credits is great, but for me, the real alpha of this course … and the real bang for your buck … is in the Discord. <strong>Spend a lot of time there!</strong></p>
<p>You are literally able to interact with other practitioners and course instructors who charge serveral hundered dollars an hour as consultants for the same kind of expertise and guidance you get for your one-time payment of $500. For me, this puts the value of the course into the tens of thousands easily, making my call to signup easily one of the best decision I’ve made in 2024.</p>
<p>Again, there is A LOT of content in the discord so my recommendation is to <strong>focus on the channels dedicated to things you care about</strong> vs.&nbsp;trying to read it all. Some of my favorites include:</p>
<ul>
<li><p>Workshops (all of them)</p></li>
<li><p>Debugging Help / training-runs (lots of learning from folks like Zach Mueller and others via real fine-tuning attempts; very helpful)</p></li>
<li><p>Debugging Help / axolotl</p></li>
<li><p>Talks / kylecorbitt_prompt_to_model</p></li>
<li><p>Talks / ankurgoyal_textsql_llms</p></li>
<li><p>Talks / jason_improving_rag</p></li>
<li><p>Talks / clavie_beyond_rag_basics (deserved way more than 30 minutes, 😁)</p></li>
<li><p>Talks / pawel-function-calling</p></li>
<li><p>Talks / whitaker_napkin_math</p></li>
<li><p>Office Hours / charles-modal</p></li>
<li><p>Office Hours / replicate</p></li>
</ul>
<p>Again, time is precious. Since my interests were with evals, structured outputs, RAG, and deployment … the above were the channels I spent most of my time in. Believe it or not, there are a few channels that remain unexplored for the time being (so much great stuff).</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip 3: How time works in this course
</div>
</div>
<div class="callout-body-container callout-body">
<p>Time works different in the workshop. For example, a “30 minute” session is really about 75 minutes, a “45 minute” session about 90 minutes, and any of the “2-hour” workshop will likely go an additonal hour.</p>
<p>Now that this course is “over”, this may not be that big of a deal. But, when I was blocking out time in my calendar these were the heuristics I followed. Think on this when that next calendar invite pops up or you end up taking some similar course in the future.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip 4: Build something
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you can do only one thing … build something and blog it!</p>
<p>This is really the golden rule of course like the LLM Workshop and fast.ai. If you really want to gain some mastery in the subject, you have to build something and explain it to others.</p>
<p>A great example of this in practice can be found by following <a href="https://mlops.systems/" target="_blank">Alex Strick van Linschoten’s</a> blog posts regarding his course project. Exceptional content and an example of what it looks like to succeed in a course like this. Its a win-win for Alex and the community at large.</p>
<p>This is the direction I’ll be going in future posts in this series.</p>
</div>
</div>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<p>I’ve learned a lot to say the least. Below are some of the key takeways I’ve taken away from the course that I hope will aid your navigation and success with it as well.</p>
<section id="trust-fucking-no-one-tfno" class="level3">
<h3 class="anchored" data-anchor-id="trust-fucking-no-one-tfno">Trust fucking no one (TFNO)</h3>
<div style="clear:both;">
<p><img src="images/blog-20240603/charles-tfno.png" alt="TFNO" style="float:left;border-radius: 15px;margin:10px;height:150px;width:auto;"></p>
<p><strong>“Trust fucking no one!”</strong> - Charles Frye (also winner of quote of the conference)</p>
<p>Indeed, when it comes to fine-tuning … “If you’re not nervous, you don’t understand.”</p>
<p>If you weren’t scared as hell about getting your <a href="https://hamel.dev/blog/posts/prompt/" target="_blank">prompt templates to match up at both training and inference time</a> already, be prepared to be terrified. And if you’re not carefully looking at examples of your fleshed out prompts during both phases, prepare to get F’d.</p>
<blockquote class="blockquote">
<p><img src="images/blog-20240603/hamel-icon.png" alt="Hamel" style="border-radius:30px;width:auto;height:25px;"> 99% of errors happen with the template</p>
</blockquote>
</div>
</section>
<section id="look-at-your-data" class="level3">
<h3 class="anchored" data-anchor-id="look-at-your-data">Look at your data</h3>
<p>You need to spend time looking at your data to understand what is going on, gain intuiton on how to improve things, and actually make things better. Automated metrics can only take you so far in the generative game and even high-quality LLMs as judges will fail. Look at your data a lot!</p>
</section>
<section id="build-an-evals-first-development-mindset" class="level3">
<h3 class="anchored" data-anchor-id="build-an-evals-first-development-mindset">Build an Evals-First Development Mindset</h3>
<p>Evals are a big part of this course and likely one of the most ignore aspects in building generative solutions where the normal course seems to be for folks to just slap a react app on top of a fastapi backend that makes calls out to gpt-4 with a prompt and RAG mechanism that seems to work. But how can you tell if it is really “working”? How can you tell it is still working and working for others besides yourself? How can you tell changes you make to your model, prompt, RAG, whatever is having a positive or negative effect?</p>
<p>The answer is you can’t without good evals, and good evals are something built up over time and a project in and of themselves. They aren’t static. Having a good system to build and continuously improve on them is one of the most important ways you can ensure you are delivering a quality AI solution.</p>
<p>Also, don’t forget to include human experts where possible!</p>
</section>
<section id="prove-you-need-to-finetune" class="level3">
<h3 class="anchored" data-anchor-id="prove-you-need-to-finetune">Prove You Need to Finetune</h3>
<blockquote class="blockquote">
<p><img src="images/blog-20240603/hamel-icon.png" alt="Hamel" style="border-radius:30px;width:auto;height:25px;"> Try not to finetune first … you need to prove to yoruefl that you should finetune [using] some minimal evaluation system and you hit a wall and can’t make progress by just prompting</p>
</blockquote>
<p>Good use cases for finetuning:</p>
<ol type="1">
<li>Owning your own model</li>
<li>Data privacy</li>
<li>Domain specific things that models like GPT and Claude haven’t been trained on</li>
</ol>
<p>The narrower the domain/problem … the better.</p>
</section>
<section id="dont-try-and-build-a-general-purpose-chatbot" class="level3">
<h3 class="anchored" data-anchor-id="dont-try-and-build-a-general-purpose-chatbot">Don’t Try and Build a General Purpose Chatbot</h3>
<p>Being asked to build a chatbot is a smell because the surface area is very large and unscoped, making it difficult to make progress on. We simply do not have the time, money, data, and resources companies like OpenAI and Anthropic have to make a chatbot that can perform as well as theirs in general … and no one wants to use an AltaVista like chatbot when you got Google.</p>
<p>I’m watching this happen in real-time at work and its not pretty.</p>
</section>
<section id="use-your-eval-pipeline-to-curate-training-datasets" class="level3">
<h3 class="anchored" data-anchor-id="use-your-eval-pipeline-to-curate-training-datasets">Use your Eval Pipeline to Curate Training Datasets</h3>
<p>In the real-world, we can’t just start with pre-processed dataset from Hugging Face. In the real-world, we’ll often have to find creative and out of the box ways to curate our own dataset to evaluating our AI systems. Having a good evals framework is how you get this done, especially at the beginning where you probably don’t have access to many, if any, human experts providing examples for you to use. Starting with a good LLM from OpenAI or Anthropic for example, along with a growing set of scoring functions and some time of your own for review, is a great way to curate an initial training dataset you can build on going forward.</p>
<p>Breakdown use cases into specific examples, log the traces, and use them in your finetunes.</p>
<p><strong>What LLMs should I use in generating synthetic data?</strong></p>
<blockquote class="blockquote">
<p><img src="images/blog-20240603/hamel-icon.png" alt="Hamel" style="border-radius:30px;width:auto;height:25px;"> I like mistral large … use the most powerful model you can”</p>
</blockquote>
</section>
<section id="evallogging-tools-i-like-so-far" class="level3">
<h3 class="anchored" data-anchor-id="evallogging-tools-i-like-so-far">Eval/Logging Tools I like (so far)</h3>
<p>I’m going to give BrainTrust and LangSmith a go. I’m most familiar with the later, but I really liked the UX and explict-ness of running evals with BrainTrust.</p>
</section>
<section id="minimize-friction" class="level3">
<h3 class="anchored" data-anchor-id="minimize-friction">Minimize Friction</h3>
<p>Just like analysis paralysis, there’s an eval paralysis where you might be so consumed on having a fully fleshed out eval pipeline or the feeling you have to pick out the right tooling that you never really get started. Don’t let that happen. Start with simple assertions, use the high-quality LLMs, use notebooks … do whatever you can to get a quality dataset you can get going with quickly</p>
</section>
<section id="get-up-and-running-quick" class="level3">
<h3 class="anchored" data-anchor-id="get-up-and-running-quick">Get Up and Running Quick</h3>
<p>No one wants to deploy a training job with 100k samples for fine-tuning a Llama3 70B only to find out that it fails after a day or so of running because you misspelled your HuggingFace username. Here’s my approach to quickly iterate and verify your finetuning strategy will work from beginning to end (learned from my time with the fast.ai course btw)</p>
<blockquote class="blockquote">
<p><img src="images/blog-20240603/wayde-icon.png" alt="Wayde" style="border-radius:10px;width:auto;height:25px;"> Use a subset of my dataset to train a model. Verify my inputs/targets look right, training works end-to-end, that my metrics are being logged to wanb correctly, and that my artifacts can be used and optionally uploaded to HF. I make my sample really small … like maybe 300-500 examples so I can iterate fast here. Once my setup is golden and everything is checked into github, login into Jarvis, grab yourself some big GPUS, clone your repo, install any libraries you need to, and train on your full dataset. Pour yourself a glass of a good scotch or whiskey.</p>
</blockquote>
<p>Try to get it down to 15-30 mins max to verify your training works from start to finish. If you are using axolotl, take set <code>max_steps</code> to something small like 500.</p>
<p><strong>How much data do you need?</strong></p>
<p>I’ve heard as little as 50-100 examples can get you far.</p>
</section>
<section id="prefer-cloud-over-local-training" class="level3">
<h3 class="anchored" data-anchor-id="prefer-cloud-over-local-training">Prefer Cloud over Local Training</h3>
<p>I had a hell of a time getting CUDA installed and getting things to run on my 2x3090s. I don’t regret it, in fact, I get some sick satisfaction out of it. There’s nothing like launching a training job on my two blower GPUs as my wife sits down to watch an episode of Bridgerton on the TV downstairs (under which sits my DL rig).</p>
<p>However, I see lots of students struggling so hard on trying to get things working locally that they become beaten down before getting to the real meat of things. So my 2 cents for most folks in genera …</p>
<blockquote class="blockquote">
<p><img src="images/blog-20240603/wayde-icon.png" alt="Wayde" style="border-radius:10px;width:auto;height:25px;"> Its probably better to use a cloud provider like <a href="https://jarvislabs.ai/" target="_blank">jarvis</a> or <a href="https://modal.com/" target="_blank">modal</a> rather than battling with your local rig. Of course, I didn’t follow my own advice because I’m OCD about things like this and can’t let things go until I understand the whys, hows, and resolutions for problems like this.</p>
</blockquote>
<p>I’ve had excellent experience working with both of these platforms, along with superb support and help from folks like Vishnu (founder at Jarvis), and Charles (AI engineer at Modal Labs and speaker at every AI conference).</p>
<p>Also, use Linux. I see so many folks trying to get things working on Windows or their slick M3 Macbook pro and that is just a pathway to misery that will make you question whether you want to even be in the ML field. Don’t do it. Linux FTW</p>
</section>
<section id="prefer-the-base-model-over-the-instruct-version-when-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="prefer-the-base-model-over-the-instruct-version-when-fine-tuning">Prefer the Base model over the Instruct Version When Fine-tuning</h3>
<p>Most LLMs come in two flavors, base and instruct. The former focuses simply on next-word prediction tasks whereas the later is focused on conversation. In general, the recommendation in the course is to use the base version to build on top of as it gives you complete control of the prompt template without having to worry about following the “instruct” template of whatever model architecture you choose to the tee. Also, as these instruct tempaltes vary widely from one LLM to another, its seems using the base model make it easier to run fine-tunes against a nubmer of LLMs without a lot of code gymnastics.</p>
<p>Insturction models have been finetuned for conversation … something you want to chat with.</p>
<p>Start with the basse model where possible … with the smallest model possible (e.g, the 7/8B parameter range)</p>
</section>
<section id="finetuning-examples-should-mimic-what-your-llm-will-see-in-production" class="level3">
<h3 class="anchored" data-anchor-id="finetuning-examples-should-mimic-what-your-llm-will-see-in-production">Finetuning Examples Should Mimic what your LLM will see in production</h3>
<p>If RAG is being used with your properitery LLMs, include it in your training examples for use in finetuning.</p>
</section>
<section id="for-structured-output-use-instructor-or-outlines" class="level3">
<h3 class="anchored" data-anchor-id="for-structured-output-use-instructor-or-outlines">For Structured Output, Use Instructor or Outlines</h3>
<blockquote class="blockquote">
<p><img src="images/blog-20240603/hamel-icon.png" alt="Hamel" style="border-radius:30px;width:auto;height:25px;"> For open models you should use outlines. for closed models APIs you should use instructor.</p>
</blockquote>
<blockquote class="blockquote">
<p><img src="images/blog-20240603/hamel-icon.png" alt="Hamel" style="border-radius:30px;width:auto;height:25px;"> Instructor uses prompting and retries to achieve the desired output (and with openai it will prompt intelligently via the function schema definitions). Outlines clamp down the model predictions to only alllow permissible tokens according to your grammar. Outlines is going to have much stronger guarantees that it will work. No retries are even necessary. If outlines is compatible with your model (you have access to the forward pass because you own the model) you should use outlines</p>
</blockquote>
</section>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<p>Build something.</p>
<p>Following <a href="https://github.com/parlance-labs/ftcourse" target="_blank">Hamel’s course project repo</a> as a guide, I’m going to explore improving a ML pipeline I built for work that allows users to upload a collection of survey comments, interview answers, etc… and have a variety of NLP analysis performed on the documents collectively and individually (eg., translation, sentiment, summarization, NER, and thematic analysis). The data and responses are styled in a way familiar to faculty, staff, and students in higher ed.</p>
<p>The current system makes a call out to gpt-4 for every document and for every collection of related documents to perform each task. It can get pricey and can take a long time depending on the number of documents being processed. I did this because I couldn’t get reliable results from asking the LLM to perform a number of tasks, whether the document/s where short or long.</p>
<p>For this course project, I’m going to start from the beginning and try to improve things by:</p>
<ol type="1">
<li><p>Curating a dataset from both generated data and local traces I have from the existing pipeline</p></li>
<li><p>Follow and “eval first” development structure, so I can evaluate the proprietary LLMs (e.g., gpt-4, gpt-4o, gpt-3.5-turbo, etc..) I’m using right now.</p></li>
<li><p>Use tool calling and more complex pydantic models to reduce the number of calls I need to make and lower costs.</p></li>
<li><p>Finetune a variety of smaller models on a curated datset to see if I can improve the quality of my predictions while also lowering costs and latency even more.</p></li>
</ol>
<p>If folks have any suggestions or ideas to improve this approach, please let me know in the comments or on twitter. I’d especially love to hear from people who have gone down a similar path and what worked (or didn’t) for them.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("^(?:http:|https:)\/\/?(www)\.ohmeow\.com\/**");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="ohmeow/ohmeow_website" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>