<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models | ohmeow</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models" />
<meta name="author" content="Wayde Gilliam" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This chapter of “Deep Learning for Coders with fastai &amp; PyTorch” details several techniques you can apply to getting SOTA results with your image classification models! It’s the last chapter dedicated to computer vision before diving into colloborate filtering, tabular, and NLP models" />
<meta property="og:description" content="This chapter of “Deep Learning for Coders with fastai &amp; PyTorch” details several techniques you can apply to getting SOTA results with your image classification models! It’s the last chapter dedicated to computer vision before diving into colloborate filtering, tabular, and NLP models" />
<link rel="canonical" href="https://ohmeow.com/posts/2022/03/28/ajtfb-chapter-7.html" />
<meta property="og:url" content="https://ohmeow.com/posts/2022/03/28/ajtfb-chapter-7.html" />
<meta property="og:site_name" content="ohmeow" />
<meta property="og:image" content="https://ohmeow.com/images/articles/fastbook.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-28T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"This chapter of “Deep Learning for Coders with fastai &amp; PyTorch” details several techniques you can apply to getting SOTA results with your image classification models! It’s the last chapter dedicated to computer vision before diving into colloborate filtering, tabular, and NLP models","mainEntityOfPage":{"@type":"WebPage","@id":"https://ohmeow.com/posts/2022/03/28/ajtfb-chapter-7.html"},"url":"https://ohmeow.com/posts/2022/03/28/ajtfb-chapter-7.html","@type":"BlogPosting","image":"https://ohmeow.com/images/articles/fastbook.jpg","author":{"@type":"Person","name":"Wayde Gilliam"},"headline":"A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models","dateModified":"2022-03-28T00:00:00-05:00","datePublished":"2022-03-28T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ohmeow.com/feed.xml" title="ohmeow" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163296836-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-163296836-1');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

    <div class="wrapper"><a class="site-title" rel="author" href="/"><img style="height: 60px;" src="/images/ohmeow_logo.png" alt="ohmeow.com"></a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger"><a class="page-link" href="/about/"><span class="top-menu-text">About Me</span></a><a class="page-link" href="/guides/"><span class="top-menu-text">Guides</span></a><a class="page-link" href="/search/"><span class="top-menu-text">Search</span></a><a class="page-link" href="/categories/"><span class="top-menu-text">Tags</span></a></div>
        </nav></div>
  </header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">A Journey Through Fastbook (AJTFB) - Chapter 7: Advanced techniques for training image classification models</h1><p class="page-description">This chapter of <a href='https://github.com/fastai/fastbook'>"Deep Learning for Coders with fastai & PyTorch"</a> details several techniques you can apply to getting SOTA results with your image classification models! It's the last chapter dedicated to computer vision before diving into colloborate filtering, tabular, and NLP models</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-03-28T00:00:00-05:00" itemprop="datePublished">
        Mar 28, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Wayde Gilliam</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#fastai">fastai</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#fastbook">fastbook</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#classification">classification</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#computer vision">computer vision</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#techniques">techniques</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#bag of tricks">bag of tricks</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/ohmeow/ohmeow_website/tree/master/_notebooks/2022-03-28-ajtfb-chapter-7.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          
          <div class="px-2">
    <a href="https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/_notebooks/2022-03-28-ajtfb-chapter-7.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Imagenette">Imagenette </a></li>
<li class="toc-entry toc-h2"><a href="#Tip-1:-Use-the-"presizing-trick"">Tip 1: Use the &quot;presizing trick&quot; </a></li>
<li class="toc-entry toc-h2"><a href="#Tip-2:-Create-a-"baseline"">Tip 2: Create a &quot;baseline&quot; </a></li>
<li class="toc-entry toc-h2"><a href="#Tip-3:-Normalize-your-data">Tip 3: Normalize your data </a></li>
<li class="toc-entry toc-h2"><a href="#Tip-4:-Use-"progressive-resizing"">Tip 4: Use &quot;progressive resizing&quot; </a></li>
<li class="toc-entry toc-h2"><a href="#Tip-5:-Use-Test-Time-Augmentation-(TTA)">Tip 5: Use Test Time Augmentation (TTA) </a></li>
<li class="toc-entry toc-h2"><a href="#Tip-6:-Use-MixUp">Tip 6: Use MixUp </a></li>
<li class="toc-entry toc-h2"><a href="#Tip-7:-Use-"Label-Smoothing"">Tip 7: Use &quot;Label Smoothing&quot; </a></li>
<li class="toc-entry toc-h2"><a href="#Resources">Resources </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-03-28-ajtfb-chapter-7.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Other posts in this series:<br>
<a href="https://ohmeow.com/posts/2020/11/06/ajtfb-chapter-1.html">A Journey Through Fastbook (AJTFB) - Chapter 1</a><br>
<a href="https://ohmeow.com/posts/2020/11/16/ajtfb-chapter-2.html">A Journey Through Fastbook (AJTFB) - Chapter 2</a><br>
<a href="https://ohmeow.com/posts/2020/11/22/ajtfb-chapter-3.html">A Journey Through Fastbook (AJTFB) - Chapter 3</a><br>
<a href="https://ohmeow.com/posts/2021/05/23/ajtfb-chapter-4.html">A Journey Through Fastbook (AJTFB) - Chapter 4</a><br>
<a href="https://ohmeow.com/posts/2021/06/03/ajtfb-chapter-5.html">A Journey Through Fastbook (AJTFB) - Chapter 5</a><br>
<a href="https://ohmeow.com/posts/2021/06/10/ajtfb-chapter-6-multilabel.html">A Journey Through Fastbook (AJTFB) - Chapter 6a</a> \
<a href="https://ohmeow.com/posts/2022/02/09/ajtfb-chapter-6-regression.html">A Journey Through Fastbook (AJTFB) - Chapter 6b</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Imagenette">
<a class="anchor" href="#Imagenette" aria-hidden="true"><span class="octicon octicon-link"></span></a>Imagenette<a class="anchor-link" href="#Imagenette"> </a>
</h2>
<p>The <strong>Imagenette</strong> is a subset of the <strong>ImageNet</strong> dataset that "contains a subset of 10 very different categories from the orginal ImageNet dataset, making for quicker training when we want to experiment"</p>
<p>Tip: Start with small datasets and models for initial experimentation and prototyping. Both will allow you to iterate over your experiments more quickly and verify your code works from beginning to end without having to wait hours for your training/validation loops to finish. "You should aim to have an iteration speed of no more than a couple of minutes .... <em>If it's taking longer to do an experiment, think about how you could cut down your dataset, or simply your model, to improve your experimentation speed.</em>"</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">IMAGENETTE</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id='Tip-1:-Use-the-"presizing-trick"'>
<a class="anchor" href="#Tip-1:-Use-the-" presizing-trick aria-hidden="true"><span class="octicon octicon-link"></span></a>Tip 1: Use the "presizing trick"<a class="anchor-link" href="#Tip-1:-Use-the-%22presizing-trick%22"> </a>
</h2>
<p>See chapter 5, pp.189-191.  The idea here is to first crop the image <strong>so that further augmentations can be applied without creating empty space</strong> (via <code>item_tfms</code>), with further augmentations being applied on the GPU on batches of images for speed (via <code>batch_tfms</code>).</p>
<p>On the training set, the initial crop area is chosen randomly with the size set to cover the entire width/height of the image with random crop and other augmentations done on the GPU.</p>
<p>On the validation set, a center square is always used in the first step and only a resize is applied on the GPU to get the image width/height equal to the final size needed.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span>
    <span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">ImageBlock</span><span class="p">(),</span> <span class="n">CategoryBlock</span><span class="p">()),</span>
    <span class="n">get_items</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">,</span>
    <span class="n">get_y</span> <span class="o">=</span> <span class="n">parent_label</span><span class="p">,</span>
    <span class="n">item_tfms</span> <span class="o">=</span> <span class="n">Resize</span><span class="p">(</span><span class="mi">460</span><span class="p">),</span>
    <span class="n">batch_tfms</span> <span class="o">=</span> <span class="n">aug_transforms</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">dls</span> <span class="o">=</span> <span class="n">dblock</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id='Tip-2:-Create-a-"baseline"'>
<a class="anchor" href="#Tip-2:-Create-a-" baseline aria-hidden="true"><span class="octicon octicon-link"></span></a>Tip 2: Create a "baseline"<a class="anchor-link" href="#Tip-2:-Create-a-%22baseline%22"> </a>
</h2>
<p>Note we are <strong><em>not</em></strong> using a pretrained model here ... we are training one from scratch.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">xresnet50</span><span class="p">()</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">CrossEntropyLossFlat</span><span class="p">(),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">3e-3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.654316</td>
      <td>4.794844</td>
      <td>0.320015</td>
      <td>05:21</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.217274</td>
      <td>1.211676</td>
      <td>0.612024</td>
      <td>05:19</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.964628</td>
      <td>1.417025</td>
      <td>0.617252</td>
      <td>05:06</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.736836</td>
      <td>0.677910</td>
      <td>0.787155</td>
      <td>05:12</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.596578</td>
      <td>0.539180</td>
      <td>0.833831</td>
      <td>05:05</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Tip-3:-Normalize-your-data">
<a class="anchor" href="#Tip-3:-Normalize-your-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tip 3: Normalize your data<a class="anchor-link" href="#Tip-3:-Normalize-your-data"> </a>
</h2>
<p>Tip: "When training a model, it helps if your input data is <em>normalized</em> - this is, <strong>has a mean of 0 and a standard deviation of 1</strong>.""</p>
<p>For images we do this over each channel (the 1 dimension) but averaging over all axes with the exception of the channel axis.  In fastai, we can utilize the <strong><code>Normalize</code></strong> transform to apply this a batch at a time.</p>
<p>Important: If we don't tell this transform what mean/std to use, "fastai will automatically calculate them from a single batch of your data"</p>
<p>Important: If we are using ImageNet images, we can use <code>imagenet_stats</code> instead of calculating the mean/std ourselves).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># (because we aren't using normalization yet, you'll see the mean and standard deviation are not very close to</span>
<span class="c1"># 0 and 1 respectively)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>

<span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]),</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(TensorImage([0.4518, 0.4554, 0.4344], device='cuda:0'),
 TensorImage([0.2868, 0.2783, 0.2998], device='cuda:0'))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_dls</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">):</span>
  <span class="n">dblock</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span>
      <span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">ImageBlock</span><span class="p">(),</span> <span class="n">CategoryBlock</span><span class="p">()),</span>
      <span class="n">get_items</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">,</span>
      <span class="n">get_y</span> <span class="o">=</span> <span class="n">parent_label</span><span class="p">,</span>
      <span class="n">item_tfms</span> <span class="o">=</span> <span class="n">Resize</span><span class="p">(</span><span class="mi">460</span><span class="p">),</span>
      <span class="n">batch_tfms</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">aug_transforms</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">image_size</span><span class="p">,</span> <span class="n">min_scale</span><span class="o">=</span><span class="mf">0.75</span><span class="p">),</span> <span class="n">Normalize</span><span class="o">.</span><span class="n">from_stats</span><span class="p">(</span><span class="o">*</span><span class="n">imagenet_stats</span><span class="p">)]</span>
  <span class="p">)</span>

  <span class="n">dls</span> <span class="o">=</span> <span class="n">dblock</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">dls</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">get_dls</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="c1"># an example of normalization calculated on a batch of images</span>
<span class="c1"># (because we are using normalization now, the mean and standard deviation are very close to 0 and 1 respectively)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]),</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]))</span>

<span class="c1"># does this normalization improve our model? Let's see ...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">xresnet50</span><span class="p">()</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">CrossEntropyLossFlat</span><span class="p">(),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">3e-3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>TensorImage([-0.0816, -0.0114,  0.0695], device='cuda:0') TensorImage([1.1806, 1.1762, 1.2825], device='cuda:0')
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.701530</td>
      <td>1.856552</td>
      <td>0.468633</td>
      <td>05:07</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.280709</td>
      <td>1.384676</td>
      <td>0.573562</td>
      <td>05:05</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.007325</td>
      <td>1.073023</td>
      <td>0.656460</td>
      <td>05:06</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.762624</td>
      <td>0.666320</td>
      <td>0.784541</td>
      <td>05:06</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.606407</td>
      <td>0.573812</td>
      <td>0.823376</td>
      <td>05:02</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Important: "... when you distribute a model, you need to also distribute the statistics used for normalization, since anyone using it for inference or transfer learning will need to use the same statistics .... <strong>If you're using a model that someone else has trained, make sure you find out what normalization statistics they used and match them.</strong>"</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id='Tip-4:-Use-"progressive-resizing"'>
<a class="anchor" href="#Tip-4:-Use-" progressive-resizing aria-hidden="true"><span class="octicon octicon-link"></span></a>Tip 4: Use "progressive resizing"<a class="anchor-link" href="#Tip-4:-Use-%22progressive-resizing%22"> </a>
</h2>
<p>"... start training using small images, and end training using large images. **Spending most of the epochs training with small images helps training complete faster."</p>
<p>Note: Think of this as a form of <strong><em>transfer learning</em></strong></p>
<p>"... <strong>the kinds of features that are learned by convolutional neural networks are not in any way specific to the size of the image</strong> .... So, when we change the image size in the middle of training, it doesn't mean that we have to find totally different parameters for our model."</p>
<p>Note: "Progressive resizing has an additional benefit: it is another form of data augmentation. Therefore, you should expect to see better generalization"</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">get_dls</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">)</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">xresnet50</span><span class="p">(),</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">CrossEntropyLossFlat</span><span class="p">(),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mf">3e-3</span><span class="p">)</span>

<span class="c1"># simply replace the `Learner.dls` with new `DataLoaders` and continue traning.</span>
<span class="n">learn</span><span class="o">.</span><span class="n">dls</span> <span class="o">=</span> <span class="n">get_dls</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fine_tune</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.848093</td>
      <td>1.582196</td>
      <td>0.526512</td>
      <td>03:02</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.297791</td>
      <td>1.205059</td>
      <td>0.616878</td>
      <td>03:01</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.985249</td>
      <td>1.022758</td>
      <td>0.690067</td>
      <td>02:55</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.762485</td>
      <td>0.688779</td>
      <td>0.787155</td>
      <td>02:53</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.845315</td>
      <td>1.171858</td>
      <td>0.650112</td>
      <td>05:08</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.635858</td>
      <td>0.834369</td>
      <td>0.751307</td>
      <td>05:06</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.664283</td>
      <td>0.665261</td>
      <td>0.796117</td>
      <td>05:10</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.585543</td>
      <td>0.634785</td>
      <td>0.796490</td>
      <td>05:11</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.478250</td>
      <td>0.495538</td>
      <td>0.840926</td>
      <td>05:02</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.429075</td>
      <td>0.448893</td>
      <td>0.855489</td>
      <td>05:08</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: To use the <code>DataLoaders</code> with bigger images, we simply assign it to <code>Learner.dls</code>.</p>
<p>Important: Bigger images will require smaller batch sizes. Also, you will not get a benefit of using images sized larger than the size of your images on disk!</p>
<p>Important: "... for transfer learning, progressive resizing <em>may</em> acutally hurt performance .... This is most likely to happen <strong>if your pretrained model was quite similar to your transfer learning task and the dataset and was trained on similar-sized images</strong>, so the weights don't need to be changed much. In that case, <strong>training on smaller images may damage the pretrained weights.</strong></p>
<p>"On the other hand, if the transfer learning task is going to use <strong>images that are of different sizes, shapes, or styles than those used in the pretraining task</strong>, progressive resizing will probably help"</p>
<p>Tip: If you are unsure, try it!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Tip-5:-Use-Test-Time-Augmentation-(TTA)">
<a class="anchor" href="#Tip-5:-Use-Test-Time-Augmentation-(TTA)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tip 5: Use Test Time Augmentation (TTA)<a class="anchor-link" href="#Tip-5:-Use-Test-Time-Augmentation-(TTA)"> </a>
</h2>
<p>Important: TTA is a form of data augmentation applied to the validation set that adds augmented versions of the images. "<strong>During inference or validation</strong>, creating multiple versions of each image using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image."</p>
<p>"... select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions. In fact, we can do this not just for different crops, but for different values across all of our test time augmentation parameters"</p>
<p><strong>What is the problem TTA addresses and why use it?</strong></p>
<p>"When we use random cropping, fastai will automatically <strong>use center-cropping for the validation set</strong>" which can be probelmatic, for example, in multi-label tasks where "sometimes there are small objects toward the edges of an image" that might be cropped out entirely or perhaps features on the fringe that are required for any classification task.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span><span class="p">,</span> <span class="n">targs</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">tta</span><span class="p">()</span>
<span class="n">accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">targs</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
      <progress value="0" class="" max="5" style="width:300px; height:20px; vertical-align: middle;"></progress>
      
    </div>
    

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.861090362071991</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>"TTA gives us a good boost in performance, <strong>with no additional training required.</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Tip-6:-Use-MixUp">
<a class="anchor" href="#Tip-6:-Use-MixUp" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tip 6: Use <code>MixUp</code><a class="anchor-link" href="#Tip-6:-Use-MixUp"> </a>
</h2>
<p>"<strong>Mixup</strong> ... is a powerful data augmentation technique that can provide dramatically higher accuracy, <strong>especially when you don't have much data and don't have a pretrained model that was trained on data similar to your dataset</strong>"</p>
<p>It is a dataset-independent form of data augmentation = can be applied without domain knowledge of the dataset to configure other forms of data augmentation (e.g., flipping and to what degree, etc...)</p>
<p><strong>How does Mixup work?</strong></p>
<ol>
<li>Select another random image</li>
<li>Pick a weight at random</li>
<li>Take a weighted average of the selected image with your image = <strong>Your independent variable</strong>
</li>
<li>Take a weighted average of the selected image's labels with your image's labels = <strong>Your dependent variable</strong>
</li>
<li>Use #3 to predict #4</li>
</ol>
<p>In pseudocode:</p>

<pre><code>img2, targ2 = dataset[randint(0, len(dataset))]
t = random_float(0.5, 1.0)
new_img = t * img1 + (1-t) * img2
new_targ = t * targ1 + (1-t) * targ2</code></pre>
<p>Note: "For this to work, our targets need to be one-hot encoded"</p>
<p>Important: "Mixup requires far more epochs"</p>
<p>Note: "One of the reasons Mixup is so exciting is that it can be applied to types of data other than photos. In fact, some people have even shown good results by **using Mixup on activations inside their models, not just on inputs - this allows Mixup to be used for NLP and other data types too."</p>
<p>See pp.247-249 for a detailed example of how Mixup works and is used in fastai</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">xresnet50</span><span class="p">()</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">CrossEntropyLossFlat</span><span class="p">(),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">cbs</span><span class="o">=</span><span class="n">MixUp</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">3e-3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>2.183965</td>
      <td>2.523945</td>
      <td>0.320762</td>
      <td>02:57</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.729223</td>
      <td>1.974045</td>
      <td>0.461538</td>
      <td>03:01</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.479313</td>
      <td>1.131723</td>
      <td>0.630695</td>
      <td>03:07</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.294975</td>
      <td>0.872954</td>
      <td>0.724421</td>
      <td>03:08</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.183486</td>
      <td>0.731506</td>
      <td>0.776699</td>
      <td>03:06</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: "... it's going to be hard to train, because ... the model has to predict two labels per image rather than just one .... Overfitting seems less likely to be a problem."</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id='Tip-7:-Use-"Label-Smoothing"'>
<a class="anchor" href="#Tip-7:-Use-" label-smoothing aria-hidden="true"><span class="octicon octicon-link"></span></a>Tip 7: Use "Label Smoothing"<a class="anchor-link" href="#Tip-7:-Use-%22Label-Smoothing%22"> </a>
</h2>
<p>"In the theoretical expression of loss, <strong>in classification problems</strong>, our targets are one hot encoded .... That means the model is trained to return 0 for all categories but one, for whith it is trained to return 1.... This encourages overfitting and gives you at inference time a model that is not going to give meaningful probabilities: it will always say 1 for the predicted category <strong>even if it's not too sure</strong>, just because it was trained that way.</p>
<p>Important: "This can become very harmful if your data is not perfectly labeled."</p>
<p>"In general, your data will never be perfect.  Even if the labels were manually produced by humans, they could make mistakes, or have differences of opinions on images that are harder to label"</p>
<p><strong>What is the solution this this?</strong></p>
<p>"... we could replace all our 1s with a number a bit less than 1, and our 0s with a number a bit more than 0, and then train" = <strong>Label smoothing</strong>. "By encouraging your model to be a less confident, label smoothing will amek your training more robust, <strong>even if there is mislabeled data [the] result will be a model that generalizes better at inference</strong>"</p>
<p>See pp.249-251 for a detailed explanation and example of how label smoothing operates. <strong>To use it, we just have to change our loss function.</strong></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">xresnet50</span><span class="p">()</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">LabelSmoothingCrossEntropy</span><span class="p">(),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">3e-3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>

</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>2.757509</td>
      <td>3.500999</td>
      <td>0.253921</td>
      <td>03:06</td>
    </tr>
    <tr>
      <td>1</td>
      <td>2.257501</td>
      <td>2.817133</td>
      <td>0.440627</td>
      <td>03:00</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.968483</td>
      <td>2.138581</td>
      <td>0.617625</td>
      <td>02:59</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.781833</td>
      <td>1.700527</td>
      <td>0.772591</td>
      <td>03:05</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.648491</td>
      <td>1.632251</td>
      <td>0.798357</td>
      <td>03:01</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Important: "As with Mixup, you won't generally see significant improvements from label smoothing until you train more epochs."</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Resources">
<a class="anchor" href="#Resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resources<a class="anchor-link" href="#Resources"> </a>
</h2>
<ol>
<li>
<p><a href="https://book.fast.ai">https://book.fast.ai</a> - The book's website; it's updated regularly with new content and recommendations from everything to GPUs to use, how to run things locally and on the cloud, etc...</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1812.01187">Bag of Tricks for Image Classification with Convolutional Neural Networks</a> discusses a variety of techniques you can use with CNNs</p>
</li>
<li>
<p><a href="https://myrtle.ai/how-to-train-your-resnet-8-bag-of-tricks/">How to Train Your ResNet 8: Bag of Tricks</a> discusses a variety of techniques you can use to training ResNets.</p>
</li>
<li>
<p><a href="https://airctic.com/0.12.0/">IceVision</a> is a great resource for all things computer vision and a fastai friendly library.  You may want to follow these twitter accounts as well: @ai_fast_track and @Fra_Pochetti (creator of IceVision)</p>
</li>
</ol>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ohmeow/ohmeow_website"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/posts/2022/03/28/ajtfb-chapter-7.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p>A full-stack web application and ML development company.</p>
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a class="fa-icon" rel="me" href="mailto:wgilliam@ohmeow.com" title="wgilliam@ohmeow.com"><i class="far fa-envelope"></i></a></li><li><a rel="me" href="https://github.com/ohmeow" title="ohmeow"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/waydegilliam" title="waydegilliam"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul></div>

  </div>

</footer>

<script>
  // method will ensure the floating header isn't covering the note header
  function goToNote(elId) {
    const el = document.getElementById(elId);
    const elPos = el.getBoundingClientRect().top + document.documentElement.scrollTop;
      
    window.focus();
    window.scrollTo(0, elPos - 90);
  }

  // ensure target note shows below floating header on # change
  window.addEventListener("hashchange", (ev) => { 
    const hash = ev.target.location.hash
    if (hash) {
      goToNote(hash.substring(1))
    }
  });

  // onload, ensure note shows below floating header
  window.addEventListener("load", (ev) => { 
    const hash = ev.target.location.hash
    if (hash) {
      goToNote(hash.substring(1))
    }
  });
</script></body>

</html>
